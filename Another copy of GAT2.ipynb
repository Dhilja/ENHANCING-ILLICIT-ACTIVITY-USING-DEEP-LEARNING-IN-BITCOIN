{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3212,"status":"ok","timestamp":1714732315382,"user":{"displayName":"Dhilja R","userId":"09907317500965682202"},"user_tz":-330},"id":"dlrWhbWEYmIN","outputId":"9bf7e007-9f6d-4eab-9da6-5300225a4f15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5817,"status":"ok","timestamp":1714732298291,"user":{"displayName":"Dhilja R","userId":"09907317500965682202"},"user_tz":-330},"id":"OEuPfhoQZgJ2","outputId":"56072ed5-c6ce-4857-903d-fe85ec4a8428"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n"]}],"source":["pip install torch-geometric"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13911,"status":"ok","timestamp":1714732312186,"user":{"displayName":"Dhilja R","userId":"09907317500965682202"},"user_tz":-330},"id":"yBLlJQNsZuY7","outputId":"f0a29df4-474c-4279-f333-22d600cc60b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Epoch [1/500], Loss: 2.6427366733551025, Train Acc: 0.0820,train F1-score:0.1011 Val Loss: 1.514008641242981, Val Acc: 0.6787\n","Epoch [2/500], Loss: 1.7787212133407593, Train Acc: 0.6442,train F1-score:0.5049 Val Loss: 1.1631855964660645, Val Acc: 0.6787\n","Epoch [3/500], Loss: 1.368759274482727, Train Acc: 0.6406,train F1-score:0.5101 Val Loss: 1.0955708026885986, Val Acc: 0.6667\n","Epoch [4/500], Loss: 1.2451982498168945, Train Acc: 0.5998,train F1-score:0.5393 Val Loss: 1.0438143014907837, Val Acc: 0.7142\n","Epoch [5/500], Loss: 1.1677541732788086, Train Acc: 0.6278,train F1-score:0.5706 Val Loss: 0.9630767703056335, Val Acc: 0.7068\n","Epoch [6/500], Loss: 1.1030761003494263, Train Acc: 0.6573,train F1-score:0.5528 Val Loss: 0.9476293325424194, Val Acc: 0.7075\n","Epoch [7/500], Loss: 1.0972388982772827, Train Acc: 0.6558,train F1-score:0.5431 Val Loss: 0.8763332962989807, Val Acc: 0.7169\n","Epoch [8/500], Loss: 1.0285813808441162, Train Acc: 0.6644,train F1-score:0.5717 Val Loss: 0.86224365234375, Val Acc: 0.7329\n","Epoch [9/500], Loss: 1.0001716613769531, Train Acc: 0.6528,train F1-score:0.6148 Val Loss: 0.8468694090843201, Val Acc: 0.7376\n","Epoch [10/500], Loss: 0.9810665845870972, Train Acc: 0.6629,train F1-score:0.6268 Val Loss: 0.7943034172058105, Val Acc: 0.7436\n","Epoch [11/500], Loss: 0.942027747631073, Train Acc: 0.6869,train F1-score:0.6236 Val Loss: 0.7827954292297363, Val Acc: 0.7510\n","Epoch [12/500], Loss: 0.9424707889556885, Train Acc: 0.6939,train F1-score:0.6171 Val Loss: 0.7598778009414673, Val Acc: 0.7544\n","Epoch [13/500], Loss: 0.9053704142570496, Train Acc: 0.6985,train F1-score:0.6248 Val Loss: 0.7445604205131531, Val Acc: 0.7631\n","Epoch [14/500], Loss: 0.8843951225280762, Train Acc: 0.7056,train F1-score:0.6465 Val Loss: 0.7523094415664673, Val Acc: 0.7711\n","Epoch [15/500], Loss: 0.8790274262428284, Train Acc: 0.7135,train F1-score:0.6731 Val Loss: 0.7388285398483276, Val Acc: 0.7751\n","Epoch [16/500], Loss: 0.8565733432769775, Train Acc: 0.7184,train F1-score:0.6836 Val Loss: 0.7132185101509094, Val Acc: 0.7731\n","Epoch [17/500], Loss: 0.8354082703590393, Train Acc: 0.7173,train F1-score:0.6731 Val Loss: 0.7001552581787109, Val Acc: 0.7697\n","Epoch [18/500], Loss: 0.823903501033783, Train Acc: 0.7198,train F1-score:0.6681 Val Loss: 0.6893980503082275, Val Acc: 0.7724\n","Epoch [19/500], Loss: 0.8142677545547485, Train Acc: 0.7215,train F1-score:0.6689 Val Loss: 0.681456983089447, Val Acc: 0.7771\n","Epoch [20/500], Loss: 0.8024519085884094, Train Acc: 0.7232,train F1-score:0.6787 Val Loss: 0.6698215007781982, Val Acc: 0.7791\n","Epoch [21/500], Loss: 0.7864164710044861, Train Acc: 0.7294,train F1-score:0.6921 Val Loss: 0.6562700867652893, Val Acc: 0.7925\n","Epoch [22/500], Loss: 0.766157865524292, Train Acc: 0.7424,train F1-score:0.7097 Val Loss: 0.6527519822120667, Val Acc: 0.7992\n","Epoch [23/500], Loss: 0.7634162306785583, Train Acc: 0.7474,train F1-score:0.7173 Val Loss: 0.6417911052703857, Val Acc: 0.7945\n","Epoch [24/500], Loss: 0.7572696805000305, Train Acc: 0.7506,train F1-score:0.7218 Val Loss: 0.628524124622345, Val Acc: 0.7959\n","Epoch [25/500], Loss: 0.7410206198692322, Train Acc: 0.7535,train F1-score:0.7257 Val Loss: 0.6216893792152405, Val Acc: 0.7952\n","Epoch [26/500], Loss: 0.7459017634391785, Train Acc: 0.7550,train F1-score:0.7276 Val Loss: 0.6142450571060181, Val Acc: 0.7972\n","Epoch [27/500], Loss: 0.7234309315681458, Train Acc: 0.7598,train F1-score:0.7337 Val Loss: 0.6080167293548584, Val Acc: 0.8019\n","Epoch [28/500], Loss: 0.7120055556297302, Train Acc: 0.7610,train F1-score:0.7353 Val Loss: 0.6023679971694946, Val Acc: 0.8059\n","Epoch [29/500], Loss: 0.7109285593032837, Train Acc: 0.7663,train F1-score:0.7428 Val Loss: 0.5951340198516846, Val Acc: 0.8092\n","Epoch [30/500], Loss: 0.6944819688796997, Train Acc: 0.7711,train F1-score:0.7501 Val Loss: 0.5896533727645874, Val Acc: 0.8099\n","Epoch [31/500], Loss: 0.6914089322090149, Train Acc: 0.7737,train F1-score:0.7538 Val Loss: 0.5816448330879211, Val Acc: 0.8126\n","Epoch [32/500], Loss: 0.6865585446357727, Train Acc: 0.7699,train F1-score:0.7466 Val Loss: 0.5731431245803833, Val Acc: 0.8092\n","Epoch [33/500], Loss: 0.6695195436477661, Train Acc: 0.7737,train F1-score:0.7498 Val Loss: 0.5668095946311951, Val Acc: 0.8133\n","Epoch [34/500], Loss: 0.6638539433479309, Train Acc: 0.7769,train F1-score:0.7540 Val Loss: 0.562545895576477, Val Acc: 0.8146\n","Epoch [35/500], Loss: 0.6607490181922913, Train Acc: 0.7768,train F1-score:0.7573 Val Loss: 0.561174213886261, Val Acc: 0.8179\n","Epoch [36/500], Loss: 0.6593126058578491, Train Acc: 0.7795,train F1-score:0.7623 Val Loss: 0.5574886798858643, Val Acc: 0.8166\n","Epoch [37/500], Loss: 0.6505860686302185, Train Acc: 0.7848,train F1-score:0.7671 Val Loss: 0.5537416338920593, Val Acc: 0.8173\n","Epoch [38/500], Loss: 0.6403077840805054, Train Acc: 0.7843,train F1-score:0.7653 Val Loss: 0.54991215467453, Val Acc: 0.8179\n","Epoch [39/500], Loss: 0.6379194855690002, Train Acc: 0.7885,train F1-score:0.7690 Val Loss: 0.5430225729942322, Val Acc: 0.8199\n","Epoch [40/500], Loss: 0.6286043524742126, Train Acc: 0.7848,train F1-score:0.7660 Val Loss: 0.5355849862098694, Val Acc: 0.8186\n","Epoch [41/500], Loss: 0.6311349868774414, Train Acc: 0.7889,train F1-score:0.7712 Val Loss: 0.5287920236587524, Val Acc: 0.8240\n","Epoch [42/500], Loss: 0.6273126006126404, Train Acc: 0.7906,train F1-score:0.7738 Val Loss: 0.5228900909423828, Val Acc: 0.8300\n","Epoch [43/500], Loss: 0.6185213327407837, Train Acc: 0.7932,train F1-score:0.7748 Val Loss: 0.5181297063827515, Val Acc: 0.8300\n","Epoch [44/500], Loss: 0.6041391491889954, Train Acc: 0.7942,train F1-score:0.7773 Val Loss: 0.514459490776062, Val Acc: 0.8286\n","Epoch [45/500], Loss: 0.6066861748695374, Train Acc: 0.7943,train F1-score:0.7771 Val Loss: 0.5129804015159607, Val Acc: 0.8266\n","Epoch [46/500], Loss: 0.6005951762199402, Train Acc: 0.7959,train F1-score:0.7793 Val Loss: 0.5122172236442566, Val Acc: 0.8246\n","Epoch [47/500], Loss: 0.6052497029304504, Train Acc: 0.7931,train F1-score:0.7773 Val Loss: 0.5088878273963928, Val Acc: 0.8286\n","Epoch [48/500], Loss: 0.5907688736915588, Train Acc: 0.7980,train F1-score:0.7822 Val Loss: 0.5022134184837341, Val Acc: 0.8320\n","Epoch [49/500], Loss: 0.5922905802726746, Train Acc: 0.7959,train F1-score:0.7801 Val Loss: 0.4955945312976837, Val Acc: 0.8320\n","Epoch [50/500], Loss: 0.5879634022712708, Train Acc: 0.8019,train F1-score:0.7866 Val Loss: 0.4912732243537903, Val Acc: 0.8347\n","Epoch [51/500], Loss: 0.5829523801803589, Train Acc: 0.8041,train F1-score:0.7885 Val Loss: 0.48820751905441284, Val Acc: 0.8353\n","Epoch [52/500], Loss: 0.5787190794944763, Train Acc: 0.8052,train F1-score:0.7896 Val Loss: 0.4877544343471527, Val Acc: 0.8360\n","Epoch [53/500], Loss: 0.5735765099525452, Train Acc: 0.8078,train F1-score:0.7928 Val Loss: 0.4854792356491089, Val Acc: 0.8360\n","Epoch [54/500], Loss: 0.5658162236213684, Train Acc: 0.8046,train F1-score:0.7888 Val Loss: 0.4824926257133484, Val Acc: 0.8380\n","Epoch [55/500], Loss: 0.5686147809028625, Train Acc: 0.8058,train F1-score:0.7905 Val Loss: 0.47795289754867554, Val Acc: 0.8400\n","Epoch [56/500], Loss: 0.5643250942230225, Train Acc: 0.8092,train F1-score:0.7950 Val Loss: 0.47317737340927124, Val Acc: 0.8420\n","Epoch [57/500], Loss: 0.5611850023269653, Train Acc: 0.8064,train F1-score:0.7914 Val Loss: 0.4692819118499756, Val Acc: 0.8420\n","Epoch [58/500], Loss: 0.551892876625061, Train Acc: 0.8073,train F1-score:0.7922 Val Loss: 0.46559587121009827, Val Acc: 0.8467\n","Epoch [59/500], Loss: 0.5518378019332886, Train Acc: 0.8139,train F1-score:0.8011 Val Loss: 0.4625873267650604, Val Acc: 0.8474\n","Epoch [60/500], Loss: 0.5446258187294006, Train Acc: 0.8110,train F1-score:0.7988 Val Loss: 0.46041542291641235, Val Acc: 0.8461\n","Epoch [61/500], Loss: 0.5561166405677795, Train Acc: 0.8096,train F1-score:0.7962 Val Loss: 0.4594516158103943, Val Acc: 0.8434\n","Epoch [62/500], Loss: 0.5466381907463074, Train Acc: 0.8112,train F1-score:0.7971 Val Loss: 0.4595632255077362, Val Acc: 0.8434\n","Epoch [63/500], Loss: 0.5370535850524902, Train Acc: 0.8141,train F1-score:0.7994 Val Loss: 0.4583198130130768, Val Acc: 0.8474\n","Epoch [64/500], Loss: 0.5297988653182983, Train Acc: 0.8166,train F1-score:0.8038 Val Loss: 0.45515671372413635, Val Acc: 0.8501\n","Epoch [65/500], Loss: 0.5393289923667908, Train Acc: 0.8167,train F1-score:0.8037 Val Loss: 0.45102256536483765, Val Acc: 0.8514\n","Epoch [66/500], Loss: 0.532142698764801, Train Acc: 0.8184,train F1-score:0.8054 Val Loss: 0.44833430647850037, Val Acc: 0.8521\n","Epoch [67/500], Loss: 0.5276970863342285, Train Acc: 0.8174,train F1-score:0.8037 Val Loss: 0.44749635457992554, Val Acc: 0.8541\n","Epoch [68/500], Loss: 0.5304651856422424, Train Acc: 0.8176,train F1-score:0.8040 Val Loss: 0.44376593828201294, Val Acc: 0.8514\n","Epoch [69/500], Loss: 0.5277402997016907, Train Acc: 0.8169,train F1-score:0.8044 Val Loss: 0.43954622745513916, Val Acc: 0.8561\n","Epoch [70/500], Loss: 0.5261697173118591, Train Acc: 0.8236,train F1-score:0.8126 Val Loss: 0.43669071793556213, Val Acc: 0.8581\n","Epoch [71/500], Loss: 0.5198567509651184, Train Acc: 0.8219,train F1-score:0.8101 Val Loss: 0.4351271986961365, Val Acc: 0.8554\n","Epoch [72/500], Loss: 0.518872082233429, Train Acc: 0.8216,train F1-score:0.8100 Val Loss: 0.4340948462486267, Val Acc: 0.8554\n","Epoch [73/500], Loss: 0.5108426213264465, Train Acc: 0.8233,train F1-score:0.8111 Val Loss: 0.43149495124816895, Val Acc: 0.8568\n","Epoch [74/500], Loss: 0.506182849407196, Train Acc: 0.8245,train F1-score:0.8131 Val Loss: 0.4274289906024933, Val Acc: 0.8561\n","Epoch [75/500], Loss: 0.5150973796844482, Train Acc: 0.8221,train F1-score:0.8103 Val Loss: 0.42421379685401917, Val Acc: 0.8574\n","Epoch [76/500], Loss: 0.512322187423706, Train Acc: 0.8233,train F1-score:0.8114 Val Loss: 0.4225551187992096, Val Acc: 0.8581\n","Epoch [77/500], Loss: 0.5067824721336365, Train Acc: 0.8230,train F1-score:0.8108 Val Loss: 0.42362937331199646, Val Acc: 0.8608\n","Epoch [78/500], Loss: 0.507055401802063, Train Acc: 0.8265,train F1-score:0.8145 Val Loss: 0.4239930510520935, Val Acc: 0.8608\n","Epoch [79/500], Loss: 0.49816662073135376, Train Acc: 0.8272,train F1-score:0.8160 Val Loss: 0.4208444356918335, Val Acc: 0.8621\n","Epoch [80/500], Loss: 0.4960792660713196, Train Acc: 0.8278,train F1-score:0.8161 Val Loss: 0.41882357001304626, Val Acc: 0.8588\n","Epoch [81/500], Loss: 0.48896682262420654, Train Acc: 0.8324,train F1-score:0.8202 Val Loss: 0.41702520847320557, Val Acc: 0.8588\n","Epoch [82/500], Loss: 0.49292653799057007, Train Acc: 0.8295,train F1-score:0.8186 Val Loss: 0.41700252890586853, Val Acc: 0.8621\n","Epoch [83/500], Loss: 0.4880085289478302, Train Acc: 0.8329,train F1-score:0.8220 Val Loss: 0.4149324297904968, Val Acc: 0.8635\n","Epoch [84/500], Loss: 0.4914261996746063, Train Acc: 0.8285,train F1-score:0.8180 Val Loss: 0.4123179614543915, Val Acc: 0.8621\n","Epoch [85/500], Loss: 0.4831797182559967, Train Acc: 0.8328,train F1-score:0.8229 Val Loss: 0.41245102882385254, Val Acc: 0.8608\n","Epoch [86/500], Loss: 0.4791538417339325, Train Acc: 0.8324,train F1-score:0.8214 Val Loss: 0.41421377658843994, Val Acc: 0.8614\n","Epoch [87/500], Loss: 0.4800803065299988, Train Acc: 0.8336,train F1-score:0.8232 Val Loss: 0.41452479362487793, Val Acc: 0.8601\n","Epoch [88/500], Loss: 0.4733813405036926, Train Acc: 0.8355,train F1-score:0.8262 Val Loss: 0.4121989607810974, Val Acc: 0.8614\n","Epoch [89/500], Loss: 0.47881197929382324, Train Acc: 0.8328,train F1-score:0.8236 Val Loss: 0.4086420238018036, Val Acc: 0.8661\n","Epoch [90/500], Loss: 0.47300609946250916, Train Acc: 0.8337,train F1-score:0.8228 Val Loss: 0.406514972448349, Val Acc: 0.8668\n","Epoch [91/500], Loss: 0.4755873680114746, Train Acc: 0.8314,train F1-score:0.8206 Val Loss: 0.4086352288722992, Val Acc: 0.8635\n","Epoch [92/500], Loss: 0.4694642722606659, Train Acc: 0.8391,train F1-score:0.8290 Val Loss: 0.41245612502098083, Val Acc: 0.8594\n","Epoch [93/500], Loss: 0.4711131453514099, Train Acc: 0.8378,train F1-score:0.8283 Val Loss: 0.4116049110889435, Val Acc: 0.8628\n","Epoch [94/500], Loss: 0.4705903232097626, Train Acc: 0.8400,train F1-score:0.8302 Val Loss: 0.40638890862464905, Val Acc: 0.8681\n","Epoch [95/500], Loss: 0.4659483730792999, Train Acc: 0.8384,train F1-score:0.8280 Val Loss: 0.4024266302585602, Val Acc: 0.8668\n","Epoch [96/500], Loss: 0.4642055630683899, Train Acc: 0.8398,train F1-score:0.8295 Val Loss: 0.4009609818458557, Val Acc: 0.8701\n","Epoch [97/500], Loss: 0.470321387052536, Train Acc: 0.8411,train F1-score:0.8322 Val Loss: 0.40185797214508057, Val Acc: 0.8675\n","Epoch [98/500], Loss: 0.460126668214798, Train Acc: 0.8402,train F1-score:0.8306 Val Loss: 0.40282076597213745, Val Acc: 0.8728\n","Epoch [99/500], Loss: 0.45345455408096313, Train Acc: 0.8432,train F1-score:0.8340 Val Loss: 0.4028782844543457, Val Acc: 0.8648\n","Epoch [100/500], Loss: 0.4581869840621948, Train Acc: 0.8421,train F1-score:0.8334 Val Loss: 0.4020465314388275, Val Acc: 0.8635\n","Epoch [101/500], Loss: 0.4560436010360718, Train Acc: 0.8404,train F1-score:0.8314 Val Loss: 0.39945876598358154, Val Acc: 0.8661\n","Epoch [102/500], Loss: 0.4625031352043152, Train Acc: 0.8409,train F1-score:0.8316 Val Loss: 0.3965941071510315, Val Acc: 0.8621\n","Epoch [103/500], Loss: 0.45145419239997864, Train Acc: 0.8444,train F1-score:0.8351 Val Loss: 0.39530429244041443, Val Acc: 0.8641\n","Epoch [104/500], Loss: 0.4596687853336334, Train Acc: 0.8437,train F1-score:0.8339 Val Loss: 0.39438965916633606, Val Acc: 0.8648\n","Epoch [105/500], Loss: 0.44930821657180786, Train Acc: 0.8445,train F1-score:0.8352 Val Loss: 0.39071789383888245, Val Acc: 0.8701\n","Epoch [106/500], Loss: 0.45013946294784546, Train Acc: 0.8386,train F1-score:0.8291 Val Loss: 0.39069586992263794, Val Acc: 0.8708\n","Epoch [107/500], Loss: 0.44790133833885193, Train Acc: 0.8437,train F1-score:0.8346 Val Loss: 0.390819787979126, Val Acc: 0.8681\n","Epoch [108/500], Loss: 0.44901415705680847, Train Acc: 0.8439,train F1-score:0.8358 Val Loss: 0.3909916281700134, Val Acc: 0.8722\n","Epoch [109/500], Loss: 0.444982647895813, Train Acc: 0.8482,train F1-score:0.8399 Val Loss: 0.3895280063152313, Val Acc: 0.8708\n","Epoch [110/500], Loss: 0.4418048560619354, Train Acc: 0.8469,train F1-score:0.8382 Val Loss: 0.39034613966941833, Val Acc: 0.8715\n","Epoch [111/500], Loss: 0.43914541602134705, Train Acc: 0.8497,train F1-score:0.8415 Val Loss: 0.38825491070747375, Val Acc: 0.8722\n","Epoch [112/500], Loss: 0.4390560984611511, Train Acc: 0.8459,train F1-score:0.8377 Val Loss: 0.3850223422050476, Val Acc: 0.8722\n","Epoch [113/500], Loss: 0.44154244661331177, Train Acc: 0.8449,train F1-score:0.8361 Val Loss: 0.3817991316318512, Val Acc: 0.8728\n","Epoch [114/500], Loss: 0.43723180890083313, Train Acc: 0.8487,train F1-score:0.8401 Val Loss: 0.3804679214954376, Val Acc: 0.8755\n","Epoch [115/500], Loss: 0.4504458010196686, Train Acc: 0.8492,train F1-score:0.8403 Val Loss: 0.3821333646774292, Val Acc: 0.8735\n","Epoch [116/500], Loss: 0.4353301227092743, Train Acc: 0.8486,train F1-score:0.8405 Val Loss: 0.3832828104496002, Val Acc: 0.8755\n","Epoch [117/500], Loss: 0.4360601007938385, Train Acc: 0.8460,train F1-score:0.8367 Val Loss: 0.38236644864082336, Val Acc: 0.8748\n","Epoch [118/500], Loss: 0.4330970048904419, Train Acc: 0.8486,train F1-score:0.8394 Val Loss: 0.38314172625541687, Val Acc: 0.8715\n","Epoch [119/500], Loss: 0.4252392053604126, Train Acc: 0.8550,train F1-score:0.8458 Val Loss: 0.3851388692855835, Val Acc: 0.8735\n","Epoch [120/500], Loss: 0.43111443519592285, Train Acc: 0.8477,train F1-score:0.8390 Val Loss: 0.3834400177001953, Val Acc: 0.8775\n","Epoch [121/500], Loss: 0.4271811246871948, Train Acc: 0.8509,train F1-score:0.8434 Val Loss: 0.38195449113845825, Val Acc: 0.8762\n","Epoch [122/500], Loss: 0.4300639033317566, Train Acc: 0.8522,train F1-score:0.8450 Val Loss: 0.3798276484012604, Val Acc: 0.8782\n","Epoch [123/500], Loss: 0.42729732394218445, Train Acc: 0.8494,train F1-score:0.8408 Val Loss: 0.3786737024784088, Val Acc: 0.8762\n","Epoch [124/500], Loss: 0.42545244097709656, Train Acc: 0.8518,train F1-score:0.8432 Val Loss: 0.3784443140029907, Val Acc: 0.8748\n","Epoch [125/500], Loss: 0.42576363682746887, Train Acc: 0.8551,train F1-score:0.8471 Val Loss: 0.3798397481441498, Val Acc: 0.8742\n","Epoch [126/500], Loss: 0.41615185141563416, Train Acc: 0.8532,train F1-score:0.8455 Val Loss: 0.3817461431026459, Val Acc: 0.8728\n","Epoch [127/500], Loss: 0.41617047786712646, Train Acc: 0.8582,train F1-score:0.8508 Val Loss: 0.37979069352149963, Val Acc: 0.8715\n","Epoch [128/500], Loss: 0.4169258773326874, Train Acc: 0.8523,train F1-score:0.8442 Val Loss: 0.37834957242012024, Val Acc: 0.8735\n","Epoch [129/500], Loss: 0.42298227548599243, Train Acc: 0.8494,train F1-score:0.8414 Val Loss: 0.37737610936164856, Val Acc: 0.8735\n","Epoch [130/500], Loss: 0.4096880853176117, Train Acc: 0.8570,train F1-score:0.8498 Val Loss: 0.3766525387763977, Val Acc: 0.8762\n","Epoch [131/500], Loss: 0.4132280945777893, Train Acc: 0.8532,train F1-score:0.8461 Val Loss: 0.3755459785461426, Val Acc: 0.8782\n","Epoch [132/500], Loss: 0.4072566032409668, Train Acc: 0.8577,train F1-score:0.8503 Val Loss: 0.37625449895858765, Val Acc: 0.8775\n","Epoch [133/500], Loss: 0.4148096740245819, Train Acc: 0.8540,train F1-score:0.8462 Val Loss: 0.3795166313648224, Val Acc: 0.8755\n","Epoch [134/500], Loss: 0.413516104221344, Train Acc: 0.8557,train F1-score:0.8474 Val Loss: 0.37998166680336, Val Acc: 0.8755\n","Epoch [135/500], Loss: 0.4060170650482178, Train Acc: 0.8575,train F1-score:0.8497 Val Loss: 0.3786524534225464, Val Acc: 0.8755\n","Epoch [136/500], Loss: 0.40765416622161865, Train Acc: 0.8568,train F1-score:0.8503 Val Loss: 0.3799970746040344, Val Acc: 0.8715\n","Epoch [137/500], Loss: 0.41666680574417114, Train Acc: 0.8572,train F1-score:0.8506 Val Loss: 0.3792736828327179, Val Acc: 0.8735\n","Epoch [138/500], Loss: 0.413583904504776, Train Acc: 0.8571,train F1-score:0.8488 Val Loss: 0.37401047348976135, Val Acc: 0.8735\n","Epoch [139/500], Loss: 0.4019603431224823, Train Acc: 0.8569,train F1-score:0.8489 Val Loss: 0.3712698221206665, Val Acc: 0.8768\n","Epoch [140/500], Loss: 0.4076753854751587, Train Acc: 0.8598,train F1-score:0.8531 Val Loss: 0.37371858954429626, Val Acc: 0.8809\n","Epoch [141/500], Loss: 0.407130628824234, Train Acc: 0.8585,train F1-score:0.8518 Val Loss: 0.37432652711868286, Val Acc: 0.8768\n","Epoch [142/500], Loss: 0.4013795852661133, Train Acc: 0.8585,train F1-score:0.8509 Val Loss: 0.37316372990608215, Val Acc: 0.8742\n","Epoch [143/500], Loss: 0.40000930428504944, Train Acc: 0.8628,train F1-score:0.8555 Val Loss: 0.3724648356437683, Val Acc: 0.8762\n","Epoch [144/500], Loss: 0.40146830677986145, Train Acc: 0.8590,train F1-score:0.8514 Val Loss: 0.37211161851882935, Val Acc: 0.8762\n","Epoch [145/500], Loss: 0.40229833126068115, Train Acc: 0.8585,train F1-score:0.8520 Val Loss: 0.3704949617385864, Val Acc: 0.8795\n","Epoch [146/500], Loss: 0.4051872193813324, Train Acc: 0.8593,train F1-score:0.8527 Val Loss: 0.3696189522743225, Val Acc: 0.8822\n","Epoch [147/500], Loss: 0.3959652781486511, Train Acc: 0.8621,train F1-score:0.8549 Val Loss: 0.36942121386528015, Val Acc: 0.8775\n","Epoch [148/500], Loss: 0.39885133504867554, Train Acc: 0.8587,train F1-score:0.8509 Val Loss: 0.3683834969997406, Val Acc: 0.8762\n","Epoch [149/500], Loss: 0.39288130402565, Train Acc: 0.8644,train F1-score:0.8579 Val Loss: 0.36781632900238037, Val Acc: 0.8788\n","Epoch [150/500], Loss: 0.39763736724853516, Train Acc: 0.8613,train F1-score:0.8547 Val Loss: 0.36576879024505615, Val Acc: 0.8815\n","Epoch [151/500], Loss: 0.39403465390205383, Train Acc: 0.8632,train F1-score:0.8555 Val Loss: 0.3667808473110199, Val Acc: 0.8762\n","Epoch [152/500], Loss: 0.3928665816783905, Train Acc: 0.8617,train F1-score:0.8537 Val Loss: 0.36829859018325806, Val Acc: 0.8802\n","Epoch [153/500], Loss: 0.38791152834892273, Train Acc: 0.8662,train F1-score:0.8596 Val Loss: 0.37060609459877014, Val Acc: 0.8782\n","Epoch [154/500], Loss: 0.38981375098228455, Train Acc: 0.8616,train F1-score:0.8549 Val Loss: 0.3687126338481903, Val Acc: 0.8788\n","Epoch [155/500], Loss: 0.3917991816997528, Train Acc: 0.8633,train F1-score:0.8567 Val Loss: 0.3666512668132782, Val Acc: 0.8835\n","Epoch [156/500], Loss: 0.38002946972846985, Train Acc: 0.8663,train F1-score:0.8590 Val Loss: 0.3664279878139496, Val Acc: 0.8876\n","Epoch [157/500], Loss: 0.3901820778846741, Train Acc: 0.8638,train F1-score:0.8571 Val Loss: 0.3685276508331299, Val Acc: 0.8876\n","Epoch [158/500], Loss: 0.38858360052108765, Train Acc: 0.8638,train F1-score:0.8578 Val Loss: 0.36821287870407104, Val Acc: 0.8835\n","Epoch [159/500], Loss: 0.38903486728668213, Train Acc: 0.8637,train F1-score:0.8570 Val Loss: 0.36779528856277466, Val Acc: 0.8842\n","Epoch [160/500], Loss: 0.3892100751399994, Train Acc: 0.8613,train F1-score:0.8543 Val Loss: 0.3671565651893616, Val Acc: 0.8829\n","Epoch [161/500], Loss: 0.38579341769218445, Train Acc: 0.8665,train F1-score:0.8598 Val Loss: 0.36777958273887634, Val Acc: 0.8849\n","Epoch [162/500], Loss: 0.3868326246738434, Train Acc: 0.8674,train F1-score:0.8612 Val Loss: 0.36820435523986816, Val Acc: 0.8822\n","Epoch [163/500], Loss: 0.38965556025505066, Train Acc: 0.8664,train F1-score:0.8600 Val Loss: 0.36841946840286255, Val Acc: 0.8829\n","Epoch [164/500], Loss: 0.3882295489311218, Train Acc: 0.8656,train F1-score:0.8586 Val Loss: 0.3687746226787567, Val Acc: 0.8829\n","Epoch [165/500], Loss: 0.3808949887752533, Train Acc: 0.8679,train F1-score:0.8619 Val Loss: 0.36941251158714294, Val Acc: 0.8809\n","Epoch [166/500], Loss: 0.37474167346954346, Train Acc: 0.8673,train F1-score:0.8615 Val Loss: 0.36957231163978577, Val Acc: 0.8855\n","Epoch [167/500], Loss: 0.38202521204948425, Train Acc: 0.8684,train F1-score:0.8617 Val Loss: 0.37081509828567505, Val Acc: 0.8809\n","Epoch [168/500], Loss: 0.39083626866340637, Train Acc: 0.8669,train F1-score:0.8596 Val Loss: 0.37221628427505493, Val Acc: 0.8809\n","Epoch [169/500], Loss: 0.3749311864376068, Train Acc: 0.8689,train F1-score:0.8621 Val Loss: 0.3721601366996765, Val Acc: 0.8809\n","Epoch [170/500], Loss: 0.3806714415550232, Train Acc: 0.8674,train F1-score:0.8608 Val Loss: 0.37006762623786926, Val Acc: 0.8842\n","Epoch [171/500], Loss: 0.3756066560745239, Train Acc: 0.8695,train F1-score:0.8644 Val Loss: 0.3708624243736267, Val Acc: 0.8869\n","Epoch [172/500], Loss: 0.3751892149448395, Train Acc: 0.8687,train F1-score:0.8627 Val Loss: 0.3705766201019287, Val Acc: 0.8849\n","Epoch [173/500], Loss: 0.37072595953941345, Train Acc: 0.8701,train F1-score:0.8640 Val Loss: 0.36857402324676514, Val Acc: 0.8862\n","Epoch [174/500], Loss: 0.3809733986854553, Train Acc: 0.8682,train F1-score:0.8623 Val Loss: 0.36630770564079285, Val Acc: 0.8876\n","Epoch [175/500], Loss: 0.37856152653694153, Train Acc: 0.8674,train F1-score:0.8619 Val Loss: 0.3634701073169708, Val Acc: 0.8889\n","Epoch [176/500], Loss: 0.3779846429824829, Train Acc: 0.8685,train F1-score:0.8620 Val Loss: 0.36201193928718567, Val Acc: 0.8869\n","Epoch [177/500], Loss: 0.36714550852775574, Train Acc: 0.8713,train F1-score:0.8645 Val Loss: 0.36164045333862305, Val Acc: 0.8835\n","Epoch [178/500], Loss: 0.372023344039917, Train Acc: 0.8724,train F1-score:0.8659 Val Loss: 0.36201536655426025, Val Acc: 0.8829\n","Epoch [179/500], Loss: 0.3717605173587799, Train Acc: 0.8687,train F1-score:0.8621 Val Loss: 0.36111435294151306, Val Acc: 0.8869\n","Epoch [180/500], Loss: 0.3643123507499695, Train Acc: 0.8694,train F1-score:0.8641 Val Loss: 0.3588256537914276, Val Acc: 0.8869\n","Epoch [181/500], Loss: 0.36386585235595703, Train Acc: 0.8715,train F1-score:0.8659 Val Loss: 0.36136460304260254, Val Acc: 0.8882\n","Epoch [182/500], Loss: 0.36849650740623474, Train Acc: 0.8708,train F1-score:0.8647 Val Loss: 0.36614108085632324, Val Acc: 0.8876\n","Epoch [183/500], Loss: 0.3681729733943939, Train Acc: 0.8710,train F1-score:0.8656 Val Loss: 0.3721018433570862, Val Acc: 0.8849\n","Epoch [184/500], Loss: 0.36318421363830566, Train Acc: 0.8746,train F1-score:0.8690 Val Loss: 0.3722853362560272, Val Acc: 0.8842\n","Epoch [185/500], Loss: 0.366763174533844, Train Acc: 0.8731,train F1-score:0.8678 Val Loss: 0.3674960136413574, Val Acc: 0.8835\n","Epoch [186/500], Loss: 0.36869245767593384, Train Acc: 0.8710,train F1-score:0.8643 Val Loss: 0.36164578795433044, Val Acc: 0.8862\n","Epoch [187/500], Loss: 0.3633247911930084, Train Acc: 0.8706,train F1-score:0.8644 Val Loss: 0.35971179604530334, Val Acc: 0.8855\n","Epoch [188/500], Loss: 0.3605443835258484, Train Acc: 0.8742,train F1-score:0.8686 Val Loss: 0.3547416925430298, Val Acc: 0.8862\n","Epoch [189/500], Loss: 0.36467134952545166, Train Acc: 0.8735,train F1-score:0.8677 Val Loss: 0.35333552956581116, Val Acc: 0.8842\n","Epoch [190/500], Loss: 0.36341747641563416, Train Acc: 0.8726,train F1-score:0.8665 Val Loss: 0.35384711623191833, Val Acc: 0.8902\n","Epoch [191/500], Loss: 0.35743647813796997, Train Acc: 0.8763,train F1-score:0.8707 Val Loss: 0.3580520451068878, Val Acc: 0.8876\n","Epoch [192/500], Loss: 0.36145374178886414, Train Acc: 0.8739,train F1-score:0.8688 Val Loss: 0.359415739774704, Val Acc: 0.8862\n","Epoch [193/500], Loss: 0.35939911007881165, Train Acc: 0.8753,train F1-score:0.8701 Val Loss: 0.3583880066871643, Val Acc: 0.8815\n","Epoch [194/500], Loss: 0.3603702485561371, Train Acc: 0.8761,train F1-score:0.8700 Val Loss: 0.35598140954971313, Val Acc: 0.8842\n","Epoch [195/500], Loss: 0.35681432485580444, Train Acc: 0.8754,train F1-score:0.8694 Val Loss: 0.35533618927001953, Val Acc: 0.8869\n","Epoch [196/500], Loss: 0.3588055670261383, Train Acc: 0.8763,train F1-score:0.8709 Val Loss: 0.355733722448349, Val Acc: 0.8922\n","Epoch [197/500], Loss: 0.3674548268318176, Train Acc: 0.8761,train F1-score:0.8714 Val Loss: 0.3554432690143585, Val Acc: 0.8916\n","Epoch [198/500], Loss: 0.35205551981925964, Train Acc: 0.8772,train F1-score:0.8713 Val Loss: 0.3576662540435791, Val Acc: 0.8862\n","Epoch [199/500], Loss: 0.3452303111553192, Train Acc: 0.8762,train F1-score:0.8705 Val Loss: 0.35697731375694275, Val Acc: 0.8889\n","Epoch [200/500], Loss: 0.345694899559021, Train Acc: 0.8767,train F1-score:0.8712 Val Loss: 0.3573712110519409, Val Acc: 0.8916\n","Epoch [201/500], Loss: 0.348871648311615, Train Acc: 0.8785,train F1-score:0.8731 Val Loss: 0.35707107186317444, Val Acc: 0.8876\n","Epoch [202/500], Loss: 0.3535679876804352, Train Acc: 0.8780,train F1-score:0.8725 Val Loss: 0.35675498843193054, Val Acc: 0.8835\n","Epoch [203/500], Loss: 0.3506569564342499, Train Acc: 0.8767,train F1-score:0.8712 Val Loss: 0.3562217354774475, Val Acc: 0.8869\n","Epoch [204/500], Loss: 0.35511767864227295, Train Acc: 0.8764,train F1-score:0.8707 Val Loss: 0.36058029532432556, Val Acc: 0.8882\n","Epoch [205/500], Loss: 0.347686231136322, Train Acc: 0.8742,train F1-score:0.8689 Val Loss: 0.3586089015007019, Val Acc: 0.8916\n","Epoch [206/500], Loss: 0.34865817427635193, Train Acc: 0.8759,train F1-score:0.8704 Val Loss: 0.3587428033351898, Val Acc: 0.8936\n","Epoch [207/500], Loss: 0.34901949763298035, Train Acc: 0.8791,train F1-score:0.8739 Val Loss: 0.35855066776275635, Val Acc: 0.8909\n","Epoch [208/500], Loss: 0.3497788608074188, Train Acc: 0.8773,train F1-score:0.8715 Val Loss: 0.36031627655029297, Val Acc: 0.8896\n","Epoch [209/500], Loss: 0.3455442488193512, Train Acc: 0.8782,train F1-score:0.8732 Val Loss: 0.35761046409606934, Val Acc: 0.8942\n","Epoch [210/500], Loss: 0.46761664748191833, Train Acc: 0.8777,train F1-score:0.8727 Val Loss: 0.35499659180641174, Val Acc: 0.8949\n","Epoch [211/500], Loss: 0.35183069109916687, Train Acc: 0.8750,train F1-score:0.8699 Val Loss: 0.35613492131233215, Val Acc: 0.8902\n","Epoch [212/500], Loss: 0.35038742423057556, Train Acc: 0.8741,train F1-score:0.8692 Val Loss: 0.3536180555820465, Val Acc: 0.8902\n","Epoch [213/500], Loss: 0.341146856546402, Train Acc: 0.8772,train F1-score:0.8722 Val Loss: 0.35438957810401917, Val Acc: 0.8882\n","Epoch [214/500], Loss: 0.3508467674255371, Train Acc: 0.8770,train F1-score:0.8718 Val Loss: 0.35360196232795715, Val Acc: 0.8922\n","Epoch [215/500], Loss: 0.3445911407470703, Train Acc: 0.8787,train F1-score:0.8732 Val Loss: 0.3510230779647827, Val Acc: 0.8896\n","Epoch [216/500], Loss: 0.34897029399871826, Train Acc: 0.8793,train F1-score:0.8744 Val Loss: 0.35037773847579956, Val Acc: 0.8896\n","Epoch [217/500], Loss: 0.35246267914772034, Train Acc: 0.8780,train F1-score:0.8728 Val Loss: 0.3521355092525482, Val Acc: 0.8869\n","Epoch [218/500], Loss: 0.3415587842464447, Train Acc: 0.8791,train F1-score:0.8739 Val Loss: 0.35522031784057617, Val Acc: 0.8869\n","Epoch [219/500], Loss: 0.3484179973602295, Train Acc: 0.8777,train F1-score:0.8722 Val Loss: 0.3560212254524231, Val Acc: 0.8869\n","Epoch [220/500], Loss: 0.34276294708251953, Train Acc: 0.8808,train F1-score:0.8759 Val Loss: 0.35494887828826904, Val Acc: 0.8896\n","Epoch [221/500], Loss: 0.3381023705005646, Train Acc: 0.8813,train F1-score:0.8768 Val Loss: 0.3545174300670624, Val Acc: 0.8929\n","Epoch [222/500], Loss: 0.34730637073516846, Train Acc: 0.8797,train F1-score:0.8755 Val Loss: 0.3542010188102722, Val Acc: 0.8949\n","Epoch [223/500], Loss: 0.3396211266517639, Train Acc: 0.8810,train F1-score:0.8766 Val Loss: 0.3520253598690033, Val Acc: 0.8889\n","Epoch [224/500], Loss: 0.33977946639060974, Train Acc: 0.8795,train F1-score:0.8733 Val Loss: 0.3497912287712097, Val Acc: 0.8922\n","Epoch [225/500], Loss: 0.33808064460754395, Train Acc: 0.8825,train F1-score:0.8770 Val Loss: 0.34608960151672363, Val Acc: 0.8956\n","Epoch [226/500], Loss: 0.33489474654197693, Train Acc: 0.8802,train F1-score:0.8751 Val Loss: 0.34574320912361145, Val Acc: 0.8942\n","Epoch [227/500], Loss: 0.34104570746421814, Train Acc: 0.8766,train F1-score:0.8716 Val Loss: 0.34623152017593384, Val Acc: 0.8909\n","Epoch [228/500], Loss: 0.3342649042606354, Train Acc: 0.8836,train F1-score:0.8783 Val Loss: 0.34609171748161316, Val Acc: 0.8902\n","Epoch [229/500], Loss: 0.3368542790412903, Train Acc: 0.8818,train F1-score:0.8764 Val Loss: 0.34706488251686096, Val Acc: 0.8929\n","Epoch [230/500], Loss: 0.33826079964637756, Train Acc: 0.8806,train F1-score:0.8756 Val Loss: 0.3477293848991394, Val Acc: 0.8929\n","Epoch [231/500], Loss: 0.3419260084629059, Train Acc: 0.8815,train F1-score:0.8767 Val Loss: 0.348249226808548, Val Acc: 0.8929\n","Epoch [232/500], Loss: 0.3349692225456238, Train Acc: 0.8833,train F1-score:0.8784 Val Loss: 0.3478940725326538, Val Acc: 0.8929\n","Epoch [233/500], Loss: 0.33400577306747437, Train Acc: 0.8814,train F1-score:0.8758 Val Loss: 0.34677693247795105, Val Acc: 0.8902\n","Epoch [234/500], Loss: 0.33919021487236023, Train Acc: 0.8809,train F1-score:0.8753 Val Loss: 0.3455192446708679, Val Acc: 0.8909\n","Epoch [235/500], Loss: 0.3323002755641937, Train Acc: 0.8847,train F1-score:0.8801 Val Loss: 0.3468726873397827, Val Acc: 0.8909\n","Epoch [236/500], Loss: 0.3364675045013428, Train Acc: 0.8810,train F1-score:0.8769 Val Loss: 0.34898677468299866, Val Acc: 0.8922\n","Epoch [237/500], Loss: 0.3304295837879181, Train Acc: 0.8850,train F1-score:0.8802 Val Loss: 0.3538590371608734, Val Acc: 0.8909\n","Epoch [238/500], Loss: 0.3351815342903137, Train Acc: 0.8810,train F1-score:0.8754 Val Loss: 0.35329899191856384, Val Acc: 0.8882\n","Epoch [239/500], Loss: 0.33143770694732666, Train Acc: 0.8834,train F1-score:0.8786 Val Loss: 0.3532088100910187, Val Acc: 0.8909\n","Epoch [240/500], Loss: 0.33078449964523315, Train Acc: 0.8862,train F1-score:0.8825 Val Loss: 0.35432663559913635, Val Acc: 0.8896\n","Epoch [241/500], Loss: 0.33791473507881165, Train Acc: 0.8772,train F1-score:0.8724 Val Loss: 0.35403308272361755, Val Acc: 0.8896\n","Epoch [242/500], Loss: 0.3300425112247467, Train Acc: 0.8843,train F1-score:0.8795 Val Loss: 0.35474854707717896, Val Acc: 0.8889\n","Epoch [243/500], Loss: 0.3292105197906494, Train Acc: 0.8854,train F1-score:0.8808 Val Loss: 0.3550587296485901, Val Acc: 0.8916\n","Epoch [244/500], Loss: 0.3340839445590973, Train Acc: 0.8817,train F1-score:0.8771 Val Loss: 0.3567327558994293, Val Acc: 0.8936\n","Epoch [245/500], Loss: 0.32793790102005005, Train Acc: 0.8832,train F1-score:0.8789 Val Loss: 0.35740938782691956, Val Acc: 0.8889\n","Epoch [246/500], Loss: 0.32963064312934875, Train Acc: 0.8839,train F1-score:0.8783 Val Loss: 0.3534002900123596, Val Acc: 0.8942\n","Epoch [247/500], Loss: 0.3299015760421753, Train Acc: 0.8849,train F1-score:0.8795 Val Loss: 0.34985285997390747, Val Acc: 0.8963\n","Epoch [248/500], Loss: 0.3242165148258209, Train Acc: 0.8868,train F1-score:0.8819 Val Loss: 0.34949854016304016, Val Acc: 0.8929\n","Epoch [249/500], Loss: 0.33036044239997864, Train Acc: 0.8810,train F1-score:0.8763 Val Loss: 0.3492746651172638, Val Acc: 0.8929\n","Epoch [250/500], Loss: 0.32850635051727295, Train Acc: 0.8835,train F1-score:0.8791 Val Loss: 0.348145067691803, Val Acc: 0.8929\n","Epoch [251/500], Loss: 0.32069259881973267, Train Acc: 0.8848,train F1-score:0.8797 Val Loss: 0.34708574414253235, Val Acc: 0.8896\n","Epoch [252/500], Loss: 0.32728344202041626, Train Acc: 0.8854,train F1-score:0.8808 Val Loss: 0.3449445962905884, Val Acc: 0.8949\n","Epoch [253/500], Loss: 0.32158219814300537, Train Acc: 0.8870,train F1-score:0.8828 Val Loss: 0.3482777178287506, Val Acc: 0.8929\n","Epoch [254/500], Loss: 0.32829561829566956, Train Acc: 0.8859,train F1-score:0.8815 Val Loss: 0.3465355932712555, Val Acc: 0.8936\n","Epoch [255/500], Loss: 0.3272252380847931, Train Acc: 0.8842,train F1-score:0.8795 Val Loss: 0.34481778740882874, Val Acc: 0.8929\n","Epoch [256/500], Loss: 0.31768423318862915, Train Acc: 0.8877,train F1-score:0.8833 Val Loss: 0.3434850573539734, Val Acc: 0.8909\n","Epoch [257/500], Loss: 0.32375967502593994, Train Acc: 0.8851,train F1-score:0.8802 Val Loss: 0.3430478870868683, Val Acc: 0.8916\n","Epoch [258/500], Loss: 0.32144320011138916, Train Acc: 0.8879,train F1-score:0.8832 Val Loss: 0.3406122624874115, Val Acc: 0.8963\n","Epoch [259/500], Loss: 0.3261527419090271, Train Acc: 0.8842,train F1-score:0.8800 Val Loss: 0.34344297647476196, Val Acc: 0.8963\n","Epoch [260/500], Loss: 0.3255057632923126, Train Acc: 0.8857,train F1-score:0.8812 Val Loss: 0.3468431532382965, Val Acc: 0.8916\n","Epoch [261/500], Loss: 0.31831857562065125, Train Acc: 0.8861,train F1-score:0.8811 Val Loss: 0.34885600209236145, Val Acc: 0.8942\n","Epoch [262/500], Loss: 0.3216978907585144, Train Acc: 0.8854,train F1-score:0.8803 Val Loss: 0.34534719586372375, Val Acc: 0.8976\n","Epoch [263/500], Loss: 0.3205794095993042, Train Acc: 0.8857,train F1-score:0.8811 Val Loss: 0.3456888794898987, Val Acc: 0.9003\n","Epoch [264/500], Loss: 0.3195981979370117, Train Acc: 0.8873,train F1-score:0.8836 Val Loss: 0.34826239943504333, Val Acc: 0.8949\n","Epoch [265/500], Loss: 0.3228839933872223, Train Acc: 0.8875,train F1-score:0.8834 Val Loss: 0.3538476526737213, Val Acc: 0.8929\n","Epoch [266/500], Loss: 0.31852829456329346, Train Acc: 0.8864,train F1-score:0.8821 Val Loss: 0.350868821144104, Val Acc: 0.8942\n","Epoch [267/500], Loss: 0.3111295700073242, Train Acc: 0.8898,train F1-score:0.8855 Val Loss: 0.3517429232597351, Val Acc: 0.8949\n","Epoch [268/500], Loss: 0.35689884424209595, Train Acc: 0.8858,train F1-score:0.8815 Val Loss: 0.357464075088501, Val Acc: 0.8936\n","Epoch [269/500], Loss: 0.31607893109321594, Train Acc: 0.8834,train F1-score:0.8787 Val Loss: 0.3595043420791626, Val Acc: 0.8929\n","Epoch [270/500], Loss: 0.32007265090942383, Train Acc: 0.8867,train F1-score:0.8817 Val Loss: 0.35471218824386597, Val Acc: 0.8989\n","Epoch [271/500], Loss: 0.3210359513759613, Train Acc: 0.8871,train F1-score:0.8830 Val Loss: 0.35107091069221497, Val Acc: 0.8989\n","Epoch [272/500], Loss: 0.31437498331069946, Train Acc: 0.8887,train F1-score:0.8850 Val Loss: 0.3480411767959595, Val Acc: 0.8983\n","Epoch [273/500], Loss: 0.31625333428382874, Train Acc: 0.8875,train F1-score:0.8831 Val Loss: 0.34825029969215393, Val Acc: 0.8963\n","Epoch [274/500], Loss: 0.3200712502002716, Train Acc: 0.8863,train F1-score:0.8814 Val Loss: 0.3453654646873474, Val Acc: 0.8976\n","Epoch [275/500], Loss: 0.35376057028770447, Train Acc: 0.8831,train F1-score:0.8784 Val Loss: 0.346680223941803, Val Acc: 0.8989\n","Epoch [276/500], Loss: 0.3227994441986084, Train Acc: 0.8843,train F1-score:0.8802 Val Loss: 0.3485616147518158, Val Acc: 0.8983\n","Epoch [277/500], Loss: 0.31726568937301636, Train Acc: 0.8880,train F1-score:0.8833 Val Loss: 0.3534201383590698, Val Acc: 0.8902\n","Epoch [278/500], Loss: 0.3123970925807953, Train Acc: 0.8890,train F1-score:0.8843 Val Loss: 0.35520341992378235, Val Acc: 0.8889\n","Epoch [279/500], Loss: 0.3231540024280548, Train Acc: 0.8846,train F1-score:0.8794 Val Loss: 0.35155612230300903, Val Acc: 0.8942\n","Epoch [280/500], Loss: 0.31336548924446106, Train Acc: 0.8894,train F1-score:0.8850 Val Loss: 0.34808558225631714, Val Acc: 0.8929\n","Epoch [281/500], Loss: 0.3152603209018707, Train Acc: 0.8866,train F1-score:0.8818 Val Loss: 0.3454430103302002, Val Acc: 0.8922\n","Epoch [282/500], Loss: 0.3066540062427521, Train Acc: 0.8918,train F1-score:0.8876 Val Loss: 0.3453923463821411, Val Acc: 0.8949\n","Epoch [283/500], Loss: 0.3147719204425812, Train Acc: 0.8899,train F1-score:0.8856 Val Loss: 0.34357231855392456, Val Acc: 0.8949\n","Epoch [284/500], Loss: 0.32081592082977295, Train Acc: 0.8881,train F1-score:0.8841 Val Loss: 0.3432621657848358, Val Acc: 0.8969\n","Epoch [285/500], Loss: 0.31431639194488525, Train Acc: 0.8879,train F1-score:0.8841 Val Loss: 0.3435612618923187, Val Acc: 0.8963\n","Epoch [286/500], Loss: 0.31292691826820374, Train Acc: 0.8861,train F1-score:0.8818 Val Loss: 0.3454427123069763, Val Acc: 0.8956\n","Epoch [287/500], Loss: 0.3084094822406769, Train Acc: 0.8910,train F1-score:0.8866 Val Loss: 0.34566056728363037, Val Acc: 0.8942\n","Epoch [288/500], Loss: 0.311348021030426, Train Acc: 0.8907,train F1-score:0.8862 Val Loss: 0.34248989820480347, Val Acc: 0.8969\n","Epoch [289/500], Loss: 0.34714818000793457, Train Acc: 0.8934,train F1-score:0.8894 Val Loss: 0.3375939726829529, Val Acc: 0.8976\n","Epoch [290/500], Loss: 0.3636443614959717, Train Acc: 0.8921,train F1-score:0.8882 Val Loss: 0.33691099286079407, Val Acc: 0.9003\n","Epoch [291/500], Loss: 0.35152003169059753, Train Acc: 0.8906,train F1-score:0.8866 Val Loss: 0.33743664622306824, Val Acc: 0.8996\n","Epoch [292/500], Loss: 0.3386155068874359, Train Acc: 0.8901,train F1-score:0.8854 Val Loss: 0.33907824754714966, Val Acc: 0.9003\n","Epoch [293/500], Loss: 0.34454983472824097, Train Acc: 0.8931,train F1-score:0.8893 Val Loss: 0.33978867530822754, Val Acc: 0.9016\n","Epoch [294/500], Loss: 0.32480955123901367, Train Acc: 0.8880,train F1-score:0.8840 Val Loss: 0.3388325273990631, Val Acc: 0.9029\n","Epoch [295/500], Loss: 0.3228229284286499, Train Acc: 0.8909,train F1-score:0.8874 Val Loss: 0.33848467469215393, Val Acc: 0.9023\n","Epoch [296/500], Loss: 0.31398800015449524, Train Acc: 0.8907,train F1-score:0.8869 Val Loss: 0.33575329184532166, Val Acc: 0.9029\n","Epoch [297/500], Loss: 0.307044118642807, Train Acc: 0.8911,train F1-score:0.8871 Val Loss: 0.3353971540927887, Val Acc: 0.9003\n","Epoch [298/500], Loss: 0.3096528649330139, Train Acc: 0.8923,train F1-score:0.8883 Val Loss: 0.337672621011734, Val Acc: 0.8996\n","Epoch [299/500], Loss: 0.30300137400627136, Train Acc: 0.8937,train F1-score:0.8900 Val Loss: 0.3408488929271698, Val Acc: 0.8976\n","Epoch [300/500], Loss: 0.3041781783103943, Train Acc: 0.8926,train F1-score:0.8884 Val Loss: 0.34152254462242126, Val Acc: 0.8969\n","Epoch [301/500], Loss: 0.31605464220046997, Train Acc: 0.8921,train F1-score:0.8882 Val Loss: 0.3400837779045105, Val Acc: 0.8983\n","Epoch [302/500], Loss: 0.30428341031074524, Train Acc: 0.8931,train F1-score:0.8891 Val Loss: 0.3382936120033264, Val Acc: 0.8983\n","Epoch [303/500], Loss: 0.30274227261543274, Train Acc: 0.8942,train F1-score:0.8905 Val Loss: 0.3368421792984009, Val Acc: 0.8989\n","Epoch [304/500], Loss: 0.294732928276062, Train Acc: 0.8938,train F1-score:0.8903 Val Loss: 0.33578893542289734, Val Acc: 0.8996\n","Epoch [305/500], Loss: 0.29946058988571167, Train Acc: 0.8969,train F1-score:0.8931 Val Loss: 0.3314911723136902, Val Acc: 0.8989\n","Epoch [306/500], Loss: 0.2982208728790283, Train Acc: 0.8973,train F1-score:0.8940 Val Loss: 0.33108770847320557, Val Acc: 0.8996\n","Epoch [307/500], Loss: 0.2967352569103241, Train Acc: 0.8936,train F1-score:0.8900 Val Loss: 0.3364242613315582, Val Acc: 0.8969\n","Epoch [308/500], Loss: 0.2992517948150635, Train Acc: 0.8945,train F1-score:0.8906 Val Loss: 0.33967921137809753, Val Acc: 0.8963\n","Epoch [309/500], Loss: 0.301409512758255, Train Acc: 0.8939,train F1-score:0.8900 Val Loss: 0.3367040157318115, Val Acc: 0.8969\n","Epoch [310/500], Loss: 0.29817092418670654, Train Acc: 0.8931,train F1-score:0.8893 Val Loss: 0.3355847895145416, Val Acc: 0.9016\n","Epoch [311/500], Loss: 0.3064676821231842, Train Acc: 0.8919,train F1-score:0.8885 Val Loss: 0.3349517583847046, Val Acc: 0.9023\n","Epoch [312/500], Loss: 0.30442002415657043, Train Acc: 0.8926,train F1-score:0.8887 Val Loss: 0.3345760703086853, Val Acc: 0.9009\n","Epoch [313/500], Loss: 0.29902952909469604, Train Acc: 0.8968,train F1-score:0.8931 Val Loss: 0.3375375270843506, Val Acc: 0.9003\n","Epoch [314/500], Loss: 0.29977551102638245, Train Acc: 0.8963,train F1-score:0.8930 Val Loss: 0.3429167568683624, Val Acc: 0.9003\n","Epoch [315/500], Loss: 0.3009292781352997, Train Acc: 0.8960,train F1-score:0.8925 Val Loss: 0.3492032587528229, Val Acc: 0.8996\n","Epoch [316/500], Loss: 0.3013911247253418, Train Acc: 0.8944,train F1-score:0.8905 Val Loss: 0.3520544469356537, Val Acc: 0.9003\n","Epoch [317/500], Loss: 0.3000631034374237, Train Acc: 0.8949,train F1-score:0.8912 Val Loss: 0.3519721031188965, Val Acc: 0.8976\n","Epoch [318/500], Loss: 0.296295166015625, Train Acc: 0.8945,train F1-score:0.8904 Val Loss: 0.35141411423683167, Val Acc: 0.8963\n","Epoch [319/500], Loss: 0.3028985261917114, Train Acc: 0.8926,train F1-score:0.8883 Val Loss: 0.3499399721622467, Val Acc: 0.8942\n","Epoch [320/500], Loss: 0.29286736249923706, Train Acc: 0.8936,train F1-score:0.8894 Val Loss: 0.3448205292224884, Val Acc: 0.8996\n","Epoch [321/500], Loss: 0.30563780665397644, Train Acc: 0.8937,train F1-score:0.8899 Val Loss: 0.34100624918937683, Val Acc: 0.8996\n","Epoch [322/500], Loss: 0.3070269525051117, Train Acc: 0.8966,train F1-score:0.8930 Val Loss: 0.342018723487854, Val Acc: 0.8983\n","Epoch [323/500], Loss: 0.30238908529281616, Train Acc: 0.8938,train F1-score:0.8900 Val Loss: 0.3443027138710022, Val Acc: 0.8983\n","Epoch [324/500], Loss: 0.2882949411869049, Train Acc: 0.8963,train F1-score:0.8932 Val Loss: 0.34601667523384094, Val Acc: 0.8996\n","Epoch [325/500], Loss: 0.29209527373313904, Train Acc: 0.8968,train F1-score:0.8933 Val Loss: 0.34734344482421875, Val Acc: 0.9023\n","Epoch [326/500], Loss: 0.31604427099227905, Train Acc: 0.8973,train F1-score:0.8935 Val Loss: 0.34790128469467163, Val Acc: 0.9036\n","Epoch [327/500], Loss: 0.2930759787559509, Train Acc: 0.8962,train F1-score:0.8923 Val Loss: 0.3463808000087738, Val Acc: 0.9009\n","Epoch [328/500], Loss: 0.29623037576675415, Train Acc: 0.8957,train F1-score:0.8918 Val Loss: 0.3413819968700409, Val Acc: 0.9029\n","Epoch [329/500], Loss: 0.29704782366752625, Train Acc: 0.8936,train F1-score:0.8904 Val Loss: 0.3394881784915924, Val Acc: 0.9029\n","Epoch [330/500], Loss: 0.2896976172924042, Train Acc: 0.8972,train F1-score:0.8940 Val Loss: 0.3408764898777008, Val Acc: 0.9003\n","Epoch [331/500], Loss: 0.2946682572364807, Train Acc: 0.8934,train F1-score:0.8897 Val Loss: 0.3443976640701294, Val Acc: 0.8983\n","Epoch [332/500], Loss: 0.29569175839424133, Train Acc: 0.8938,train F1-score:0.8900 Val Loss: 0.34806516766548157, Val Acc: 0.8969\n","Epoch [333/500], Loss: 0.2832210958003998, Train Acc: 0.8967,train F1-score:0.8929 Val Loss: 0.3480434715747833, Val Acc: 0.9003\n","Epoch [334/500], Loss: 0.29826587438583374, Train Acc: 0.8952,train F1-score:0.8909 Val Loss: 0.3475903570652008, Val Acc: 0.8989\n","Epoch [335/500], Loss: 0.2973044514656067, Train Acc: 0.8942,train F1-score:0.8911 Val Loss: 0.3443819582462311, Val Acc: 0.9003\n","Epoch [336/500], Loss: 0.28227105736732483, Train Acc: 0.8991,train F1-score:0.8958 Val Loss: 0.3426791727542877, Val Acc: 0.8989\n","Epoch [337/500], Loss: 0.292048841714859, Train Acc: 0.8937,train F1-score:0.8902 Val Loss: 0.34209150075912476, Val Acc: 0.8963\n","Epoch [338/500], Loss: 0.29224202036857605, Train Acc: 0.8947,train F1-score:0.8912 Val Loss: 0.3417660892009735, Val Acc: 0.9009\n","Epoch [339/500], Loss: 0.28745803236961365, Train Acc: 0.8973,train F1-score:0.8946 Val Loss: 0.34359320998191833, Val Acc: 0.9023\n","Epoch [340/500], Loss: 0.2929445803165436, Train Acc: 0.8955,train F1-score:0.8922 Val Loss: 0.346089631319046, Val Acc: 0.9009\n","Epoch [341/500], Loss: 0.28759369254112244, Train Acc: 0.8976,train F1-score:0.8942 Val Loss: 0.3494570851325989, Val Acc: 0.8989\n","Epoch [342/500], Loss: 0.2985362708568573, Train Acc: 0.8957,train F1-score:0.8919 Val Loss: 0.34832683205604553, Val Acc: 0.8969\n","Epoch [343/500], Loss: 0.2928919494152069, Train Acc: 0.8941,train F1-score:0.8904 Val Loss: 0.34709852933883667, Val Acc: 0.8963\n","Epoch [344/500], Loss: 0.296162486076355, Train Acc: 0.8942,train F1-score:0.8909 Val Loss: 0.3448113203048706, Val Acc: 0.8989\n","Epoch [345/500], Loss: 0.28953251242637634, Train Acc: 0.8968,train F1-score:0.8937 Val Loss: 0.34331417083740234, Val Acc: 0.8969\n","Epoch [346/500], Loss: 0.2852652370929718, Train Acc: 0.9007,train F1-score:0.8976 Val Loss: 0.34638115763664246, Val Acc: 0.8983\n","Epoch [347/500], Loss: 0.2948106527328491, Train Acc: 0.8973,train F1-score:0.8941 Val Loss: 0.3414703905582428, Val Acc: 0.9003\n","Epoch [348/500], Loss: 0.28946423530578613, Train Acc: 0.8975,train F1-score:0.8939 Val Loss: 0.33824971318244934, Val Acc: 0.9023\n","Epoch [349/500], Loss: 0.2805667221546173, Train Acc: 0.8995,train F1-score:0.8967 Val Loss: 0.3386058807373047, Val Acc: 0.9016\n","Epoch [350/500], Loss: 0.28845685720443726, Train Acc: 0.8982,train F1-score:0.8942 Val Loss: 0.3377743363380432, Val Acc: 0.9016\n","Epoch [351/500], Loss: 0.3016418516635895, Train Acc: 0.8950,train F1-score:0.8911 Val Loss: 0.33577561378479004, Val Acc: 0.9023\n","Epoch [352/500], Loss: 0.29288819432258606, Train Acc: 0.8978,train F1-score:0.8938 Val Loss: 0.33396482467651367, Val Acc: 0.9056\n","Epoch [353/500], Loss: 0.2878061532974243, Train Acc: 0.8999,train F1-score:0.8966 Val Loss: 0.3344530463218689, Val Acc: 0.9029\n","Epoch [354/500], Loss: 0.28638121485710144, Train Acc: 0.8964,train F1-score:0.8925 Val Loss: 0.33242690563201904, Val Acc: 0.9036\n","Epoch [355/500], Loss: 0.28830069303512573, Train Acc: 0.8973,train F1-score:0.8939 Val Loss: 0.32886210083961487, Val Acc: 0.9056\n","Epoch [356/500], Loss: 0.28769034147262573, Train Acc: 0.8973,train F1-score:0.8942 Val Loss: 0.33275988698005676, Val Acc: 0.9050\n","Epoch [357/500], Loss: 0.28148162364959717, Train Acc: 0.8965,train F1-score:0.8928 Val Loss: 0.3351375162601471, Val Acc: 0.9023\n","Epoch [358/500], Loss: 0.29669663310050964, Train Acc: 0.8955,train F1-score:0.8911 Val Loss: 0.337757408618927, Val Acc: 0.9070\n","Epoch [359/500], Loss: 0.29417774081230164, Train Acc: 0.8983,train F1-score:0.8951 Val Loss: 0.3319333791732788, Val Acc: 0.9063\n","Epoch [360/500], Loss: 0.293678343296051, Train Acc: 0.8947,train F1-score:0.8911 Val Loss: 0.3348582088947296, Val Acc: 0.9043\n","Epoch [361/500], Loss: 0.28917258977890015, Train Acc: 0.8984,train F1-score:0.8950 Val Loss: 0.3393465578556061, Val Acc: 0.9043\n","Epoch [362/500], Loss: 0.2943825125694275, Train Acc: 0.8970,train F1-score:0.8932 Val Loss: 0.34217485785484314, Val Acc: 0.9029\n","Epoch [363/500], Loss: 0.29401326179504395, Train Acc: 0.8957,train F1-score:0.8918 Val Loss: 0.34165963530540466, Val Acc: 0.9016\n","Epoch [364/500], Loss: 0.28651782870292664, Train Acc: 0.9008,train F1-score:0.8977 Val Loss: 0.3439543843269348, Val Acc: 0.8983\n","Epoch [365/500], Loss: 0.2880343496799469, Train Acc: 0.8993,train F1-score:0.8963 Val Loss: 0.34508636593818665, Val Acc: 0.8976\n","Epoch [366/500], Loss: 0.2989133596420288, Train Acc: 0.8968,train F1-score:0.8936 Val Loss: 0.3441663384437561, Val Acc: 0.8989\n","Epoch [367/500], Loss: 0.2912713289260864, Train Acc: 0.8991,train F1-score:0.8958 Val Loss: 0.3427606225013733, Val Acc: 0.8969\n","Epoch [368/500], Loss: 0.28737127780914307, Train Acc: 0.8993,train F1-score:0.8958 Val Loss: 0.3423769772052765, Val Acc: 0.8976\n","Epoch [369/500], Loss: 0.29452913999557495, Train Acc: 0.8993,train F1-score:0.8956 Val Loss: 0.347053200006485, Val Acc: 0.8983\n","Epoch [370/500], Loss: 0.28860098123550415, Train Acc: 0.8976,train F1-score:0.8942 Val Loss: 0.3553362190723419, Val Acc: 0.8963\n","Epoch [371/500], Loss: 0.2871948182582855, Train Acc: 0.9029,train F1-score:0.9000 Val Loss: 0.3567802309989929, Val Acc: 0.8996\n","Epoch [372/500], Loss: 0.29149550199508667, Train Acc: 0.8964,train F1-score:0.8938 Val Loss: 0.353339821100235, Val Acc: 0.8983\n","Epoch [373/500], Loss: 0.2858312726020813, Train Acc: 0.8992,train F1-score:0.8959 Val Loss: 0.35194218158721924, Val Acc: 0.8996\n","Epoch [374/500], Loss: 0.2936561405658722, Train Acc: 0.8970,train F1-score:0.8936 Val Loss: 0.3495713472366333, Val Acc: 0.9003\n","Epoch [375/500], Loss: 0.2923855185508728, Train Acc: 0.9008,train F1-score:0.8976 Val Loss: 0.3454768657684326, Val Acc: 0.9003\n","Epoch [376/500], Loss: 0.2928643524646759, Train Acc: 0.8981,train F1-score:0.8946 Val Loss: 0.3432368338108063, Val Acc: 0.9016\n","Epoch [377/500], Loss: 0.2900366187095642, Train Acc: 0.8974,train F1-score:0.8942 Val Loss: 0.3436451554298401, Val Acc: 0.9043\n","Epoch [378/500], Loss: 0.2942811846733093, Train Acc: 0.8960,train F1-score:0.8928 Val Loss: 0.3433501124382019, Val Acc: 0.9043\n","Epoch [379/500], Loss: 0.295398473739624, Train Acc: 0.8938,train F1-score:0.8898 Val Loss: 0.3421046733856201, Val Acc: 0.9016\n","Epoch [380/500], Loss: 0.2792862355709076, Train Acc: 0.8994,train F1-score:0.8957 Val Loss: 0.33929774165153503, Val Acc: 0.8989\n","Epoch [381/500], Loss: 0.2982349991798401, Train Acc: 0.8950,train F1-score:0.8917 Val Loss: 0.33600914478302, Val Acc: 0.8996\n","Epoch [382/500], Loss: 0.292285680770874, Train Acc: 0.8990,train F1-score:0.8960 Val Loss: 0.3354748487472534, Val Acc: 0.8983\n","Epoch [383/500], Loss: 0.2923484742641449, Train Acc: 0.8969,train F1-score:0.8938 Val Loss: 0.33751410245895386, Val Acc: 0.8983\n","Epoch [384/500], Loss: 0.29197677969932556, Train Acc: 0.8929,train F1-score:0.8890 Val Loss: 0.3377875089645386, Val Acc: 0.9016\n","Epoch [385/500], Loss: 0.2926614284515381, Train Acc: 0.8977,train F1-score:0.8946 Val Loss: 0.3367404341697693, Val Acc: 0.8983\n","Epoch [386/500], Loss: 0.2893812954425812, Train Acc: 0.8970,train F1-score:0.8940 Val Loss: 0.33773618936538696, Val Acc: 0.8983\n","Epoch [387/500], Loss: 0.2865241467952728, Train Acc: 0.9009,train F1-score:0.8978 Val Loss: 0.34006965160369873, Val Acc: 0.8996\n","Epoch [388/500], Loss: 0.29000797867774963, Train Acc: 0.8971,train F1-score:0.8936 Val Loss: 0.3410913050174713, Val Acc: 0.8989\n","Epoch [389/500], Loss: 0.2869355380535126, Train Acc: 0.8951,train F1-score:0.8917 Val Loss: 0.3414499759674072, Val Acc: 0.9009\n","Epoch [390/500], Loss: 0.2892945110797882, Train Acc: 0.8970,train F1-score:0.8937 Val Loss: 0.3417706787586212, Val Acc: 0.9050\n","Epoch [391/500], Loss: 0.2824573516845703, Train Acc: 0.9028,train F1-score:0.8994 Val Loss: 0.34112027287483215, Val Acc: 0.9043\n","Epoch [392/500], Loss: 0.30115529894828796, Train Acc: 0.8952,train F1-score:0.8914 Val Loss: 0.3391733169555664, Val Acc: 0.9009\n","Epoch [393/500], Loss: 0.2864542305469513, Train Acc: 0.8968,train F1-score:0.8932 Val Loss: 0.33965209126472473, Val Acc: 0.9023\n","Epoch [394/500], Loss: 0.28697624802589417, Train Acc: 0.9015,train F1-score:0.8979 Val Loss: 0.34136927127838135, Val Acc: 0.8989\n","Epoch [395/500], Loss: 0.2912292778491974, Train Acc: 0.8991,train F1-score:0.8956 Val Loss: 0.3408289849758148, Val Acc: 0.8976\n","Epoch [396/500], Loss: 0.2826805114746094, Train Acc: 0.8973,train F1-score:0.8935 Val Loss: 0.34020698070526123, Val Acc: 0.8949\n","Epoch [397/500], Loss: 0.28766193985939026, Train Acc: 0.8977,train F1-score:0.8944 Val Loss: 0.34068217873573303, Val Acc: 0.8976\n","Epoch [398/500], Loss: 0.2891429364681244, Train Acc: 0.8990,train F1-score:0.8956 Val Loss: 0.3431790769100189, Val Acc: 0.8963\n","Epoch [399/500], Loss: 0.2874244451522827, Train Acc: 0.8993,train F1-score:0.8957 Val Loss: 0.3439260423183441, Val Acc: 0.8976\n","Epoch [400/500], Loss: 0.2894969582557678, Train Acc: 0.8982,train F1-score:0.8950 Val Loss: 0.34385257959365845, Val Acc: 0.8996\n","Epoch [401/500], Loss: 0.27635353803634644, Train Acc: 0.9009,train F1-score:0.8980 Val Loss: 0.3445672392845154, Val Acc: 0.8969\n","Epoch [402/500], Loss: 0.28225696086883545, Train Acc: 0.8997,train F1-score:0.8968 Val Loss: 0.3461778461933136, Val Acc: 0.9036\n","Epoch [403/500], Loss: 0.2803416848182678, Train Acc: 0.9013,train F1-score:0.8977 Val Loss: 0.34743788838386536, Val Acc: 0.9016\n","Epoch [404/500], Loss: 0.2857986092567444, Train Acc: 0.8989,train F1-score:0.8948 Val Loss: 0.34784257411956787, Val Acc: 0.9029\n","Epoch [405/500], Loss: 0.27795976400375366, Train Acc: 0.8995,train F1-score:0.8962 Val Loss: 0.3499965965747833, Val Acc: 0.9023\n","Epoch [406/500], Loss: 0.2763366103172302, Train Acc: 0.9009,train F1-score:0.8985 Val Loss: 0.35146722197532654, Val Acc: 0.9023\n","Epoch [407/500], Loss: 0.2685740888118744, Train Acc: 0.9073,train F1-score:0.9042 Val Loss: 0.3512987196445465, Val Acc: 0.8963\n","Epoch [408/500], Loss: 0.2777046263217926, Train Acc: 0.9002,train F1-score:0.8967 Val Loss: 0.3505667448043823, Val Acc: 0.8942\n","Epoch [409/500], Loss: 0.28307944536209106, Train Acc: 0.9004,train F1-score:0.8967 Val Loss: 0.3487568497657776, Val Acc: 0.8969\n","Epoch [410/500], Loss: 0.27893608808517456, Train Acc: 0.9001,train F1-score:0.8976 Val Loss: 0.34815895557403564, Val Acc: 0.9009\n","Epoch [411/500], Loss: 0.2794504761695862, Train Acc: 0.9033,train F1-score:0.9009 Val Loss: 0.3484843075275421, Val Acc: 0.9029\n","Epoch [412/500], Loss: 0.2659660279750824, Train Acc: 0.9048,train F1-score:0.9021 Val Loss: 0.3475084900856018, Val Acc: 0.9023\n","Epoch [413/500], Loss: 0.2810913622379303, Train Acc: 0.8993,train F1-score:0.8956 Val Loss: 0.3440505862236023, Val Acc: 0.9016\n","Epoch [414/500], Loss: 0.2766955494880676, Train Acc: 0.8997,train F1-score:0.8963 Val Loss: 0.33810773491859436, Val Acc: 0.9029\n","Epoch [415/500], Loss: 0.27639535069465637, Train Acc: 0.9023,train F1-score:0.8994 Val Loss: 0.3351345956325531, Val Acc: 0.9023\n","Epoch [416/500], Loss: 0.2776080369949341, Train Acc: 0.9024,train F1-score:0.9001 Val Loss: 0.336772620677948, Val Acc: 0.9016\n","Epoch [417/500], Loss: 0.27419933676719666, Train Acc: 0.9031,train F1-score:0.8999 Val Loss: 0.33943480253219604, Val Acc: 0.9016\n","Epoch [418/500], Loss: 0.27205583453178406, Train Acc: 0.9012,train F1-score:0.8977 Val Loss: 0.3378368020057678, Val Acc: 0.9036\n","Epoch [419/500], Loss: 0.2765877842903137, Train Acc: 0.9003,train F1-score:0.8971 Val Loss: 0.3381267189979553, Val Acc: 0.9036\n","Epoch [420/500], Loss: 0.2735276222229004, Train Acc: 0.9040,train F1-score:0.9010 Val Loss: 0.33643853664398193, Val Acc: 0.9036\n","Epoch [421/500], Loss: 0.2775717079639435, Train Acc: 0.9020,train F1-score:0.8987 Val Loss: 0.3351064920425415, Val Acc: 0.9043\n","Epoch [422/500], Loss: 0.2667010724544525, Train Acc: 0.9060,train F1-score:0.9029 Val Loss: 0.33314579725265503, Val Acc: 0.9023\n","Epoch [423/500], Loss: 0.2829594016075134, Train Acc: 0.9039,train F1-score:0.9010 Val Loss: 0.3330073356628418, Val Acc: 0.9043\n","Epoch [424/500], Loss: 0.2715678811073303, Train Acc: 0.9058,train F1-score:0.9031 Val Loss: 0.33530691266059875, Val Acc: 0.9043\n","Epoch [425/500], Loss: 0.27061009407043457, Train Acc: 0.9037,train F1-score:0.9008 Val Loss: 0.33669590950012207, Val Acc: 0.9023\n","Epoch [426/500], Loss: 0.2740928828716278, Train Acc: 0.9032,train F1-score:0.8999 Val Loss: 0.33592498302459717, Val Acc: 0.8996\n","Epoch [427/500], Loss: 0.27055713534355164, Train Acc: 0.9030,train F1-score:0.9004 Val Loss: 0.3354443609714508, Val Acc: 0.8969\n","Epoch [428/500], Loss: 0.2647160291671753, Train Acc: 0.9060,train F1-score:0.9031 Val Loss: 0.33510738611221313, Val Acc: 0.8989\n","Epoch [429/500], Loss: 0.2675165832042694, Train Acc: 0.9040,train F1-score:0.9013 Val Loss: 0.3375645875930786, Val Acc: 0.8983\n","Epoch [430/500], Loss: 0.27456727623939514, Train Acc: 0.9045,train F1-score:0.9021 Val Loss: 0.33867138624191284, Val Acc: 0.9023\n","Epoch [431/500], Loss: 0.2725786864757538, Train Acc: 0.9050,train F1-score:0.9019 Val Loss: 0.33625689148902893, Val Acc: 0.9043\n","Epoch [432/500], Loss: 0.2688924968242645, Train Acc: 0.9055,train F1-score:0.9024 Val Loss: 0.3354562520980835, Val Acc: 0.9043\n","Epoch [433/500], Loss: 0.26657915115356445, Train Acc: 0.9055,train F1-score:0.9026 Val Loss: 0.33608192205429077, Val Acc: 0.9043\n","Epoch [434/500], Loss: 0.2622006833553314, Train Acc: 0.9042,train F1-score:0.9016 Val Loss: 0.33694392442703247, Val Acc: 0.9016\n","Epoch [435/500], Loss: 0.27588722109794617, Train Acc: 0.9036,train F1-score:0.9001 Val Loss: 0.3367108106613159, Val Acc: 0.8983\n","Epoch [436/500], Loss: 0.2591566741466522, Train Acc: 0.9088,train F1-score:0.9062 Val Loss: 0.3351852297782898, Val Acc: 0.8983\n","Epoch [437/500], Loss: 0.26604828238487244, Train Acc: 0.9031,train F1-score:0.9003 Val Loss: 0.34022057056427, Val Acc: 0.8969\n","Epoch [438/500], Loss: 0.273940771818161, Train Acc: 0.9041,train F1-score:0.9013 Val Loss: 0.3435436487197876, Val Acc: 0.8969\n","Epoch [439/500], Loss: 0.27218952775001526, Train Acc: 0.9032,train F1-score:0.9002 Val Loss: 0.34606900811195374, Val Acc: 0.9003\n","Epoch [440/500], Loss: 0.2669409513473511, Train Acc: 0.9060,train F1-score:0.9037 Val Loss: 0.34529295563697815, Val Acc: 0.9003\n","Epoch [441/500], Loss: 0.28002098202705383, Train Acc: 0.8996,train F1-score:0.8963 Val Loss: 0.3448580801486969, Val Acc: 0.9029\n","Epoch [442/500], Loss: 0.264644980430603, Train Acc: 0.9065,train F1-score:0.9039 Val Loss: 0.3463476300239563, Val Acc: 0.9023\n","Epoch [443/500], Loss: 0.272041380405426, Train Acc: 0.9030,train F1-score:0.9003 Val Loss: 0.3446832299232483, Val Acc: 0.9036\n","Epoch [444/500], Loss: 0.2680300772190094, Train Acc: 0.9027,train F1-score:0.9001 Val Loss: 0.3456421196460724, Val Acc: 0.9023\n","Epoch [445/500], Loss: 0.27297380566596985, Train Acc: 0.9045,train F1-score:0.9014 Val Loss: 0.34736475348472595, Val Acc: 0.9023\n","Epoch [446/500], Loss: 0.2776803970336914, Train Acc: 0.9075,train F1-score:0.9046 Val Loss: 0.3469817042350769, Val Acc: 0.9043\n","Epoch [447/500], Loss: 0.2709762156009674, Train Acc: 0.9039,train F1-score:0.9010 Val Loss: 0.34686508774757385, Val Acc: 0.9063\n","Epoch [448/500], Loss: 0.26068973541259766, Train Acc: 0.9076,train F1-score:0.9051 Val Loss: 0.34862589836120605, Val Acc: 0.9063\n","Epoch [449/500], Loss: 0.2650773823261261, Train Acc: 0.9040,train F1-score:0.9012 Val Loss: 0.3505730628967285, Val Acc: 0.9036\n","Epoch [450/500], Loss: 0.2650323808193207, Train Acc: 0.9045,train F1-score:0.9014 Val Loss: 0.348082959651947, Val Acc: 0.9043\n","Epoch [451/500], Loss: 0.2638108432292938, Train Acc: 0.9087,train F1-score:0.9059 Val Loss: 0.3461037278175354, Val Acc: 0.9050\n","Epoch [452/500], Loss: 0.2654837965965271, Train Acc: 0.9051,train F1-score:0.9029 Val Loss: 0.3461536169052124, Val Acc: 0.9043\n","Epoch [453/500], Loss: 0.27707919478416443, Train Acc: 0.9025,train F1-score:0.8998 Val Loss: 0.3452080488204956, Val Acc: 0.9050\n","Epoch [454/500], Loss: 0.26650184392929077, Train Acc: 0.9049,train F1-score:0.9016 Val Loss: 0.34348592162132263, Val Acc: 0.9009\n","Early stopping at epoch 454\n","Test Loss: 0.35757261514663696, Test Accuracy: 0.9121983914209115\n","Precision: 0.9079, Recall: 0.9122, F1-score: 0.9093\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GATNet(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_dim, out_channels, num_layers):\n","        super(GATNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GATConv(in_channels, hidden_dim, heads=8))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GATConv(hidden_dim * 8, hidden_dim, heads=8))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim * 8, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.fc(x)\n","        return x\n","\n","# Define model\n","model = GATNet(in_channels=X_train.shape[1], hidden_dim=32, out_channels=13, num_layers=2).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/train_f1_scores_GAT.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/train_loss_GAT.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/epochs_GAT.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/f1_score_GAT.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":654,"status":"ok","timestamp":1714732505987,"user":{"displayName":"Dhilja R","userId":"09907317500965682202"},"user_tz":-330},"id":"sZJciy2xhiKJ","outputId":"3596a07c-5612-4d8d-e0f4-a6baaaf361f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.934851783620834\n","Validation Accuracy (Decision Tree): 0.8969210174029452\n","Test Accuracy (Decision Tree): 0.8947721179624665\n","Precision: 0.8935, Recall: 0.8948, F1-score: 0.8931\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=8)\n","decision_tree.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(train_outputs)\n","val_pred = decision_tree.predict(val_outputs)\n","test_pred = decision_tree.predict(test_outputs)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","# Save the F1-score to a text file\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GAT_DT.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1914,"status":"ok","timestamp":1714732510397,"user":{"displayName":"Dhilja R","userId":"09907317500965682202"},"user_tz":-330},"id":"bn2qG0LHhwUN","outputId":"383ad10b-cb29-4f45-aeef-878abfb8e925"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.9266454530229442\n","Validation Accuracy (Decision Tree): 0.893574297188755\n","Test Accuracy (Decision Tree): 0.9001340482573726\n","Precision: 0.8961, Recall: 0.9001, F1-score: 0.8960\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Concatenate the GIN model output with the original feature matrices\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=7)\n","decision_tree.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(X_train_combined)\n","val_pred = decision_tree.predict(X_val_combined)\n","test_pred = decision_tree.predict(X_test_combined)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GAT_DT_comb.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3540,"status":"ok","timestamp":1714732556999,"user":{"displayName":"Dhilja R","userId":"09907317500965682202"},"user_tz":-330},"id":"BDVZuO2mimyx","outputId":"a656080a-9379-4973-9d03-ec1cb287309a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9418020432088428\n","Validation Accuracy (SVM): 0.9049531459170014\n","Test Accuracy (SVM): 0.9101876675603218\n","Precision: 0.9104, Recall: 0.9102, F1-score: 0.9099\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["F1-score saved to file.\n"]}],"source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(train_outputs)\n","val_pred = svm_classifier.predict(val_outputs)\n","test_pred = svm_classifier.predict(test_outputs)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GAT_SVM.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"GAbAq_fciw4W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714732586060,"user_tz":-330,"elapsed":9329,"user":{"displayName":"Dhilja R","userId":"09907317500965682202"}},"outputId":"a3fd7ca2-17c7-4421-c107-f0e037529d45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9518506112878915\n","Validation Accuracy (SVM): 0.9022757697456493\n","Test Accuracy (SVM): 0.9189008042895442\n","Precision: 0.9184, Recall: 0.9189, F1-score: 0.9182\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["F1-score saved to file.\n"]}],"source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Combine the output of the GIN model with the original features\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(X_train_combined)\n","val_pred = svm_classifier.predict(X_val_combined)\n","test_pred = svm_classifier.predict(X_test_combined)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GAT_SVM_COMB.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GATNet(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_dim, out_channels, num_layers):\n","        super(GATNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GATConv(in_channels, hidden_dim, heads=8))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GATConv(hidden_dim * 8, hidden_dim, heads=8))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim * 8, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.fc(x)\n","        return x\n","\n","# Define model\n","model = GATNet(in_channels=X_train.shape[1], hidden_dim=8, out_channels=13, num_layers=2).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model_23.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model_23.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","\n","\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model_23.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Combine the output of the GIN model with the original features\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(X_train_combined)\n","val_pred = svm_classifier.predict(X_val_combined)\n","test_pred = svm_classifier.predict(X_test_combined)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYT8mmItcl7S","outputId":"6d66b644-6c3a-40fc-9b36-c1605bbfd7da","executionInfo":{"status":"ok","timestamp":1714726683753,"user_tz":-330,"elapsed":27432,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Epoch [1/500], Loss: 2.8108060359954834, Train Acc: 0.0785,train F1-score:0.0983 Val Loss: 2.0881409645080566, Val Acc: 0.6720\n","Epoch [2/500], Loss: 2.15169095993042, Train Acc: 0.4686,train F1-score:0.4588 Val Loss: 1.6338529586791992, Val Acc: 0.6787\n","Epoch [3/500], Loss: 1.7400093078613281, Train Acc: 0.6223,train F1-score:0.5139 Val Loss: 1.273297905921936, Val Acc: 0.6787\n","Epoch [4/500], Loss: 1.4924780130386353, Train Acc: 0.6389,train F1-score:0.5069 Val Loss: 1.1374655961990356, Val Acc: 0.6787\n","Epoch [5/500], Loss: 1.4105299711227417, Train Acc: 0.6429,train F1-score:0.5075 Val Loss: 1.0888171195983887, Val Acc: 0.6787\n","Epoch [6/500], Loss: 1.3879661560058594, Train Acc: 0.6419,train F1-score:0.5074 Val Loss: 1.017858862876892, Val Acc: 0.6787\n","Epoch [7/500], Loss: 1.2867190837860107, Train Acc: 0.6408,train F1-score:0.5168 Val Loss: 0.984176516532898, Val Acc: 0.6794\n","Epoch [8/500], Loss: 1.2316173315048218, Train Acc: 0.6214,train F1-score:0.5355 Val Loss: 0.9970557689666748, Val Acc: 0.7229\n","Epoch [9/500], Loss: 1.2330149412155151, Train Acc: 0.5955,train F1-score:0.5496 Val Loss: 0.9851911067962646, Val Acc: 0.7155\n","Epoch [10/500], Loss: 1.197916865348816, Train Acc: 0.5991,train F1-score:0.5592 Val Loss: 0.9412099719047546, Val Acc: 0.7075\n","Epoch [11/500], Loss: 1.1466457843780518, Train Acc: 0.6208,train F1-score:0.5639 Val Loss: 0.9018466472625732, Val Acc: 0.7142\n","Epoch [12/500], Loss: 1.1087779998779297, Train Acc: 0.6447,train F1-score:0.5616 Val Loss: 0.884962797164917, Val Acc: 0.7149\n","Epoch [13/500], Loss: 1.0993938446044922, Train Acc: 0.6510,train F1-score:0.5501 Val Loss: 0.8801286816596985, Val Acc: 0.7055\n","Epoch [14/500], Loss: 1.0878411531448364, Train Acc: 0.6532,train F1-score:0.5432 Val Loss: 0.8721126317977905, Val Acc: 0.6928\n","Epoch [15/500], Loss: 1.0838706493377686, Train Acc: 0.6534,train F1-score:0.5435 Val Loss: 0.8593902587890625, Val Acc: 0.7055\n","Epoch [16/500], Loss: 1.0543640851974487, Train Acc: 0.6557,train F1-score:0.5536 Val Loss: 0.8495727181434631, Val Acc: 0.7249\n","Epoch [17/500], Loss: 1.0420756340026855, Train Acc: 0.6614,train F1-score:0.5692 Val Loss: 0.8447745442390442, Val Acc: 0.7316\n","Epoch [18/500], Loss: 1.027642011642456, Train Acc: 0.6607,train F1-score:0.5795 Val Loss: 0.8377376794815063, Val Acc: 0.7390\n","Epoch [19/500], Loss: 1.0086253881454468, Train Acc: 0.6610,train F1-score:0.5868 Val Loss: 0.8248454332351685, Val Acc: 0.7470\n","Epoch [20/500], Loss: 1.0083065032958984, Train Acc: 0.6625,train F1-score:0.5902 Val Loss: 0.8126723766326904, Val Acc: 0.7483\n","Epoch [21/500], Loss: 0.998218834400177, Train Acc: 0.6700,train F1-score:0.5945 Val Loss: 0.8036922812461853, Val Acc: 0.7396\n","Epoch [22/500], Loss: 0.9888420104980469, Train Acc: 0.6740,train F1-score:0.5941 Val Loss: 0.7968395948410034, Val Acc: 0.7356\n","Epoch [23/500], Loss: 0.9791461825370789, Train Acc: 0.6740,train F1-score:0.5896 Val Loss: 0.7893875241279602, Val Acc: 0.7349\n","Epoch [24/500], Loss: 0.9761316776275635, Train Acc: 0.6746,train F1-score:0.5883 Val Loss: 0.7804083228111267, Val Acc: 0.7396\n","Epoch [25/500], Loss: 0.9650547504425049, Train Acc: 0.6758,train F1-score:0.5926 Val Loss: 0.7733744382858276, Val Acc: 0.7503\n","Epoch [26/500], Loss: 0.9519840478897095, Train Acc: 0.6769,train F1-score:0.5973 Val Loss: 0.7708825469017029, Val Acc: 0.7570\n","Epoch [27/500], Loss: 0.9459665417671204, Train Acc: 0.6794,train F1-score:0.6047 Val Loss: 0.7697980999946594, Val Acc: 0.7550\n","Epoch [28/500], Loss: 0.9463138580322266, Train Acc: 0.6799,train F1-score:0.6115 Val Loss: 0.7646738290786743, Val Acc: 0.7604\n","Epoch [29/500], Loss: 0.9308461546897888, Train Acc: 0.6853,train F1-score:0.6173 Val Loss: 0.75706946849823, Val Acc: 0.7604\n","Epoch [30/500], Loss: 0.9274100661277771, Train Acc: 0.6886,train F1-score:0.6193 Val Loss: 0.7503089308738708, Val Acc: 0.7557\n","Epoch [31/500], Loss: 0.9190875291824341, Train Acc: 0.6908,train F1-score:0.6223 Val Loss: 0.7443788647651672, Val Acc: 0.7557\n","Epoch [32/500], Loss: 0.9140418171882629, Train Acc: 0.6961,train F1-score:0.6264 Val Loss: 0.7368107438087463, Val Acc: 0.7550\n","Epoch [33/500], Loss: 0.9073941111564636, Train Acc: 0.6969,train F1-score:0.6297 Val Loss: 0.7269047498703003, Val Acc: 0.7564\n","Epoch [34/500], Loss: 0.903440535068512, Train Acc: 0.6961,train F1-score:0.6307 Val Loss: 0.7180660963058472, Val Acc: 0.7597\n","Epoch [35/500], Loss: 0.8933263421058655, Train Acc: 0.7009,train F1-score:0.6392 Val Loss: 0.711424708366394, Val Acc: 0.7644\n","Epoch [36/500], Loss: 0.8956966996192932, Train Acc: 0.7020,train F1-score:0.6425 Val Loss: 0.7061724066734314, Val Acc: 0.7684\n","Epoch [37/500], Loss: 0.8804975152015686, Train Acc: 0.7057,train F1-score:0.6490 Val Loss: 0.700663149356842, Val Acc: 0.7718\n","Epoch [38/500], Loss: 0.868099570274353, Train Acc: 0.7105,train F1-score:0.6561 Val Loss: 0.6952614188194275, Val Acc: 0.7731\n","Epoch [39/500], Loss: 0.8627318143844604, Train Acc: 0.7094,train F1-score:0.6563 Val Loss: 0.6907921433448792, Val Acc: 0.7771\n","Epoch [40/500], Loss: 0.8588001132011414, Train Acc: 0.7148,train F1-score:0.6614 Val Loss: 0.6861640810966492, Val Acc: 0.7784\n","Epoch [41/500], Loss: 0.8580203652381897, Train Acc: 0.7141,train F1-score:0.6643 Val Loss: 0.6806980967521667, Val Acc: 0.7831\n","Epoch [42/500], Loss: 0.8572724461555481, Train Acc: 0.7181,train F1-score:0.6706 Val Loss: 0.6748158931732178, Val Acc: 0.7918\n","Epoch [43/500], Loss: 0.8407391309738159, Train Acc: 0.7212,train F1-score:0.6754 Val Loss: 0.6690697073936462, Val Acc: 0.7985\n","Epoch [44/500], Loss: 0.8432905673980713, Train Acc: 0.7241,train F1-score:0.6819 Val Loss: 0.6638555526733398, Val Acc: 0.7985\n","Epoch [45/500], Loss: 0.8294097185134888, Train Acc: 0.7215,train F1-score:0.6802 Val Loss: 0.6591594815254211, Val Acc: 0.7999\n","Epoch [46/500], Loss: 0.8272479772567749, Train Acc: 0.7267,train F1-score:0.6844 Val Loss: 0.6552857160568237, Val Acc: 0.7992\n","Epoch [47/500], Loss: 0.8296687602996826, Train Acc: 0.7237,train F1-score:0.6821 Val Loss: 0.6523715257644653, Val Acc: 0.7999\n","Epoch [48/500], Loss: 0.8201867341995239, Train Acc: 0.7283,train F1-score:0.6846 Val Loss: 0.6497107148170471, Val Acc: 0.8012\n","Epoch [49/500], Loss: 0.8057624101638794, Train Acc: 0.7296,train F1-score:0.6854 Val Loss: 0.6458646059036255, Val Acc: 0.8039\n","Epoch [50/500], Loss: 0.8187434077262878, Train Acc: 0.7320,train F1-score:0.6906 Val Loss: 0.6413851380348206, Val Acc: 0.8072\n","Epoch [51/500], Loss: 0.8024201393127441, Train Acc: 0.7340,train F1-score:0.6947 Val Loss: 0.6366280317306519, Val Acc: 0.8072\n","Epoch [52/500], Loss: 0.796088457107544, Train Acc: 0.7375,train F1-score:0.7001 Val Loss: 0.6319698691368103, Val Acc: 0.8072\n","Epoch [53/500], Loss: 0.8003035187721252, Train Acc: 0.7346,train F1-score:0.6986 Val Loss: 0.6285166144371033, Val Acc: 0.8072\n","Epoch [54/500], Loss: 0.7965722680091858, Train Acc: 0.7374,train F1-score:0.7016 Val Loss: 0.6260011792182922, Val Acc: 0.8086\n","Epoch [55/500], Loss: 0.7917253375053406, Train Acc: 0.7365,train F1-score:0.6997 Val Loss: 0.6243109703063965, Val Acc: 0.8099\n","Epoch [56/500], Loss: 0.7812449336051941, Train Acc: 0.7387,train F1-score:0.7015 Val Loss: 0.6223779916763306, Val Acc: 0.8106\n","Epoch [57/500], Loss: 0.7797096967697144, Train Acc: 0.7441,train F1-score:0.7082 Val Loss: 0.6196010708808899, Val Acc: 0.8099\n","Epoch [58/500], Loss: 0.7728591561317444, Train Acc: 0.7449,train F1-score:0.7104 Val Loss: 0.6162166595458984, Val Acc: 0.8092\n","Epoch [59/500], Loss: 0.7759217619895935, Train Acc: 0.7411,train F1-score:0.7064 Val Loss: 0.6130063533782959, Val Acc: 0.8086\n","Epoch [60/500], Loss: 0.7752232551574707, Train Acc: 0.7448,train F1-score:0.7119 Val Loss: 0.6098260879516602, Val Acc: 0.8106\n","Epoch [61/500], Loss: 0.760983943939209, Train Acc: 0.7457,train F1-score:0.7128 Val Loss: 0.6074435710906982, Val Acc: 0.8126\n","Epoch [62/500], Loss: 0.7638973593711853, Train Acc: 0.7503,train F1-score:0.7177 Val Loss: 0.6053444147109985, Val Acc: 0.8126\n","Epoch [63/500], Loss: 0.7625146508216858, Train Acc: 0.7487,train F1-score:0.7153 Val Loss: 0.6032085418701172, Val Acc: 0.8146\n","Epoch [64/500], Loss: 0.751837968826294, Train Acc: 0.7516,train F1-score:0.7204 Val Loss: 0.6010705232620239, Val Acc: 0.8186\n","Epoch [65/500], Loss: 0.7534649968147278, Train Acc: 0.7541,train F1-score:0.7238 Val Loss: 0.5989248156547546, Val Acc: 0.8186\n","Epoch [66/500], Loss: 0.7416340708732605, Train Acc: 0.7543,train F1-score:0.7257 Val Loss: 0.597259521484375, Val Acc: 0.8193\n","Epoch [67/500], Loss: 0.750817596912384, Train Acc: 0.7551,train F1-score:0.7269 Val Loss: 0.5953041315078735, Val Acc: 0.8199\n","Epoch [68/500], Loss: 0.7488049268722534, Train Acc: 0.7548,train F1-score:0.7252 Val Loss: 0.5931081175804138, Val Acc: 0.8213\n","Epoch [69/500], Loss: 0.7402060627937317, Train Acc: 0.7565,train F1-score:0.7273 Val Loss: 0.5909721255302429, Val Acc: 0.8199\n","Epoch [70/500], Loss: 0.7366203665733337, Train Acc: 0.7582,train F1-score:0.7291 Val Loss: 0.58829665184021, Val Acc: 0.8193\n","Epoch [71/500], Loss: 0.7406012415885925, Train Acc: 0.7553,train F1-score:0.7255 Val Loss: 0.5862574577331543, Val Acc: 0.8193\n","Epoch [72/500], Loss: 0.7333987355232239, Train Acc: 0.7619,train F1-score:0.7332 Val Loss: 0.58449786901474, Val Acc: 0.8213\n","Epoch [73/500], Loss: 0.7272176742553711, Train Acc: 0.7608,train F1-score:0.7334 Val Loss: 0.5827313661575317, Val Acc: 0.8233\n","Epoch [74/500], Loss: 0.7244483828544617, Train Acc: 0.7610,train F1-score:0.7331 Val Loss: 0.5808976292610168, Val Acc: 0.8266\n","Epoch [75/500], Loss: 0.7274267077445984, Train Acc: 0.7626,train F1-score:0.7361 Val Loss: 0.5792954564094543, Val Acc: 0.8273\n","Epoch [76/500], Loss: 0.7164244055747986, Train Acc: 0.7665,train F1-score:0.7400 Val Loss: 0.5767648816108704, Val Acc: 0.8280\n","Epoch [77/500], Loss: 0.722612738609314, Train Acc: 0.7645,train F1-score:0.7387 Val Loss: 0.5741555690765381, Val Acc: 0.8293\n","Epoch [78/500], Loss: 0.7217401266098022, Train Acc: 0.7648,train F1-score:0.7386 Val Loss: 0.5727089643478394, Val Acc: 0.8286\n","Epoch [79/500], Loss: 0.7170034646987915, Train Acc: 0.7665,train F1-score:0.7403 Val Loss: 0.5717037916183472, Val Acc: 0.8280\n","Epoch [80/500], Loss: 0.71194988489151, Train Acc: 0.7673,train F1-score:0.7415 Val Loss: 0.5706383585929871, Val Acc: 0.8280\n","Epoch [81/500], Loss: 0.7116913199424744, Train Acc: 0.7639,train F1-score:0.7374 Val Loss: 0.5699487924575806, Val Acc: 0.8300\n","Epoch [82/500], Loss: 0.7109776735305786, Train Acc: 0.7618,train F1-score:0.7350 Val Loss: 0.5687422156333923, Val Acc: 0.8300\n","Epoch [83/500], Loss: 0.7111942768096924, Train Acc: 0.7648,train F1-score:0.7393 Val Loss: 0.5662760138511658, Val Acc: 0.8300\n","Epoch [84/500], Loss: 0.7037436366081238, Train Acc: 0.7689,train F1-score:0.7436 Val Loss: 0.5629056096076965, Val Acc: 0.8307\n","Epoch [85/500], Loss: 0.7035393714904785, Train Acc: 0.7691,train F1-score:0.7449 Val Loss: 0.5601528286933899, Val Acc: 0.8307\n","Epoch [86/500], Loss: 0.6965434551239014, Train Acc: 0.7696,train F1-score:0.7454 Val Loss: 0.5572540760040283, Val Acc: 0.8347\n","Epoch [87/500], Loss: 0.69468754529953, Train Acc: 0.7689,train F1-score:0.7438 Val Loss: 0.5561731457710266, Val Acc: 0.8367\n","Epoch [88/500], Loss: 0.6928038001060486, Train Acc: 0.7716,train F1-score:0.7474 Val Loss: 0.5551866292953491, Val Acc: 0.8380\n","Epoch [89/500], Loss: 0.688257098197937, Train Acc: 0.7738,train F1-score:0.7509 Val Loss: 0.5542623996734619, Val Acc: 0.8373\n","Epoch [90/500], Loss: 0.6891807317733765, Train Acc: 0.7749,train F1-score:0.7516 Val Loss: 0.5524366497993469, Val Acc: 0.8360\n","Epoch [91/500], Loss: 0.6858552098274231, Train Acc: 0.7739,train F1-score:0.7489 Val Loss: 0.55006343126297, Val Acc: 0.8347\n","Epoch [92/500], Loss: 0.688721239566803, Train Acc: 0.7737,train F1-score:0.7501 Val Loss: 0.5480364561080933, Val Acc: 0.8353\n","Epoch [93/500], Loss: 0.6804396510124207, Train Acc: 0.7784,train F1-score:0.7560 Val Loss: 0.5463618040084839, Val Acc: 0.8327\n","Epoch [94/500], Loss: 0.677444577217102, Train Acc: 0.7795,train F1-score:0.7583 Val Loss: 0.5458484292030334, Val Acc: 0.8320\n","Epoch [95/500], Loss: 0.6848735809326172, Train Acc: 0.7747,train F1-score:0.7527 Val Loss: 0.5456504821777344, Val Acc: 0.8340\n","Epoch [96/500], Loss: 0.6830524206161499, Train Acc: 0.7750,train F1-score:0.7512 Val Loss: 0.5447494983673096, Val Acc: 0.8360\n","Epoch [97/500], Loss: 0.6808324456214905, Train Acc: 0.7756,train F1-score:0.7521 Val Loss: 0.544346034526825, Val Acc: 0.8394\n","Epoch [98/500], Loss: 0.6871128678321838, Train Acc: 0.7770,train F1-score:0.7533 Val Loss: 0.5428362488746643, Val Acc: 0.8394\n","Epoch [99/500], Loss: 0.678399384021759, Train Acc: 0.7819,train F1-score:0.7602 Val Loss: 0.5408676862716675, Val Acc: 0.8387\n","Epoch [100/500], Loss: 0.6702059507369995, Train Acc: 0.7794,train F1-score:0.7574 Val Loss: 0.539607048034668, Val Acc: 0.8380\n","Epoch [101/500], Loss: 0.6730108857154846, Train Acc: 0.7804,train F1-score:0.7582 Val Loss: 0.5367774963378906, Val Acc: 0.8400\n","Epoch [102/500], Loss: 0.6731910109519958, Train Acc: 0.7785,train F1-score:0.7552 Val Loss: 0.532311737537384, Val Acc: 0.8414\n","Epoch [103/500], Loss: 0.6638955473899841, Train Acc: 0.7852,train F1-score:0.7637 Val Loss: 0.5291727185249329, Val Acc: 0.8414\n","Epoch [104/500], Loss: 0.6631203889846802, Train Acc: 0.7819,train F1-score:0.7610 Val Loss: 0.52751624584198, Val Acc: 0.8427\n","Epoch [105/500], Loss: 0.6698784828186035, Train Acc: 0.7796,train F1-score:0.7586 Val Loss: 0.5272870659828186, Val Acc: 0.8427\n","Epoch [106/500], Loss: 0.6681080460548401, Train Acc: 0.7809,train F1-score:0.7602 Val Loss: 0.5279664993286133, Val Acc: 0.8400\n","Epoch [107/500], Loss: 0.6561736464500427, Train Acc: 0.7848,train F1-score:0.7636 Val Loss: 0.5281664729118347, Val Acc: 0.8394\n","Epoch [108/500], Loss: 0.6568331718444824, Train Acc: 0.7856,train F1-score:0.7642 Val Loss: 0.5280511975288391, Val Acc: 0.8394\n","Epoch [109/500], Loss: 0.6679728031158447, Train Acc: 0.7836,train F1-score:0.7621 Val Loss: 0.5264354944229126, Val Acc: 0.8380\n","Epoch [110/500], Loss: 0.6603460311889648, Train Acc: 0.7847,train F1-score:0.7632 Val Loss: 0.5248053669929504, Val Acc: 0.8394\n","Epoch [111/500], Loss: 0.6523142457008362, Train Acc: 0.7881,train F1-score:0.7673 Val Loss: 0.5239986181259155, Val Acc: 0.8407\n","Epoch [112/500], Loss: 0.6543629765510559, Train Acc: 0.7854,train F1-score:0.7642 Val Loss: 0.5243359804153442, Val Acc: 0.8407\n","Epoch [113/500], Loss: 0.6477471590042114, Train Acc: 0.7885,train F1-score:0.7670 Val Loss: 0.5228440761566162, Val Acc: 0.8400\n","Epoch [114/500], Loss: 0.6536615490913391, Train Acc: 0.7864,train F1-score:0.7661 Val Loss: 0.5199466347694397, Val Acc: 0.8400\n","Epoch [115/500], Loss: 0.6408624649047852, Train Acc: 0.7886,train F1-score:0.7677 Val Loss: 0.5163102149963379, Val Acc: 0.8440\n","Epoch [116/500], Loss: 0.6494265198707581, Train Acc: 0.7885,train F1-score:0.7678 Val Loss: 0.5134744644165039, Val Acc: 0.8420\n","Epoch [117/500], Loss: 0.6547073125839233, Train Acc: 0.7888,train F1-score:0.7693 Val Loss: 0.5117112398147583, Val Acc: 0.8427\n","Epoch [118/500], Loss: 0.6519259810447693, Train Acc: 0.7907,train F1-score:0.7710 Val Loss: 0.5110184550285339, Val Acc: 0.8447\n","Epoch [119/500], Loss: 0.6522684097290039, Train Acc: 0.7879,train F1-score:0.7699 Val Loss: 0.5111925005912781, Val Acc: 0.8440\n","Epoch [120/500], Loss: 0.6454071998596191, Train Acc: 0.7912,train F1-score:0.7728 Val Loss: 0.5121661424636841, Val Acc: 0.8440\n","Epoch [121/500], Loss: 0.6375603675842285, Train Acc: 0.7908,train F1-score:0.7701 Val Loss: 0.5131444334983826, Val Acc: 0.8414\n","Epoch [122/500], Loss: 0.6403720378875732, Train Acc: 0.7868,train F1-score:0.7653 Val Loss: 0.5146054625511169, Val Acc: 0.8440\n","Epoch [123/500], Loss: 0.6380927562713623, Train Acc: 0.7893,train F1-score:0.7681 Val Loss: 0.5146657824516296, Val Acc: 0.8427\n","Epoch [124/500], Loss: 0.6405775547027588, Train Acc: 0.7903,train F1-score:0.7698 Val Loss: 0.5120013952255249, Val Acc: 0.8420\n","Epoch [125/500], Loss: 0.6373466849327087, Train Acc: 0.7903,train F1-score:0.7705 Val Loss: 0.5084822177886963, Val Acc: 0.8447\n","Epoch [126/500], Loss: 0.6349648237228394, Train Acc: 0.7888,train F1-score:0.7701 Val Loss: 0.5056744813919067, Val Acc: 0.8467\n","Epoch [127/500], Loss: 0.6299173831939697, Train Acc: 0.7946,train F1-score:0.7755 Val Loss: 0.50461745262146, Val Acc: 0.8447\n","Epoch [128/500], Loss: 0.63218092918396, Train Acc: 0.7965,train F1-score:0.7771 Val Loss: 0.5042599439620972, Val Acc: 0.8454\n","Epoch [129/500], Loss: 0.6297966837882996, Train Acc: 0.7970,train F1-score:0.7793 Val Loss: 0.5057896971702576, Val Acc: 0.8461\n","Epoch [130/500], Loss: 0.6358466744422913, Train Acc: 0.7920,train F1-score:0.7733 Val Loss: 0.5061853528022766, Val Acc: 0.8467\n","Epoch [131/500], Loss: 0.7049039602279663, Train Acc: 0.7948,train F1-score:0.7767 Val Loss: 0.5039913654327393, Val Acc: 0.8440\n","Epoch [132/500], Loss: 0.6300586462020874, Train Acc: 0.7953,train F1-score:0.7769 Val Loss: 0.50108802318573, Val Acc: 0.8467\n","Epoch [133/500], Loss: 0.6345246434211731, Train Acc: 0.7926,train F1-score:0.7727 Val Loss: 0.49753338098526, Val Acc: 0.8487\n","Epoch [134/500], Loss: 0.6348015069961548, Train Acc: 0.7922,train F1-score:0.7726 Val Loss: 0.49473628401756287, Val Acc: 0.8501\n","Epoch [135/500], Loss: 0.6359476447105408, Train Acc: 0.7941,train F1-score:0.7752 Val Loss: 0.49305102229118347, Val Acc: 0.8494\n","Epoch [136/500], Loss: 0.6216375827789307, Train Acc: 0.7961,train F1-score:0.7772 Val Loss: 0.49266523122787476, Val Acc: 0.8487\n","Epoch [137/500], Loss: 0.62538081407547, Train Acc: 0.7969,train F1-score:0.7788 Val Loss: 0.4941132962703705, Val Acc: 0.8487\n","Epoch [138/500], Loss: 0.6228110194206238, Train Acc: 0.7953,train F1-score:0.7767 Val Loss: 0.49463000893592834, Val Acc: 0.8481\n","Epoch [139/500], Loss: 0.6245060563087463, Train Acc: 0.7939,train F1-score:0.7752 Val Loss: 0.49543705582618713, Val Acc: 0.8447\n","Epoch [140/500], Loss: 0.6243988871574402, Train Acc: 0.7992,train F1-score:0.7804 Val Loss: 0.49436384439468384, Val Acc: 0.8440\n","Epoch [141/500], Loss: 0.6245630979537964, Train Acc: 0.8008,train F1-score:0.7823 Val Loss: 0.49238184094429016, Val Acc: 0.8467\n","Epoch [142/500], Loss: 0.6251217126846313, Train Acc: 0.7993,train F1-score:0.7810 Val Loss: 0.4916207790374756, Val Acc: 0.8507\n","Epoch [143/500], Loss: 0.6286979913711548, Train Acc: 0.7969,train F1-score:0.7794 Val Loss: 0.490007609128952, Val Acc: 0.8494\n","Epoch [144/500], Loss: 0.6200000047683716, Train Acc: 0.7975,train F1-score:0.7799 Val Loss: 0.48859426379203796, Val Acc: 0.8461\n","Epoch [145/500], Loss: 0.6086578965187073, Train Acc: 0.7964,train F1-score:0.7783 Val Loss: 0.4870477318763733, Val Acc: 0.8481\n","Epoch [146/500], Loss: 0.6146796941757202, Train Acc: 0.7989,train F1-score:0.7811 Val Loss: 0.48861443996429443, Val Acc: 0.8481\n","Epoch [147/500], Loss: 0.6205875277519226, Train Acc: 0.7967,train F1-score:0.7789 Val Loss: 0.48967450857162476, Val Acc: 0.8481\n","Epoch [148/500], Loss: 0.6150473356246948, Train Acc: 0.8009,train F1-score:0.7827 Val Loss: 0.4900512397289276, Val Acc: 0.8474\n","Epoch [149/500], Loss: 0.6129201650619507, Train Acc: 0.7964,train F1-score:0.7774 Val Loss: 0.4892905056476593, Val Acc: 0.8481\n","Epoch [150/500], Loss: 0.6154049634933472, Train Acc: 0.7973,train F1-score:0.7794 Val Loss: 0.48726174235343933, Val Acc: 0.8487\n","Epoch [151/500], Loss: 0.6074873805046082, Train Acc: 0.8058,train F1-score:0.7884 Val Loss: 0.486069917678833, Val Acc: 0.8507\n","Epoch [152/500], Loss: 0.5992687940597534, Train Acc: 0.8061,train F1-score:0.7885 Val Loss: 0.4844801425933838, Val Acc: 0.8501\n","Epoch [153/500], Loss: 0.6069016456604004, Train Acc: 0.8028,train F1-score:0.7843 Val Loss: 0.4829210638999939, Val Acc: 0.8494\n","Epoch [154/500], Loss: 0.607525646686554, Train Acc: 0.8028,train F1-score:0.7842 Val Loss: 0.48152196407318115, Val Acc: 0.8487\n","Epoch [155/500], Loss: 0.6032676696777344, Train Acc: 0.8020,train F1-score:0.7844 Val Loss: 0.481601744890213, Val Acc: 0.8481\n","Epoch [156/500], Loss: 0.5978773832321167, Train Acc: 0.8020,train F1-score:0.7845 Val Loss: 0.48212382197380066, Val Acc: 0.8474\n","Epoch [157/500], Loss: 0.6070771813392639, Train Acc: 0.8007,train F1-score:0.7828 Val Loss: 0.48210808634757996, Val Acc: 0.8501\n","Epoch [158/500], Loss: 0.6061341762542725, Train Acc: 0.7999,train F1-score:0.7823 Val Loss: 0.48118355870246887, Val Acc: 0.8521\n","Epoch [159/500], Loss: 0.6118682026863098, Train Acc: 0.8041,train F1-score:0.7866 Val Loss: 0.47992372512817383, Val Acc: 0.8514\n","Epoch [160/500], Loss: 0.5982054471969604, Train Acc: 0.8015,train F1-score:0.7846 Val Loss: 0.4809447228908539, Val Acc: 0.8554\n","Epoch [161/500], Loss: 0.601168155670166, Train Acc: 0.8044,train F1-score:0.7879 Val Loss: 0.48247891664505005, Val Acc: 0.8561\n","Epoch [162/500], Loss: 0.5975175499916077, Train Acc: 0.8036,train F1-score:0.7863 Val Loss: 0.48325613141059875, Val Acc: 0.8541\n","Epoch [163/500], Loss: 0.5987133979797363, Train Acc: 0.8051,train F1-score:0.7870 Val Loss: 0.48172107338905334, Val Acc: 0.8527\n","Epoch [164/500], Loss: 0.591562807559967, Train Acc: 0.8059,train F1-score:0.7878 Val Loss: 0.47781115770339966, Val Acc: 0.8521\n","Epoch [165/500], Loss: 0.5860983729362488, Train Acc: 0.8079,train F1-score:0.7907 Val Loss: 0.4731362760066986, Val Acc: 0.8527\n","Epoch [166/500], Loss: 0.5964158773422241, Train Acc: 0.8067,train F1-score:0.7894 Val Loss: 0.46972548961639404, Val Acc: 0.8541\n","Epoch [167/500], Loss: 0.5926159024238586, Train Acc: 0.8051,train F1-score:0.7891 Val Loss: 0.4690408706665039, Val Acc: 0.8548\n","Epoch [168/500], Loss: 0.5937818288803101, Train Acc: 0.8060,train F1-score:0.7890 Val Loss: 0.46895113587379456, Val Acc: 0.8534\n","Epoch [169/500], Loss: 0.5910997986793518, Train Acc: 0.8090,train F1-score:0.7919 Val Loss: 0.4670332074165344, Val Acc: 0.8541\n","Epoch [170/500], Loss: 0.5809706449508667, Train Acc: 0.8096,train F1-score:0.7934 Val Loss: 0.4672972559928894, Val Acc: 0.8521\n","Epoch [171/500], Loss: 0.5832687616348267, Train Acc: 0.8086,train F1-score:0.7925 Val Loss: 0.46924492716789246, Val Acc: 0.8514\n","Epoch [172/500], Loss: 0.5840620994567871, Train Acc: 0.8112,train F1-score:0.7958 Val Loss: 0.4701792597770691, Val Acc: 0.8521\n","Epoch [173/500], Loss: 0.588406503200531, Train Acc: 0.8096,train F1-score:0.7943 Val Loss: 0.4697345793247223, Val Acc: 0.8534\n","Epoch [174/500], Loss: 0.5882439017295837, Train Acc: 0.8070,train F1-score:0.7910 Val Loss: 0.46902644634246826, Val Acc: 0.8527\n","Epoch [175/500], Loss: 0.588333785533905, Train Acc: 0.8076,train F1-score:0.7910 Val Loss: 0.4705137312412262, Val Acc: 0.8514\n","Epoch [176/500], Loss: 0.5828710794448853, Train Acc: 0.8082,train F1-score:0.7919 Val Loss: 0.4711907207965851, Val Acc: 0.8534\n","Epoch [177/500], Loss: 0.5885225534439087, Train Acc: 0.8107,train F1-score:0.7943 Val Loss: 0.46995851397514343, Val Acc: 0.8568\n","Epoch [178/500], Loss: 0.5763015151023865, Train Acc: 0.8111,train F1-score:0.7956 Val Loss: 0.46938836574554443, Val Acc: 0.8588\n","Epoch [179/500], Loss: 0.5783203840255737, Train Acc: 0.8104,train F1-score:0.7945 Val Loss: 0.4678257703781128, Val Acc: 0.8588\n","Epoch [180/500], Loss: 0.5784984827041626, Train Acc: 0.8132,train F1-score:0.7988 Val Loss: 0.4669966399669647, Val Acc: 0.8541\n","Epoch [181/500], Loss: 0.575798749923706, Train Acc: 0.8115,train F1-score:0.7951 Val Loss: 0.4655343294143677, Val Acc: 0.8527\n","Epoch [182/500], Loss: 0.5741352438926697, Train Acc: 0.8120,train F1-score:0.7959 Val Loss: 0.4623902440071106, Val Acc: 0.8541\n","Epoch [183/500], Loss: 0.5855807662010193, Train Acc: 0.8094,train F1-score:0.7936 Val Loss: 0.459924578666687, Val Acc: 0.8548\n","Epoch [184/500], Loss: 0.5668193101882935, Train Acc: 0.8125,train F1-score:0.7971 Val Loss: 0.46050751209259033, Val Acc: 0.8548\n","Epoch [185/500], Loss: 0.5712985396385193, Train Acc: 0.8145,train F1-score:0.7996 Val Loss: 0.46157222986221313, Val Acc: 0.8581\n","Epoch [186/500], Loss: 0.5799775123596191, Train Acc: 0.8109,train F1-score:0.7946 Val Loss: 0.4625808298587799, Val Acc: 0.8561\n","Epoch [187/500], Loss: 0.5778468251228333, Train Acc: 0.8132,train F1-score:0.7973 Val Loss: 0.46087104082107544, Val Acc: 0.8568\n","Epoch [188/500], Loss: 0.5784828662872314, Train Acc: 0.8127,train F1-score:0.7972 Val Loss: 0.4588375389575958, Val Acc: 0.8581\n","Epoch [189/500], Loss: 0.56373131275177, Train Acc: 0.8132,train F1-score:0.7983 Val Loss: 0.45659714937210083, Val Acc: 0.8601\n","Epoch [190/500], Loss: 0.5821278095245361, Train Acc: 0.8112,train F1-score:0.7965 Val Loss: 0.45439520478248596, Val Acc: 0.8568\n","Epoch [191/500], Loss: 0.5722211003303528, Train Acc: 0.8129,train F1-score:0.7980 Val Loss: 0.4543876647949219, Val Acc: 0.8581\n","Epoch [192/500], Loss: 0.5708501935005188, Train Acc: 0.8132,train F1-score:0.7974 Val Loss: 0.45394366979599, Val Acc: 0.8568\n","Epoch [193/500], Loss: 0.5693636536598206, Train Acc: 0.8175,train F1-score:0.8020 Val Loss: 0.452282190322876, Val Acc: 0.8594\n","Epoch [194/500], Loss: 0.5716738700866699, Train Acc: 0.8132,train F1-score:0.7974 Val Loss: 0.45097866654396057, Val Acc: 0.8614\n","Epoch [195/500], Loss: 0.5705364346504211, Train Acc: 0.8149,train F1-score:0.8000 Val Loss: 0.45224714279174805, Val Acc: 0.8601\n","Epoch [196/500], Loss: 0.5638810992240906, Train Acc: 0.8170,train F1-score:0.8022 Val Loss: 0.45426633954048157, Val Acc: 0.8574\n","Epoch [197/500], Loss: 0.5738359689712524, Train Acc: 0.8140,train F1-score:0.7978 Val Loss: 0.45034709572792053, Val Acc: 0.8588\n","Epoch [198/500], Loss: 0.5638222694396973, Train Acc: 0.8144,train F1-score:0.7986 Val Loss: 0.44649749994277954, Val Acc: 0.8581\n","Epoch [199/500], Loss: 0.57244873046875, Train Acc: 0.8124,train F1-score:0.7967 Val Loss: 0.44501492381095886, Val Acc: 0.8588\n","Epoch [200/500], Loss: 0.56285560131073, Train Acc: 0.8151,train F1-score:0.8003 Val Loss: 0.44628027081489563, Val Acc: 0.8574\n","Epoch [201/500], Loss: 0.5565252900123596, Train Acc: 0.8190,train F1-score:0.8057 Val Loss: 0.4479449391365051, Val Acc: 0.8608\n","Epoch [202/500], Loss: 0.5690965056419373, Train Acc: 0.8148,train F1-score:0.7998 Val Loss: 0.44672369956970215, Val Acc: 0.8608\n","Epoch [203/500], Loss: 0.5687710642814636, Train Acc: 0.8165,train F1-score:0.8016 Val Loss: 0.44416001439094543, Val Acc: 0.8601\n","Epoch [204/500], Loss: 0.5556408762931824, Train Acc: 0.8199,train F1-score:0.8060 Val Loss: 0.44448190927505493, Val Acc: 0.8601\n","Epoch [205/500], Loss: 0.5601304173469543, Train Acc: 0.8168,train F1-score:0.8024 Val Loss: 0.4449906647205353, Val Acc: 0.8628\n","Epoch [206/500], Loss: 0.5615056157112122, Train Acc: 0.8195,train F1-score:0.8051 Val Loss: 0.44544556736946106, Val Acc: 0.8608\n","Epoch [207/500], Loss: 0.5583841800689697, Train Acc: 0.8180,train F1-score:0.8025 Val Loss: 0.44562748074531555, Val Acc: 0.8594\n","Epoch [208/500], Loss: 0.5640060305595398, Train Acc: 0.8150,train F1-score:0.8007 Val Loss: 0.44597768783569336, Val Acc: 0.8608\n","Epoch [209/500], Loss: 0.5573658347129822, Train Acc: 0.8160,train F1-score:0.8017 Val Loss: 0.44527891278266907, Val Acc: 0.8601\n","Epoch [210/500], Loss: 0.5627046823501587, Train Acc: 0.8154,train F1-score:0.8007 Val Loss: 0.44351011514663696, Val Acc: 0.8628\n","Epoch [211/500], Loss: 0.5512779355049133, Train Acc: 0.8195,train F1-score:0.8049 Val Loss: 0.44249728322029114, Val Acc: 0.8621\n","Epoch [212/500], Loss: 0.5624593496322632, Train Acc: 0.8159,train F1-score:0.8010 Val Loss: 0.441157728433609, Val Acc: 0.8614\n","Epoch [213/500], Loss: 0.5513123273849487, Train Acc: 0.8186,train F1-score:0.8041 Val Loss: 0.4393281936645508, Val Acc: 0.8614\n","Epoch [214/500], Loss: 0.5546740889549255, Train Acc: 0.8195,train F1-score:0.8058 Val Loss: 0.4399236738681793, Val Acc: 0.8614\n","Epoch [215/500], Loss: 0.5601901412010193, Train Acc: 0.8169,train F1-score:0.8021 Val Loss: 0.44166678190231323, Val Acc: 0.8621\n","Epoch [216/500], Loss: 0.5500277876853943, Train Acc: 0.8175,train F1-score:0.8036 Val Loss: 0.44181814789772034, Val Acc: 0.8655\n","Epoch [217/500], Loss: 0.5548383593559265, Train Acc: 0.8179,train F1-score:0.8037 Val Loss: 0.4406685531139374, Val Acc: 0.8641\n","Epoch [218/500], Loss: 0.5542640089988708, Train Acc: 0.8207,train F1-score:0.8064 Val Loss: 0.43935349583625793, Val Acc: 0.8655\n","Epoch [219/500], Loss: 0.5553078651428223, Train Acc: 0.8205,train F1-score:0.8070 Val Loss: 0.43897545337677, Val Acc: 0.8655\n","Epoch [220/500], Loss: 0.5547692775726318, Train Acc: 0.8201,train F1-score:0.8059 Val Loss: 0.4377772808074951, Val Acc: 0.8621\n","Epoch [221/500], Loss: 0.5499592423439026, Train Acc: 0.8181,train F1-score:0.8038 Val Loss: 0.43813642859458923, Val Acc: 0.8648\n","Epoch [222/500], Loss: 0.549197256565094, Train Acc: 0.8231,train F1-score:0.8095 Val Loss: 0.4388227164745331, Val Acc: 0.8614\n","Epoch [223/500], Loss: 0.5523374080657959, Train Acc: 0.8188,train F1-score:0.8049 Val Loss: 0.43731120228767395, Val Acc: 0.8601\n","Epoch [224/500], Loss: 0.5537892580032349, Train Acc: 0.8201,train F1-score:0.8065 Val Loss: 0.4337552785873413, Val Acc: 0.8588\n","Epoch [225/500], Loss: 0.5466912388801575, Train Acc: 0.8213,train F1-score:0.8065 Val Loss: 0.43148207664489746, Val Acc: 0.8588\n","Epoch [226/500], Loss: 0.5459581613540649, Train Acc: 0.8206,train F1-score:0.8064 Val Loss: 0.42971768975257874, Val Acc: 0.8581\n","Epoch [227/500], Loss: 0.5497258901596069, Train Acc: 0.8194,train F1-score:0.8057 Val Loss: 0.4296903610229492, Val Acc: 0.8608\n","Epoch [228/500], Loss: 0.5452565550804138, Train Acc: 0.8194,train F1-score:0.8060 Val Loss: 0.42818453907966614, Val Acc: 0.8608\n","Epoch [229/500], Loss: 0.5465348958969116, Train Acc: 0.8198,train F1-score:0.8060 Val Loss: 0.4291301667690277, Val Acc: 0.8635\n","Epoch [230/500], Loss: 0.5473559498786926, Train Acc: 0.8201,train F1-score:0.8067 Val Loss: 0.43353888392448425, Val Acc: 0.8594\n","Epoch [231/500], Loss: 0.5443991422653198, Train Acc: 0.8205,train F1-score:0.8066 Val Loss: 0.4337747395038605, Val Acc: 0.8588\n","Epoch [232/500], Loss: 0.5379131436347961, Train Acc: 0.8214,train F1-score:0.8075 Val Loss: 0.4347513020038605, Val Acc: 0.8621\n","Epoch [233/500], Loss: 0.5463349223136902, Train Acc: 0.8214,train F1-score:0.8076 Val Loss: 0.4364129304885864, Val Acc: 0.8608\n","Epoch [234/500], Loss: 0.5457091927528381, Train Acc: 0.8232,train F1-score:0.8099 Val Loss: 0.43755465745925903, Val Acc: 0.8635\n","Epoch [235/500], Loss: 0.546028196811676, Train Acc: 0.8236,train F1-score:0.8091 Val Loss: 0.43639323115348816, Val Acc: 0.8648\n","Epoch [236/500], Loss: 0.5300082564353943, Train Acc: 0.8262,train F1-score:0.8125 Val Loss: 0.43258917331695557, Val Acc: 0.8668\n","Epoch [237/500], Loss: 0.5310959219932556, Train Acc: 0.8238,train F1-score:0.8106 Val Loss: 0.43111807107925415, Val Acc: 0.8655\n","Epoch [238/500], Loss: 0.5475082397460938, Train Acc: 0.8206,train F1-score:0.8074 Val Loss: 0.43121424317359924, Val Acc: 0.8641\n","Epoch [239/500], Loss: 0.5428159236907959, Train Acc: 0.8229,train F1-score:0.8088 Val Loss: 0.4324270486831665, Val Acc: 0.8621\n","Epoch [240/500], Loss: 0.5490080714225769, Train Acc: 0.8197,train F1-score:0.8044 Val Loss: 0.430631160736084, Val Acc: 0.8641\n","Epoch [241/500], Loss: 0.5414052605628967, Train Acc: 0.8231,train F1-score:0.8086 Val Loss: 0.42956894636154175, Val Acc: 0.8675\n","Epoch [242/500], Loss: 0.5412436723709106, Train Acc: 0.8241,train F1-score:0.8110 Val Loss: 0.43089962005615234, Val Acc: 0.8661\n","Epoch [243/500], Loss: 0.5419867038726807, Train Acc: 0.8206,train F1-score:0.8073 Val Loss: 0.4301758110523224, Val Acc: 0.8661\n","Epoch [244/500], Loss: 0.5395231246948242, Train Acc: 0.8265,train F1-score:0.8134 Val Loss: 0.42913445830345154, Val Acc: 0.8648\n","Epoch [245/500], Loss: 0.5363085269927979, Train Acc: 0.8231,train F1-score:0.8100 Val Loss: 0.42772868275642395, Val Acc: 0.8641\n","Epoch [246/500], Loss: 0.540718138217926, Train Acc: 0.8205,train F1-score:0.8070 Val Loss: 0.42615604400634766, Val Acc: 0.8628\n","Epoch [247/500], Loss: 0.5352179408073425, Train Acc: 0.8221,train F1-score:0.8078 Val Loss: 0.4252919554710388, Val Acc: 0.8641\n","Epoch [248/500], Loss: 0.5362244248390198, Train Acc: 0.8240,train F1-score:0.8103 Val Loss: 0.4242227375507355, Val Acc: 0.8655\n","Epoch [249/500], Loss: 0.5368556380271912, Train Acc: 0.8207,train F1-score:0.8078 Val Loss: 0.42331790924072266, Val Acc: 0.8655\n","Epoch [250/500], Loss: 0.5468391180038452, Train Acc: 0.8196,train F1-score:0.8067 Val Loss: 0.42292121052742004, Val Acc: 0.8668\n","Epoch [251/500], Loss: 0.542384147644043, Train Acc: 0.8190,train F1-score:0.8056 Val Loss: 0.4236399233341217, Val Acc: 0.8675\n","Epoch [252/500], Loss: 0.5378398299217224, Train Acc: 0.8234,train F1-score:0.8098 Val Loss: 0.42334747314453125, Val Acc: 0.8648\n","Epoch [253/500], Loss: 0.5431060791015625, Train Acc: 0.8231,train F1-score:0.8096 Val Loss: 0.42320388555526733, Val Acc: 0.8621\n","Epoch [254/500], Loss: 0.5316413044929504, Train Acc: 0.8231,train F1-score:0.8094 Val Loss: 0.42272284626960754, Val Acc: 0.8635\n","Epoch [255/500], Loss: 0.5308082699775696, Train Acc: 0.8239,train F1-score:0.8100 Val Loss: 0.4221611022949219, Val Acc: 0.8635\n","Epoch [256/500], Loss: 0.5378153324127197, Train Acc: 0.8185,train F1-score:0.8042 Val Loss: 0.4217268228530884, Val Acc: 0.8641\n","Epoch [257/500], Loss: 0.5357569456100464, Train Acc: 0.8240,train F1-score:0.8105 Val Loss: 0.42231735587120056, Val Acc: 0.8648\n","Epoch [258/500], Loss: 0.5339691638946533, Train Acc: 0.8268,train F1-score:0.8132 Val Loss: 0.42175284028053284, Val Acc: 0.8668\n","Epoch [259/500], Loss: 0.532676637172699, Train Acc: 0.8242,train F1-score:0.8110 Val Loss: 0.4224494397640228, Val Acc: 0.8668\n","Epoch [260/500], Loss: 0.5325382351875305, Train Acc: 0.8278,train F1-score:0.8149 Val Loss: 0.42468416690826416, Val Acc: 0.8695\n","Epoch [261/500], Loss: 0.5244272351264954, Train Acc: 0.8277,train F1-score:0.8152 Val Loss: 0.42602255940437317, Val Acc: 0.8668\n","Epoch [262/500], Loss: 0.5341934561729431, Train Acc: 0.8253,train F1-score:0.8121 Val Loss: 0.42659538984298706, Val Acc: 0.8661\n","Epoch [263/500], Loss: 0.5290155410766602, Train Acc: 0.8266,train F1-score:0.8133 Val Loss: 0.42702266573905945, Val Acc: 0.8635\n","Epoch [264/500], Loss: 0.5321705937385559, Train Acc: 0.8265,train F1-score:0.8137 Val Loss: 0.42789116501808167, Val Acc: 0.8648\n","Epoch [265/500], Loss: 0.531152606010437, Train Acc: 0.8244,train F1-score:0.8114 Val Loss: 0.42815887928009033, Val Acc: 0.8628\n","Epoch [266/500], Loss: 0.5295040011405945, Train Acc: 0.8262,train F1-score:0.8134 Val Loss: 0.4255528748035431, Val Acc: 0.8695\n","Epoch [267/500], Loss: 0.5370797514915466, Train Acc: 0.8255,train F1-score:0.8121 Val Loss: 0.4226034879684448, Val Acc: 0.8688\n","Epoch [268/500], Loss: 0.5287655591964722, Train Acc: 0.8241,train F1-score:0.8102 Val Loss: 0.42096278071403503, Val Acc: 0.8681\n","Epoch [269/500], Loss: 0.5311554074287415, Train Acc: 0.8278,train F1-score:0.8153 Val Loss: 0.422393262386322, Val Acc: 0.8661\n","Epoch [270/500], Loss: 0.5242889523506165, Train Acc: 0.8277,train F1-score:0.8147 Val Loss: 0.4226260781288147, Val Acc: 0.8681\n","Epoch [271/500], Loss: 0.5264817476272583, Train Acc: 0.8293,train F1-score:0.8163 Val Loss: 0.4206470549106598, Val Acc: 0.8688\n","Epoch [272/500], Loss: 0.5302242040634155, Train Acc: 0.8273,train F1-score:0.8143 Val Loss: 0.4192621409893036, Val Acc: 0.8722\n","Epoch [273/500], Loss: 0.527682900428772, Train Acc: 0.8293,train F1-score:0.8168 Val Loss: 0.41977182030677795, Val Acc: 0.8728\n","Epoch [274/500], Loss: 0.522426187992096, Train Acc: 0.8273,train F1-score:0.8147 Val Loss: 0.4198349714279175, Val Acc: 0.8708\n","Epoch [275/500], Loss: 0.5310086011886597, Train Acc: 0.8250,train F1-score:0.8119 Val Loss: 0.4210807681083679, Val Acc: 0.8695\n","Epoch [276/500], Loss: 0.5235551595687866, Train Acc: 0.8258,train F1-score:0.8136 Val Loss: 0.4225488007068634, Val Acc: 0.8648\n","Epoch [277/500], Loss: 0.524681568145752, Train Acc: 0.8272,train F1-score:0.8142 Val Loss: 0.41995692253112793, Val Acc: 0.8668\n","Epoch [278/500], Loss: 0.5182817578315735, Train Acc: 0.8291,train F1-score:0.8165 Val Loss: 0.41816261410713196, Val Acc: 0.8681\n","Epoch [279/500], Loss: 0.5335759520530701, Train Acc: 0.8263,train F1-score:0.8133 Val Loss: 0.41735342144966125, Val Acc: 0.8675\n","Epoch [280/500], Loss: 0.5237278342247009, Train Acc: 0.8257,train F1-score:0.8129 Val Loss: 0.41953200101852417, Val Acc: 0.8668\n","Epoch [281/500], Loss: 0.5287919640541077, Train Acc: 0.8278,train F1-score:0.8153 Val Loss: 0.420425683259964, Val Acc: 0.8661\n","Epoch [282/500], Loss: 0.5220076441764832, Train Acc: 0.8266,train F1-score:0.8139 Val Loss: 0.41863444447517395, Val Acc: 0.8675\n","Epoch [283/500], Loss: 0.5274131894111633, Train Acc: 0.8274,train F1-score:0.8146 Val Loss: 0.418190062046051, Val Acc: 0.8695\n","Epoch [284/500], Loss: 0.5232222080230713, Train Acc: 0.8286,train F1-score:0.8154 Val Loss: 0.41807714104652405, Val Acc: 0.8681\n","Epoch [285/500], Loss: 0.5226102471351624, Train Acc: 0.8254,train F1-score:0.8116 Val Loss: 0.4186023771762848, Val Acc: 0.8641\n","Epoch [286/500], Loss: 0.5202250480651855, Train Acc: 0.8283,train F1-score:0.8151 Val Loss: 0.4156095087528229, Val Acc: 0.8675\n","Epoch [287/500], Loss: 0.5087966918945312, Train Acc: 0.8310,train F1-score:0.8187 Val Loss: 0.4130357801914215, Val Acc: 0.8701\n","Epoch [288/500], Loss: 0.5217627286911011, Train Acc: 0.8286,train F1-score:0.8155 Val Loss: 0.411367267370224, Val Acc: 0.8695\n","Epoch [289/500], Loss: 0.5197457075119019, Train Acc: 0.8294,train F1-score:0.8175 Val Loss: 0.4112391173839569, Val Acc: 0.8668\n","Epoch [290/500], Loss: 0.5286797881126404, Train Acc: 0.8282,train F1-score:0.8157 Val Loss: 0.411949098110199, Val Acc: 0.8675\n","Epoch [291/500], Loss: 0.519018292427063, Train Acc: 0.8272,train F1-score:0.8144 Val Loss: 0.41153958439826965, Val Acc: 0.8661\n","Epoch [292/500], Loss: 0.5219160318374634, Train Acc: 0.8233,train F1-score:0.8104 Val Loss: 0.4122905731201172, Val Acc: 0.8688\n","Epoch [293/500], Loss: 0.5166743397712708, Train Acc: 0.8305,train F1-score:0.8187 Val Loss: 0.41538453102111816, Val Acc: 0.8661\n","Epoch [294/500], Loss: 0.5222037434577942, Train Acc: 0.8273,train F1-score:0.8146 Val Loss: 0.4164121448993683, Val Acc: 0.8681\n","Epoch [295/500], Loss: 0.5199199914932251, Train Acc: 0.8285,train F1-score:0.8160 Val Loss: 0.41432538628578186, Val Acc: 0.8688\n","Epoch [296/500], Loss: 0.5152658820152283, Train Acc: 0.8276,train F1-score:0.8147 Val Loss: 0.4125988781452179, Val Acc: 0.8722\n","Epoch [297/500], Loss: 0.5270385146141052, Train Acc: 0.8238,train F1-score:0.8118 Val Loss: 0.41355034708976746, Val Acc: 0.8695\n","Epoch [298/500], Loss: 0.5075539946556091, Train Acc: 0.8301,train F1-score:0.8172 Val Loss: 0.4145437479019165, Val Acc: 0.8701\n","Epoch [299/500], Loss: 0.5169798135757446, Train Acc: 0.8304,train F1-score:0.8181 Val Loss: 0.41425618529319763, Val Acc: 0.8668\n","Epoch [300/500], Loss: 0.5130683183670044, Train Acc: 0.8294,train F1-score:0.8172 Val Loss: 0.4143794775009155, Val Acc: 0.8655\n","Epoch [301/500], Loss: 0.5225788950920105, Train Acc: 0.8263,train F1-score:0.8136 Val Loss: 0.4138576090335846, Val Acc: 0.8641\n","Epoch [302/500], Loss: 0.5171312093734741, Train Acc: 0.8303,train F1-score:0.8176 Val Loss: 0.4136910140514374, Val Acc: 0.8648\n","Epoch [303/500], Loss: 0.514985203742981, Train Acc: 0.8284,train F1-score:0.8162 Val Loss: 0.41331300139427185, Val Acc: 0.8668\n","Epoch [304/500], Loss: 0.5163682103157043, Train Acc: 0.8286,train F1-score:0.8153 Val Loss: 0.4115737974643707, Val Acc: 0.8708\n","Epoch [305/500], Loss: 0.5132169127464294, Train Acc: 0.8301,train F1-score:0.8182 Val Loss: 0.41185811161994934, Val Acc: 0.8715\n","Epoch [306/500], Loss: 0.5233331322669983, Train Acc: 0.8299,train F1-score:0.8185 Val Loss: 0.41015899181365967, Val Acc: 0.8708\n","Epoch [307/500], Loss: 0.517156720161438, Train Acc: 0.8298,train F1-score:0.8182 Val Loss: 0.4091111421585083, Val Acc: 0.8701\n","Epoch [308/500], Loss: 0.5209398865699768, Train Acc: 0.8303,train F1-score:0.8184 Val Loss: 0.4099818468093872, Val Acc: 0.8695\n","Epoch [309/500], Loss: 0.5084729790687561, Train Acc: 0.8288,train F1-score:0.8165 Val Loss: 0.41198402643203735, Val Acc: 0.8668\n","Epoch [310/500], Loss: 0.5215573310852051, Train Acc: 0.8270,train F1-score:0.8141 Val Loss: 0.4111470580101013, Val Acc: 0.8675\n","Epoch [311/500], Loss: 0.5165961384773254, Train Acc: 0.8276,train F1-score:0.8148 Val Loss: 0.4083196520805359, Val Acc: 0.8695\n","Epoch [312/500], Loss: 0.5136660933494568, Train Acc: 0.8298,train F1-score:0.8176 Val Loss: 0.40770214796066284, Val Acc: 0.8708\n","Epoch [313/500], Loss: 0.5165790915489197, Train Acc: 0.8307,train F1-score:0.8189 Val Loss: 0.40943244099617004, Val Acc: 0.8708\n","Epoch [314/500], Loss: 0.5218271017074585, Train Acc: 0.8252,train F1-score:0.8132 Val Loss: 0.41026848554611206, Val Acc: 0.8708\n","Epoch [315/500], Loss: 0.5161435008049011, Train Acc: 0.8311,train F1-score:0.8188 Val Loss: 0.4089272618293762, Val Acc: 0.8715\n","Epoch [316/500], Loss: 0.5031532049179077, Train Acc: 0.8308,train F1-score:0.8184 Val Loss: 0.4100818634033203, Val Acc: 0.8715\n","Epoch [317/500], Loss: 0.5065435171127319, Train Acc: 0.8319,train F1-score:0.8195 Val Loss: 0.4105309545993805, Val Acc: 0.8728\n","Epoch [318/500], Loss: 0.5041361451148987, Train Acc: 0.8300,train F1-score:0.8176 Val Loss: 0.4120800793170929, Val Acc: 0.8722\n","Epoch [319/500], Loss: 0.5068101286888123, Train Acc: 0.8323,train F1-score:0.8208 Val Loss: 0.4155341386795044, Val Acc: 0.8715\n","Epoch [320/500], Loss: 0.5076097846031189, Train Acc: 0.8308,train F1-score:0.8187 Val Loss: 0.41890189051628113, Val Acc: 0.8701\n","Epoch [321/500], Loss: 0.5104667544364929, Train Acc: 0.8327,train F1-score:0.8207 Val Loss: 0.41893628239631653, Val Acc: 0.8675\n","Epoch [322/500], Loss: 0.5190021395683289, Train Acc: 0.8262,train F1-score:0.8146 Val Loss: 0.4185355603694916, Val Acc: 0.8701\n","Epoch [323/500], Loss: 0.5097995400428772, Train Acc: 0.8292,train F1-score:0.8175 Val Loss: 0.41791242361068726, Val Acc: 0.8695\n","Epoch [324/500], Loss: 0.5106900334358215, Train Acc: 0.8324,train F1-score:0.8206 Val Loss: 0.41555190086364746, Val Acc: 0.8695\n","Epoch [325/500], Loss: 0.5043235421180725, Train Acc: 0.8314,train F1-score:0.8190 Val Loss: 0.4122841954231262, Val Acc: 0.8708\n","Epoch [326/500], Loss: 0.5081885457038879, Train Acc: 0.8343,train F1-score:0.8222 Val Loss: 0.41082707047462463, Val Acc: 0.8715\n","Epoch [327/500], Loss: 0.5067594647407532, Train Acc: 0.8324,train F1-score:0.8200 Val Loss: 0.41036832332611084, Val Acc: 0.8695\n","Epoch [328/500], Loss: 0.5087242722511292, Train Acc: 0.8334,train F1-score:0.8214 Val Loss: 0.4125285744667053, Val Acc: 0.8715\n","Epoch [329/500], Loss: 0.5031242966651917, Train Acc: 0.8313,train F1-score:0.8186 Val Loss: 0.41548535227775574, Val Acc: 0.8722\n","Epoch [330/500], Loss: 0.5149118900299072, Train Acc: 0.8306,train F1-score:0.8181 Val Loss: 0.4159546196460724, Val Acc: 0.8722\n","Epoch [331/500], Loss: 0.5019641518592834, Train Acc: 0.8332,train F1-score:0.8210 Val Loss: 0.41540810465812683, Val Acc: 0.8722\n","Epoch [332/500], Loss: 0.50603848695755, Train Acc: 0.8339,train F1-score:0.8214 Val Loss: 0.4128418266773224, Val Acc: 0.8728\n","Epoch [333/500], Loss: 0.5092693567276001, Train Acc: 0.8301,train F1-score:0.8178 Val Loss: 0.40975499153137207, Val Acc: 0.8762\n","Epoch [334/500], Loss: 0.5049271583557129, Train Acc: 0.8322,train F1-score:0.8210 Val Loss: 0.40807777643203735, Val Acc: 0.8762\n","Epoch [335/500], Loss: 0.5136203765869141, Train Acc: 0.8278,train F1-score:0.8161 Val Loss: 0.40769633650779724, Val Acc: 0.8728\n","Epoch [336/500], Loss: 0.5080568194389343, Train Acc: 0.8334,train F1-score:0.8216 Val Loss: 0.40915581583976746, Val Acc: 0.8708\n","Epoch [337/500], Loss: 0.5095053911209106, Train Acc: 0.8319,train F1-score:0.8197 Val Loss: 0.41008439660072327, Val Acc: 0.8681\n","Epoch [338/500], Loss: 0.5023303031921387, Train Acc: 0.8347,train F1-score:0.8228 Val Loss: 0.41128355264663696, Val Acc: 0.8681\n","Epoch [339/500], Loss: 0.5095628499984741, Train Acc: 0.8320,train F1-score:0.8206 Val Loss: 0.41357332468032837, Val Acc: 0.8675\n","Epoch [340/500], Loss: 0.5019462704658508, Train Acc: 0.8332,train F1-score:0.8214 Val Loss: 0.41470229625701904, Val Acc: 0.8701\n","Epoch [341/500], Loss: 0.499138742685318, Train Acc: 0.8345,train F1-score:0.8229 Val Loss: 0.4129256308078766, Val Acc: 0.8701\n","Epoch [342/500], Loss: 0.4987848103046417, Train Acc: 0.8334,train F1-score:0.8218 Val Loss: 0.4109196662902832, Val Acc: 0.8715\n","Epoch [343/500], Loss: 0.5053502917289734, Train Acc: 0.8304,train F1-score:0.8186 Val Loss: 0.4086439311504364, Val Acc: 0.8708\n","Epoch [344/500], Loss: 0.5025663375854492, Train Acc: 0.8342,train F1-score:0.8224 Val Loss: 0.40903759002685547, Val Acc: 0.8722\n","Epoch [345/500], Loss: 0.49067550897598267, Train Acc: 0.8320,train F1-score:0.8202 Val Loss: 0.4092731475830078, Val Acc: 0.8748\n","Epoch [346/500], Loss: 0.506865382194519, Train Acc: 0.8331,train F1-score:0.8203 Val Loss: 0.40708017349243164, Val Acc: 0.8748\n","Epoch [347/500], Loss: 0.5020715594291687, Train Acc: 0.8348,train F1-score:0.8232 Val Loss: 0.4062977433204651, Val Acc: 0.8762\n","Epoch [348/500], Loss: 0.506044864654541, Train Acc: 0.8329,train F1-score:0.8216 Val Loss: 0.4066917896270752, Val Acc: 0.8775\n","Epoch [349/500], Loss: 0.5102313756942749, Train Acc: 0.8300,train F1-score:0.8183 Val Loss: 0.407720148563385, Val Acc: 0.8742\n","Epoch [350/500], Loss: 0.5025664567947388, Train Acc: 0.8334,train F1-score:0.8211 Val Loss: 0.4106152355670929, Val Acc: 0.8695\n","Epoch [351/500], Loss: 0.4975496530532837, Train Acc: 0.8334,train F1-score:0.8209 Val Loss: 0.4119485020637512, Val Acc: 0.8715\n","Epoch [352/500], Loss: 0.49377137422561646, Train Acc: 0.8344,train F1-score:0.8225 Val Loss: 0.4121503233909607, Val Acc: 0.8708\n","Epoch [353/500], Loss: 0.5036223530769348, Train Acc: 0.8339,train F1-score:0.8225 Val Loss: 0.412619948387146, Val Acc: 0.8722\n","Epoch [354/500], Loss: 0.5043102502822876, Train Acc: 0.8343,train F1-score:0.8225 Val Loss: 0.41119900345802307, Val Acc: 0.8728\n","Epoch [355/500], Loss: 0.4948703348636627, Train Acc: 0.8334,train F1-score:0.8215 Val Loss: 0.4087461829185486, Val Acc: 0.8742\n","Epoch [356/500], Loss: 0.5024652481079102, Train Acc: 0.8369,train F1-score:0.8255 Val Loss: 0.4054379463195801, Val Acc: 0.8748\n","Epoch [357/500], Loss: 0.4963792562484741, Train Acc: 0.8330,train F1-score:0.8213 Val Loss: 0.4036042094230652, Val Acc: 0.8722\n","Epoch [358/500], Loss: 0.4979211986064911, Train Acc: 0.8380,train F1-score:0.8262 Val Loss: 0.40157485008239746, Val Acc: 0.8708\n","Epoch [359/500], Loss: 0.4985675513744354, Train Acc: 0.8327,train F1-score:0.8209 Val Loss: 0.40012070536613464, Val Acc: 0.8722\n","Epoch [360/500], Loss: 0.49518799781799316, Train Acc: 0.8338,train F1-score:0.8220 Val Loss: 0.39948543906211853, Val Acc: 0.8722\n","Epoch [361/500], Loss: 0.5023123621940613, Train Acc: 0.8337,train F1-score:0.8221 Val Loss: 0.3999714255332947, Val Acc: 0.8742\n","Epoch [362/500], Loss: 0.4982125461101532, Train Acc: 0.8334,train F1-score:0.8224 Val Loss: 0.4015471339225769, Val Acc: 0.8722\n","Epoch [363/500], Loss: 0.5074412226676941, Train Acc: 0.8340,train F1-score:0.8230 Val Loss: 0.40431612730026245, Val Acc: 0.8695\n","Epoch [364/500], Loss: 0.4986831843852997, Train Acc: 0.8385,train F1-score:0.8261 Val Loss: 0.40656694769859314, Val Acc: 0.8681\n","Epoch [365/500], Loss: 0.49730464816093445, Train Acc: 0.8335,train F1-score:0.8219 Val Loss: 0.407299280166626, Val Acc: 0.8675\n","Epoch [366/500], Loss: 0.49480995535850525, Train Acc: 0.8344,train F1-score:0.8225 Val Loss: 0.4065002202987671, Val Acc: 0.8688\n","Epoch [367/500], Loss: 0.4915139079093933, Train Acc: 0.8375,train F1-score:0.8263 Val Loss: 0.40417081117630005, Val Acc: 0.8695\n","Epoch [368/500], Loss: 0.4942481517791748, Train Acc: 0.8312,train F1-score:0.8198 Val Loss: 0.4027414619922638, Val Acc: 0.8722\n","Epoch [369/500], Loss: 0.4946189224720001, Train Acc: 0.8394,train F1-score:0.8282 Val Loss: 0.40147727727890015, Val Acc: 0.8715\n","Epoch [370/500], Loss: 0.5013805627822876, Train Acc: 0.8314,train F1-score:0.8196 Val Loss: 0.40099987387657166, Val Acc: 0.8715\n","Epoch [371/500], Loss: 0.4946543276309967, Train Acc: 0.8321,train F1-score:0.8203 Val Loss: 0.4008541405200958, Val Acc: 0.8715\n","Epoch [372/500], Loss: 0.49613937735557556, Train Acc: 0.8361,train F1-score:0.8238 Val Loss: 0.4027262330055237, Val Acc: 0.8722\n","Epoch [373/500], Loss: 0.5011008977890015, Train Acc: 0.8360,train F1-score:0.8244 Val Loss: 0.40445780754089355, Val Acc: 0.8715\n","Epoch [374/500], Loss: 0.48784327507019043, Train Acc: 0.8411,train F1-score:0.8307 Val Loss: 0.40761449933052063, Val Acc: 0.8728\n","Epoch [375/500], Loss: 0.5049527883529663, Train Acc: 0.8346,train F1-score:0.8234 Val Loss: 0.40935397148132324, Val Acc: 0.8722\n","Epoch [376/500], Loss: 0.5045763850212097, Train Acc: 0.8356,train F1-score:0.8240 Val Loss: 0.4105924069881439, Val Acc: 0.8762\n","Epoch [377/500], Loss: 0.4965316653251648, Train Acc: 0.8390,train F1-score:0.8269 Val Loss: 0.4106839895248413, Val Acc: 0.8762\n","Epoch [378/500], Loss: 0.4955420196056366, Train Acc: 0.8353,train F1-score:0.8243 Val Loss: 0.4092940390110016, Val Acc: 0.8748\n","Epoch [379/500], Loss: 0.4988681375980377, Train Acc: 0.8339,train F1-score:0.8219 Val Loss: 0.40726763010025024, Val Acc: 0.8742\n","Epoch [380/500], Loss: 0.4997057020664215, Train Acc: 0.8337,train F1-score:0.8222 Val Loss: 0.40739700198173523, Val Acc: 0.8735\n","Epoch [381/500], Loss: 0.49018633365631104, Train Acc: 0.8350,train F1-score:0.8231 Val Loss: 0.40685299038887024, Val Acc: 0.8722\n","Epoch [382/500], Loss: 0.48638835549354553, Train Acc: 0.8371,train F1-score:0.8259 Val Loss: 0.4087095856666565, Val Acc: 0.8755\n","Epoch [383/500], Loss: 0.48846954107284546, Train Acc: 0.8386,train F1-score:0.8286 Val Loss: 0.41334068775177, Val Acc: 0.8728\n","Epoch [384/500], Loss: 0.49402686953544617, Train Acc: 0.8352,train F1-score:0.8242 Val Loss: 0.41914451122283936, Val Acc: 0.8708\n","Epoch [385/500], Loss: 0.5009667873382568, Train Acc: 0.8331,train F1-score:0.8211 Val Loss: 0.42014065384864807, Val Acc: 0.8681\n","Epoch [386/500], Loss: 0.4986673593521118, Train Acc: 0.8371,train F1-score:0.8257 Val Loss: 0.4174565374851227, Val Acc: 0.8695\n","Epoch [387/500], Loss: 0.4988190829753876, Train Acc: 0.8329,train F1-score:0.8204 Val Loss: 0.4156560003757477, Val Acc: 0.8701\n","Epoch [388/500], Loss: 0.4917682111263275, Train Acc: 0.8365,train F1-score:0.8256 Val Loss: 0.4140079915523529, Val Acc: 0.8715\n","Epoch [389/500], Loss: 0.4943332076072693, Train Acc: 0.8368,train F1-score:0.8252 Val Loss: 0.41213923692703247, Val Acc: 0.8722\n","Epoch [390/500], Loss: 0.49763935804367065, Train Acc: 0.8319,train F1-score:0.8203 Val Loss: 0.411515474319458, Val Acc: 0.8735\n","Epoch [391/500], Loss: 0.4920095205307007, Train Acc: 0.8334,train F1-score:0.8210 Val Loss: 0.40870824456214905, Val Acc: 0.8735\n","Epoch [392/500], Loss: 0.49517562985420227, Train Acc: 0.8370,train F1-score:0.8241 Val Loss: 0.40743398666381836, Val Acc: 0.8755\n","Epoch [393/500], Loss: 0.4914312958717346, Train Acc: 0.8350,train F1-score:0.8231 Val Loss: 0.4081171154975891, Val Acc: 0.8748\n","Epoch [394/500], Loss: 0.4929361045360565, Train Acc: 0.8368,train F1-score:0.8258 Val Loss: 0.410780131816864, Val Acc: 0.8728\n","Epoch [395/500], Loss: 0.489075630903244, Train Acc: 0.8372,train F1-score:0.8266 Val Loss: 0.41268184781074524, Val Acc: 0.8722\n","Epoch [396/500], Loss: 0.4930480122566223, Train Acc: 0.8378,train F1-score:0.8268 Val Loss: 0.4127621650695801, Val Acc: 0.8748\n","Epoch [397/500], Loss: 0.4898744821548462, Train Acc: 0.8337,train F1-score:0.8229 Val Loss: 0.4115535318851471, Val Acc: 0.8748\n","Epoch [398/500], Loss: 0.4922851324081421, Train Acc: 0.8334,train F1-score:0.8215 Val Loss: 0.4110903739929199, Val Acc: 0.8775\n","Epoch [399/500], Loss: 0.4832296669483185, Train Acc: 0.8404,train F1-score:0.8286 Val Loss: 0.41044673323631287, Val Acc: 0.8722\n","Epoch [400/500], Loss: 0.4912312924861908, Train Acc: 0.8358,train F1-score:0.8251 Val Loss: 0.40908417105674744, Val Acc: 0.8722\n","Epoch [401/500], Loss: 0.4909863770008087, Train Acc: 0.8371,train F1-score:0.8263 Val Loss: 0.40702956914901733, Val Acc: 0.8722\n","Epoch [402/500], Loss: 0.49840959906578064, Train Acc: 0.8355,train F1-score:0.8246 Val Loss: 0.40728482604026794, Val Acc: 0.8715\n","Epoch [403/500], Loss: 0.49369698762893677, Train Acc: 0.8370,train F1-score:0.8266 Val Loss: 0.41080769896507263, Val Acc: 0.8701\n","Epoch [404/500], Loss: 0.4979081153869629, Train Acc: 0.8370,train F1-score:0.8253 Val Loss: 0.4114588499069214, Val Acc: 0.8695\n","Epoch [405/500], Loss: 0.48596903681755066, Train Acc: 0.8379,train F1-score:0.8268 Val Loss: 0.4090215563774109, Val Acc: 0.8701\n","Epoch [406/500], Loss: 0.48646125197410583, Train Acc: 0.8372,train F1-score:0.8254 Val Loss: 0.4080032706260681, Val Acc: 0.8695\n","Epoch [407/500], Loss: 0.48601219058036804, Train Acc: 0.8412,train F1-score:0.8306 Val Loss: 0.40701213479042053, Val Acc: 0.8688\n","Epoch [408/500], Loss: 0.4860699474811554, Train Acc: 0.8376,train F1-score:0.8267 Val Loss: 0.40862077474594116, Val Acc: 0.8668\n","Epoch [409/500], Loss: 0.48831814527511597, Train Acc: 0.8375,train F1-score:0.8265 Val Loss: 0.40851929783821106, Val Acc: 0.8681\n","Epoch [410/500], Loss: 0.486331969499588, Train Acc: 0.8377,train F1-score:0.8264 Val Loss: 0.4041876792907715, Val Acc: 0.8701\n","Epoch [411/500], Loss: 0.4839974045753479, Train Acc: 0.8367,train F1-score:0.8257 Val Loss: 0.4023140072822571, Val Acc: 0.8715\n","Epoch [412/500], Loss: 0.4819124937057495, Train Acc: 0.8376,train F1-score:0.8272 Val Loss: 0.4037416875362396, Val Acc: 0.8728\n","Epoch [413/500], Loss: 0.49417951703071594, Train Acc: 0.8378,train F1-score:0.8268 Val Loss: 0.4070046842098236, Val Acc: 0.8742\n","Epoch [414/500], Loss: 0.4819871187210083, Train Acc: 0.8381,train F1-score:0.8273 Val Loss: 0.411285400390625, Val Acc: 0.8762\n","Epoch [415/500], Loss: 0.48484504222869873, Train Acc: 0.8370,train F1-score:0.8256 Val Loss: 0.4122008979320526, Val Acc: 0.8762\n","Epoch [416/500], Loss: 0.49955910444259644, Train Acc: 0.8346,train F1-score:0.8228 Val Loss: 0.4098670482635498, Val Acc: 0.8762\n","Epoch [417/500], Loss: 0.4854733645915985, Train Acc: 0.8375,train F1-score:0.8259 Val Loss: 0.4085378348827362, Val Acc: 0.8735\n","Epoch [418/500], Loss: 0.4925614297389984, Train Acc: 0.8375,train F1-score:0.8267 Val Loss: 0.40815067291259766, Val Acc: 0.8722\n","Epoch [419/500], Loss: 0.4926997423171997, Train Acc: 0.8365,train F1-score:0.8251 Val Loss: 0.40734055638313293, Val Acc: 0.8715\n","Epoch [420/500], Loss: 0.49062272906303406, Train Acc: 0.8375,train F1-score:0.8268 Val Loss: 0.4084188640117645, Val Acc: 0.8681\n","Epoch [421/500], Loss: 0.49140894412994385, Train Acc: 0.8379,train F1-score:0.8277 Val Loss: 0.4083361327648163, Val Acc: 0.8675\n","Epoch [422/500], Loss: 0.4832002520561218, Train Acc: 0.8359,train F1-score:0.8251 Val Loss: 0.4074121117591858, Val Acc: 0.8681\n","Epoch [423/500], Loss: 0.4799327254295349, Train Acc: 0.8401,train F1-score:0.8287 Val Loss: 0.40689948201179504, Val Acc: 0.8688\n","Epoch [424/500], Loss: 0.48743167519569397, Train Acc: 0.8355,train F1-score:0.8240 Val Loss: 0.40828678011894226, Val Acc: 0.8708\n","Epoch [425/500], Loss: 0.49329784512519836, Train Acc: 0.8343,train F1-score:0.8226 Val Loss: 0.4088801145553589, Val Acc: 0.8728\n","Epoch [426/500], Loss: 0.48463156819343567, Train Acc: 0.8357,train F1-score:0.8243 Val Loss: 0.40932899713516235, Val Acc: 0.8728\n","Epoch [427/500], Loss: 0.4872518479824066, Train Acc: 0.8374,train F1-score:0.8272 Val Loss: 0.4083729386329651, Val Acc: 0.8742\n","Epoch [428/500], Loss: 0.4827355742454529, Train Acc: 0.8394,train F1-score:0.8289 Val Loss: 0.4078114330768585, Val Acc: 0.8735\n","Epoch [429/500], Loss: 0.4875085651874542, Train Acc: 0.8381,train F1-score:0.8270 Val Loss: 0.40911588072776794, Val Acc: 0.8722\n","Epoch [430/500], Loss: 0.48106902837753296, Train Acc: 0.8383,train F1-score:0.8269 Val Loss: 0.4097285568714142, Val Acc: 0.8715\n","Epoch [431/500], Loss: 0.4856511354446411, Train Acc: 0.8369,train F1-score:0.8255 Val Loss: 0.4111698567867279, Val Acc: 0.8762\n","Epoch [432/500], Loss: 0.4875897169113159, Train Acc: 0.8396,train F1-score:0.8285 Val Loss: 0.40788593888282776, Val Acc: 0.8748\n","Epoch [433/500], Loss: 0.48874756693840027, Train Acc: 0.8382,train F1-score:0.8276 Val Loss: 0.40757986903190613, Val Acc: 0.8742\n","Epoch [434/500], Loss: 0.4744286835193634, Train Acc: 0.8422,train F1-score:0.8320 Val Loss: 0.4098583459854126, Val Acc: 0.8722\n","Epoch [435/500], Loss: 0.4826304018497467, Train Acc: 0.8357,train F1-score:0.8252 Val Loss: 0.41240957379341125, Val Acc: 0.8735\n","Epoch [436/500], Loss: 0.48238077759742737, Train Acc: 0.8401,train F1-score:0.8291 Val Loss: 0.4137533903121948, Val Acc: 0.8742\n","Epoch [437/500], Loss: 0.4827694892883301, Train Acc: 0.8369,train F1-score:0.8256 Val Loss: 0.41532111167907715, Val Acc: 0.8742\n","Epoch [438/500], Loss: 0.48744145035743713, Train Acc: 0.8383,train F1-score:0.8272 Val Loss: 0.4186474084854126, Val Acc: 0.8748\n","Epoch [439/500], Loss: 0.4874754250049591, Train Acc: 0.8346,train F1-score:0.8233 Val Loss: 0.4153069853782654, Val Acc: 0.8735\n","Epoch [440/500], Loss: 0.48482635617256165, Train Acc: 0.8369,train F1-score:0.8266 Val Loss: 0.4105590581893921, Val Acc: 0.8728\n","Epoch [441/500], Loss: 0.48579028248786926, Train Acc: 0.8355,train F1-score:0.8240 Val Loss: 0.40842002630233765, Val Acc: 0.8688\n","Epoch [442/500], Loss: 0.4854890704154968, Train Acc: 0.8355,train F1-score:0.8238 Val Loss: 0.4085978865623474, Val Acc: 0.8701\n","Epoch [443/500], Loss: 0.4797316789627075, Train Acc: 0.8387,train F1-score:0.8277 Val Loss: 0.411574125289917, Val Acc: 0.8701\n","Epoch [444/500], Loss: 0.4807535707950592, Train Acc: 0.8404,train F1-score:0.8288 Val Loss: 0.41018158197402954, Val Acc: 0.8708\n","Epoch [445/500], Loss: 0.48800820112228394, Train Acc: 0.8394,train F1-score:0.8285 Val Loss: 0.40768948197364807, Val Acc: 0.8762\n","Epoch [446/500], Loss: 0.4871181547641754, Train Acc: 0.8360,train F1-score:0.8258 Val Loss: 0.40651988983154297, Val Acc: 0.8782\n","Epoch [447/500], Loss: 0.47851234674453735, Train Acc: 0.8391,train F1-score:0.8294 Val Loss: 0.4072904586791992, Val Acc: 0.8782\n","Epoch [448/500], Loss: 0.48647838830947876, Train Acc: 0.8347,train F1-score:0.8241 Val Loss: 0.4074240028858185, Val Acc: 0.8782\n","Epoch [449/500], Loss: 0.4863116443157196, Train Acc: 0.8391,train F1-score:0.8284 Val Loss: 0.40933775901794434, Val Acc: 0.8788\n","Epoch [450/500], Loss: 0.4843452274799347, Train Acc: 0.8381,train F1-score:0.8272 Val Loss: 0.4116586446762085, Val Acc: 0.8762\n","Epoch [451/500], Loss: 0.4793705940246582, Train Acc: 0.8405,train F1-score:0.8300 Val Loss: 0.4130198657512665, Val Acc: 0.8742\n","Epoch [452/500], Loss: 0.48713353276252747, Train Acc: 0.8365,train F1-score:0.8258 Val Loss: 0.41203951835632324, Val Acc: 0.8735\n","Epoch [453/500], Loss: 0.4838159382343292, Train Acc: 0.8375,train F1-score:0.8263 Val Loss: 0.40962478518486023, Val Acc: 0.8722\n","Epoch [454/500], Loss: 0.4848993718624115, Train Acc: 0.8431,train F1-score:0.8327 Val Loss: 0.40817591547966003, Val Acc: 0.8742\n","Epoch [455/500], Loss: 0.4735758900642395, Train Acc: 0.8410,train F1-score:0.8314 Val Loss: 0.40963876247406006, Val Acc: 0.8708\n","Epoch [456/500], Loss: 0.4795311391353607, Train Acc: 0.8398,train F1-score:0.8298 Val Loss: 0.41041627526283264, Val Acc: 0.8722\n","Epoch [457/500], Loss: 0.4739249646663666, Train Acc: 0.8410,train F1-score:0.8300 Val Loss: 0.4086635708808899, Val Acc: 0.8722\n","Epoch [458/500], Loss: 0.48373037576675415, Train Acc: 0.8406,train F1-score:0.8299 Val Loss: 0.4076690971851349, Val Acc: 0.8755\n","Epoch [459/500], Loss: 0.4788782298564911, Train Acc: 0.8406,train F1-score:0.8297 Val Loss: 0.4077492952346802, Val Acc: 0.8762\n","Early stopping at epoch 459\n","Test Loss: 0.4057711958885193, Test Accuracy: 0.8719839142091153\n","Precision: 0.8708, Recall: 0.8720, F1-score: 0.8623\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9005191760174175\n","Validation Accuracy (SVM): 0.8969210174029452\n","Test Accuracy (SVM): 0.8894101876675603\n","Precision: 0.8849, Recall: 0.8894, F1-score: 0.8847\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14745,"status":"ok","timestamp":1714454357716,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"nGi-53-ER5GL","outputId":"a88a9dec-c38b-472a-d562-922a2150123a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Epoch [1/500], Loss: 2.586740016937256, Train Acc: 0.0724,train F1-score:0.1120 Val Loss: 1.1814968585968018, Val Acc: 0.7008\n","Epoch [2/500], Loss: 1.3016562461853027, Train Acc: 0.6567,train F1-score:0.5675 Val Loss: 1.1019054651260376, Val Acc: 0.6934\n","Epoch [3/500], Loss: 1.2367472648620605, Train Acc: 0.6557,train F1-score:0.5326 Val Loss: 0.9842931032180786, Val Acc: 0.7001\n","Epoch [4/500], Loss: 1.0936131477355957, Train Acc: 0.6715,train F1-score:0.6205 Val Loss: 0.9630354642868042, Val Acc: 0.6888\n","Epoch [5/500], Loss: 1.0389503240585327, Train Acc: 0.6587,train F1-score:0.6401 Val Loss: 0.843820333480835, Val Acc: 0.7463\n","Epoch [6/500], Loss: 0.918756902217865, Train Acc: 0.7039,train F1-score:0.6583 Val Loss: 0.8288828134536743, Val Acc: 0.7503\n","Epoch [7/500], Loss: 0.8875752091407776, Train Acc: 0.7151,train F1-score:0.6582 Val Loss: 0.8249802589416504, Val Acc: 0.7557\n","Epoch [8/500], Loss: 0.8847577571868896, Train Acc: 0.7223,train F1-score:0.6755 Val Loss: 0.7856302857398987, Val Acc: 0.7664\n","Epoch [9/500], Loss: 0.8312332630157471, Train Acc: 0.7307,train F1-score:0.6931 Val Loss: 0.723697304725647, Val Acc: 0.7818\n","Epoch [10/500], Loss: 0.7802114486694336, Train Acc: 0.7445,train F1-score:0.7077 Val Loss: 0.7106829881668091, Val Acc: 0.7778\n","Epoch [11/500], Loss: 0.7848929166793823, Train Acc: 0.7430,train F1-score:0.7060 Val Loss: 0.7009106278419495, Val Acc: 0.7885\n","Epoch [12/500], Loss: 0.7571840286254883, Train Acc: 0.7561,train F1-score:0.7270 Val Loss: 0.6903261542320251, Val Acc: 0.7865\n","Epoch [13/500], Loss: 0.7435153722763062, Train Acc: 0.7553,train F1-score:0.7312 Val Loss: 0.6652353405952454, Val Acc: 0.7979\n","Epoch [14/500], Loss: 0.7121016383171082, Train Acc: 0.7687,train F1-score:0.7422 Val Loss: 0.6623693108558655, Val Acc: 0.8025\n","Epoch [15/500], Loss: 0.6967672109603882, Train Acc: 0.7763,train F1-score:0.7487 Val Loss: 0.6579342484474182, Val Acc: 0.8052\n","Epoch [16/500], Loss: 0.6805213689804077, Train Acc: 0.7845,train F1-score:0.7664 Val Loss: 0.6462743878364563, Val Acc: 0.7985\n","Epoch [17/500], Loss: 0.6804341077804565, Train Acc: 0.7824,train F1-score:0.7724 Val Loss: 0.6064940690994263, Val Acc: 0.8072\n","Epoch [18/500], Loss: 0.6508974432945251, Train Acc: 0.7952,train F1-score:0.7795 Val Loss: 0.5903165340423584, Val Acc: 0.8072\n","Epoch [19/500], Loss: 0.6449523568153381, Train Acc: 0.7916,train F1-score:0.7679 Val Loss: 0.5781276822090149, Val Acc: 0.8119\n","Epoch [20/500], Loss: 0.6355429887771606, Train Acc: 0.7917,train F1-score:0.7712 Val Loss: 0.5713539123535156, Val Acc: 0.8199\n","Epoch [21/500], Loss: 0.6205052733421326, Train Acc: 0.7968,train F1-score:0.7830 Val Loss: 0.5660951733589172, Val Acc: 0.8240\n","Epoch [22/500], Loss: 0.604440450668335, Train Acc: 0.8051,train F1-score:0.7948 Val Loss: 0.5594074130058289, Val Acc: 0.8307\n","Epoch [23/500], Loss: 0.5906842947006226, Train Acc: 0.8051,train F1-score:0.7909 Val Loss: 0.561538815498352, Val Acc: 0.8273\n","Epoch [24/500], Loss: 0.6383908987045288, Train Acc: 0.8050,train F1-score:0.7896 Val Loss: 0.5637890696525574, Val Acc: 0.8226\n","Epoch [25/500], Loss: 0.5780417323112488, Train Acc: 0.8047,train F1-score:0.7901 Val Loss: 0.5561787486076355, Val Acc: 0.8246\n","Epoch [26/500], Loss: 0.5731054544448853, Train Acc: 0.8097,train F1-score:0.7975 Val Loss: 0.5449662208557129, Val Acc: 0.8313\n","Epoch [27/500], Loss: 0.5562185049057007, Train Acc: 0.8163,train F1-score:0.8048 Val Loss: 0.5346403121948242, Val Acc: 0.8320\n","Epoch [28/500], Loss: 0.5560634136199951, Train Acc: 0.8139,train F1-score:0.8023 Val Loss: 0.5260383486747742, Val Acc: 0.8313\n","Epoch [29/500], Loss: 0.5449179410934448, Train Acc: 0.8179,train F1-score:0.8069 Val Loss: 0.5201122164726257, Val Acc: 0.8353\n","Epoch [30/500], Loss: 0.5369278788566589, Train Acc: 0.8150,train F1-score:0.8017 Val Loss: 0.5160108804702759, Val Acc: 0.8380\n","Epoch [31/500], Loss: 0.5335127115249634, Train Acc: 0.8185,train F1-score:0.8049 Val Loss: 0.5139536261558533, Val Acc: 0.8313\n","Epoch [32/500], Loss: 0.5273746252059937, Train Acc: 0.8220,train F1-score:0.8112 Val Loss: 0.5151448249816895, Val Acc: 0.8367\n","Epoch [33/500], Loss: 0.5264666080474854, Train Acc: 0.8224,train F1-score:0.8135 Val Loss: 0.511099636554718, Val Acc: 0.8367\n","Epoch [34/500], Loss: 0.518683910369873, Train Acc: 0.8242,train F1-score:0.8152 Val Loss: 0.5015649199485779, Val Acc: 0.8420\n","Epoch [35/500], Loss: 0.5052922368049622, Train Acc: 0.8255,train F1-score:0.8156 Val Loss: 0.4971158504486084, Val Acc: 0.8420\n","Epoch [36/500], Loss: 0.5022485852241516, Train Acc: 0.8308,train F1-score:0.8211 Val Loss: 0.4926910698413849, Val Acc: 0.8407\n","Epoch [37/500], Loss: 0.4947792589664459, Train Acc: 0.8288,train F1-score:0.8182 Val Loss: 0.4896671175956726, Val Acc: 0.8434\n","Epoch [38/500], Loss: 0.49081897735595703, Train Acc: 0.8316,train F1-score:0.8220 Val Loss: 0.4967455267906189, Val Acc: 0.8467\n","Epoch [39/500], Loss: 0.49844664335250854, Train Acc: 0.8356,train F1-score:0.8269 Val Loss: 0.4855918288230896, Val Acc: 0.8467\n","Epoch [40/500], Loss: 0.483089804649353, Train Acc: 0.8339,train F1-score:0.8253 Val Loss: 0.48172667622566223, Val Acc: 0.8467\n","Epoch [41/500], Loss: 0.48557087779045105, Train Acc: 0.8362,train F1-score:0.8272 Val Loss: 0.4783639907836914, Val Acc: 0.8494\n","Epoch [42/500], Loss: 0.4790404736995697, Train Acc: 0.8371,train F1-score:0.8272 Val Loss: 0.4750906825065613, Val Acc: 0.8494\n","Epoch [43/500], Loss: 0.4745151400566101, Train Acc: 0.8389,train F1-score:0.8299 Val Loss: 0.4724392294883728, Val Acc: 0.8527\n","Epoch [44/500], Loss: 0.4709356129169464, Train Acc: 0.8403,train F1-score:0.8330 Val Loss: 0.46917787194252014, Val Acc: 0.8548\n","Epoch [45/500], Loss: 0.46764177083969116, Train Acc: 0.8406,train F1-score:0.8324 Val Loss: 0.46954345703125, Val Acc: 0.8534\n","Epoch [46/500], Loss: 0.46035680174827576, Train Acc: 0.8433,train F1-score:0.8344 Val Loss: 0.4691222906112671, Val Acc: 0.8581\n","Epoch [47/500], Loss: 0.45455965399742126, Train Acc: 0.8453,train F1-score:0.8380 Val Loss: 0.46600839495658875, Val Acc: 0.8554\n","Epoch [48/500], Loss: 0.45115095376968384, Train Acc: 0.8463,train F1-score:0.8390 Val Loss: 0.462823748588562, Val Acc: 0.8548\n","Epoch [49/500], Loss: 0.45189911127090454, Train Acc: 0.8457,train F1-score:0.8378 Val Loss: 0.4610016345977783, Val Acc: 0.8541\n","Epoch [50/500], Loss: 0.44753724336624146, Train Acc: 0.8461,train F1-score:0.8379 Val Loss: 0.45758387446403503, Val Acc: 0.8568\n","Epoch [51/500], Loss: 0.44244688749313354, Train Acc: 0.8476,train F1-score:0.8400 Val Loss: 0.4541226029396057, Val Acc: 0.8594\n","Epoch [52/500], Loss: 0.43991565704345703, Train Acc: 0.8489,train F1-score:0.8418 Val Loss: 0.45186570286750793, Val Acc: 0.8635\n","Epoch [53/500], Loss: 0.43657341599464417, Train Acc: 0.8502,train F1-score:0.8438 Val Loss: 0.4489654302597046, Val Acc: 0.8621\n","Epoch [54/500], Loss: 0.4312748312950134, Train Acc: 0.8536,train F1-score:0.8478 Val Loss: 0.4518422782421112, Val Acc: 0.8548\n","Epoch [55/500], Loss: 0.43306657671928406, Train Acc: 0.8524,train F1-score:0.8453 Val Loss: 0.44885650277137756, Val Acc: 0.8568\n","Epoch [56/500], Loss: 0.4357227385044098, Train Acc: 0.8514,train F1-score:0.8451 Val Loss: 0.4407466650009155, Val Acc: 0.8568\n","Epoch [57/500], Loss: 0.4280792772769928, Train Acc: 0.8504,train F1-score:0.8435 Val Loss: 0.4326068162918091, Val Acc: 0.8648\n","Epoch [58/500], Loss: 0.42404577136039734, Train Acc: 0.8523,train F1-score:0.8452 Val Loss: 0.4318656027317047, Val Acc: 0.8648\n","Epoch [59/500], Loss: 0.4155307412147522, Train Acc: 0.8587,train F1-score:0.8523 Val Loss: 0.43628329038619995, Val Acc: 0.8614\n","Epoch [60/500], Loss: 0.4115264117717743, Train Acc: 0.8580,train F1-score:0.8520 Val Loss: 0.4398789405822754, Val Acc: 0.8621\n","Epoch [61/500], Loss: 0.42201271653175354, Train Acc: 0.8572,train F1-score:0.8509 Val Loss: 0.43529829382896423, Val Acc: 0.8635\n","Epoch [62/500], Loss: 0.4090002775192261, Train Acc: 0.8574,train F1-score:0.8512 Val Loss: 0.43261948227882385, Val Acc: 0.8635\n","Epoch [63/500], Loss: 0.41306278109550476, Train Acc: 0.8568,train F1-score:0.8508 Val Loss: 0.4291149079799652, Val Acc: 0.8681\n","Epoch [64/500], Loss: 0.4169948101043701, Train Acc: 0.8567,train F1-score:0.8501 Val Loss: 0.42739203572273254, Val Acc: 0.8675\n","Epoch [65/500], Loss: 0.41097888350486755, Train Acc: 0.8619,train F1-score:0.8562 Val Loss: 0.42369723320007324, Val Acc: 0.8701\n","Epoch [66/500], Loss: 0.39779892563819885, Train Acc: 0.8657,train F1-score:0.8601 Val Loss: 0.42936304211616516, Val Acc: 0.8668\n","Epoch [67/500], Loss: 0.39873257279396057, Train Acc: 0.8648,train F1-score:0.8584 Val Loss: 0.4321330785751343, Val Acc: 0.8661\n","Epoch [68/500], Loss: 0.3973282277584076, Train Acc: 0.8643,train F1-score:0.8592 Val Loss: 0.428075909614563, Val Acc: 0.8661\n","Epoch [69/500], Loss: 0.3906753361225128, Train Acc: 0.8650,train F1-score:0.8597 Val Loss: 0.4236607849597931, Val Acc: 0.8661\n","Epoch [70/500], Loss: 0.38984760642051697, Train Acc: 0.8663,train F1-score:0.8608 Val Loss: 0.4191388785839081, Val Acc: 0.8708\n","Epoch [71/500], Loss: 0.396837443113327, Train Acc: 0.8675,train F1-score:0.8624 Val Loss: 0.419447660446167, Val Acc: 0.8708\n","Epoch [72/500], Loss: 0.38943108916282654, Train Acc: 0.8670,train F1-score:0.8617 Val Loss: 0.4181928038597107, Val Acc: 0.8688\n","Epoch [73/500], Loss: 0.3915737569332123, Train Acc: 0.8652,train F1-score:0.8599 Val Loss: 0.41513410210609436, Val Acc: 0.8708\n","Epoch [74/500], Loss: 0.37726807594299316, Train Acc: 0.8691,train F1-score:0.8640 Val Loss: 0.4155299663543701, Val Acc: 0.8708\n","Epoch [75/500], Loss: 0.375815749168396, Train Acc: 0.8671,train F1-score:0.8616 Val Loss: 0.4175902307033539, Val Acc: 0.8688\n","Epoch [76/500], Loss: 0.3865981996059418, Train Acc: 0.8667,train F1-score:0.8613 Val Loss: 0.4127604067325592, Val Acc: 0.8742\n","Epoch [77/500], Loss: 0.37691912055015564, Train Acc: 0.8709,train F1-score:0.8660 Val Loss: 0.40711286664009094, Val Acc: 0.8762\n","Epoch [78/500], Loss: 0.3814491033554077, Train Acc: 0.8679,train F1-score:0.8629 Val Loss: 0.40473151206970215, Val Acc: 0.8755\n","Epoch [79/500], Loss: 0.3754042685031891, Train Acc: 0.8705,train F1-score:0.8654 Val Loss: 0.4090442657470703, Val Acc: 0.8735\n","Epoch [80/500], Loss: 0.3694044351577759, Train Acc: 0.8698,train F1-score:0.8643 Val Loss: 0.40594562888145447, Val Acc: 0.8728\n","Epoch [81/500], Loss: 0.3704475164413452, Train Acc: 0.8738,train F1-score:0.8689 Val Loss: 0.4022112488746643, Val Acc: 0.8775\n","Epoch [82/500], Loss: 0.3737868666648865, Train Acc: 0.8734,train F1-score:0.8694 Val Loss: 0.40037980675697327, Val Acc: 0.8788\n","Epoch [83/500], Loss: 0.3705178201198578, Train Acc: 0.8757,train F1-score:0.8708 Val Loss: 0.3998512625694275, Val Acc: 0.8802\n","Epoch [84/500], Loss: 0.36247971653938293, Train Acc: 0.8732,train F1-score:0.8686 Val Loss: 0.40037423372268677, Val Acc: 0.8795\n","Epoch [85/500], Loss: 0.3766550123691559, Train Acc: 0.8741,train F1-score:0.8697 Val Loss: 0.4059862494468689, Val Acc: 0.8782\n","Epoch [86/500], Loss: 0.36421453952789307, Train Acc: 0.8745,train F1-score:0.8691 Val Loss: 0.4063461720943451, Val Acc: 0.8782\n","Epoch [87/500], Loss: 0.36405718326568604, Train Acc: 0.8730,train F1-score:0.8685 Val Loss: 0.40627503395080566, Val Acc: 0.8795\n","Epoch [88/500], Loss: 0.3629050850868225, Train Acc: 0.8764,train F1-score:0.8724 Val Loss: 0.4088672697544098, Val Acc: 0.8775\n","Epoch [89/500], Loss: 0.36334162950515747, Train Acc: 0.8757,train F1-score:0.8706 Val Loss: 0.4035015106201172, Val Acc: 0.8829\n","Epoch [90/500], Loss: 0.35745760798454285, Train Acc: 0.8789,train F1-score:0.8752 Val Loss: 0.4027199149131775, Val Acc: 0.8835\n","Epoch [91/500], Loss: 0.35762158036231995, Train Acc: 0.8791,train F1-score:0.8756 Val Loss: 0.4060327112674713, Val Acc: 0.8829\n","Epoch [92/500], Loss: 0.3495391309261322, Train Acc: 0.8788,train F1-score:0.8743 Val Loss: 0.4065919518470764, Val Acc: 0.8849\n","Epoch [93/500], Loss: 0.35103705525398254, Train Acc: 0.8803,train F1-score:0.8753 Val Loss: 0.40550824999809265, Val Acc: 0.8815\n","Epoch [94/500], Loss: 0.35784563422203064, Train Acc: 0.8786,train F1-score:0.8748 Val Loss: 0.4038101136684418, Val Acc: 0.8809\n","Epoch [95/500], Loss: 0.36399781703948975, Train Acc: 0.8764,train F1-score:0.8717 Val Loss: 0.38838666677474976, Val Acc: 0.8882\n","Epoch [96/500], Loss: 0.35432082414627075, Train Acc: 0.8793,train F1-score:0.8755 Val Loss: 0.390299916267395, Val Acc: 0.8882\n","Epoch [97/500], Loss: 0.3497971296310425, Train Acc: 0.8776,train F1-score:0.8735 Val Loss: 0.395187109708786, Val Acc: 0.8815\n","Epoch [98/500], Loss: 0.34679216146469116, Train Acc: 0.8809,train F1-score:0.8759 Val Loss: 0.40041500329971313, Val Acc: 0.8842\n","Epoch [99/500], Loss: 0.3413675129413605, Train Acc: 0.8824,train F1-score:0.8783 Val Loss: 0.41472184658050537, Val Acc: 0.8842\n","Epoch [100/500], Loss: 0.3473670482635498, Train Acc: 0.8816,train F1-score:0.8783 Val Loss: 0.41226083040237427, Val Acc: 0.8829\n","Epoch [101/500], Loss: 0.33955323696136475, Train Acc: 0.8830,train F1-score:0.8791 Val Loss: 0.4054681062698364, Val Acc: 0.8842\n","Epoch [102/500], Loss: 0.34573400020599365, Train Acc: 0.8808,train F1-score:0.8764 Val Loss: 0.40233513712882996, Val Acc: 0.8835\n","Epoch [103/500], Loss: 0.34091833233833313, Train Acc: 0.8812,train F1-score:0.8776 Val Loss: 0.40008893609046936, Val Acc: 0.8849\n","Epoch [104/500], Loss: 0.34247472882270813, Train Acc: 0.8830,train F1-score:0.8788 Val Loss: 0.39505094289779663, Val Acc: 0.8849\n","Epoch [105/500], Loss: 0.3356539309024811, Train Acc: 0.8844,train F1-score:0.8808 Val Loss: 0.3985300362110138, Val Acc: 0.8829\n","Epoch [106/500], Loss: 0.34323009848594666, Train Acc: 0.8830,train F1-score:0.8793 Val Loss: 0.40315303206443787, Val Acc: 0.8809\n","Epoch [107/500], Loss: 0.329863965511322, Train Acc: 0.8828,train F1-score:0.8781 Val Loss: 0.39945462346076965, Val Acc: 0.8809\n","Epoch [108/500], Loss: 0.33803966641426086, Train Acc: 0.8844,train F1-score:0.8812 Val Loss: 0.39753761887550354, Val Acc: 0.8815\n","Epoch [109/500], Loss: 0.331238716840744, Train Acc: 0.8875,train F1-score:0.8834 Val Loss: 0.3997824788093567, Val Acc: 0.8815\n","Epoch [110/500], Loss: 0.3283698260784149, Train Acc: 0.8848,train F1-score:0.8804 Val Loss: 0.39354071021080017, Val Acc: 0.8842\n","Epoch [111/500], Loss: 0.33669513463974, Train Acc: 0.8842,train F1-score:0.8806 Val Loss: 0.38961106538772583, Val Acc: 0.8849\n","Epoch [112/500], Loss: 0.32632359862327576, Train Acc: 0.8872,train F1-score:0.8836 Val Loss: 0.3859177827835083, Val Acc: 0.8862\n","Epoch [113/500], Loss: 0.3351195156574249, Train Acc: 0.8847,train F1-score:0.8811 Val Loss: 0.38689810037612915, Val Acc: 0.8849\n","Epoch [114/500], Loss: 0.3265499770641327, Train Acc: 0.8850,train F1-score:0.8822 Val Loss: 0.38456985354423523, Val Acc: 0.8822\n","Epoch [115/500], Loss: 0.327155202627182, Train Acc: 0.8856,train F1-score:0.8816 Val Loss: 0.3835902214050293, Val Acc: 0.8842\n","Epoch [116/500], Loss: 0.32366806268692017, Train Acc: 0.8856,train F1-score:0.8822 Val Loss: 0.3932363986968994, Val Acc: 0.8862\n","Epoch [117/500], Loss: 0.3285426199436188, Train Acc: 0.8884,train F1-score:0.8843 Val Loss: 0.37817731499671936, Val Acc: 0.8922\n","Epoch [118/500], Loss: 0.3236630856990814, Train Acc: 0.8875,train F1-score:0.8844 Val Loss: 0.3733848035335541, Val Acc: 0.8922\n","Epoch [119/500], Loss: 0.31667816638946533, Train Acc: 0.8912,train F1-score:0.8881 Val Loss: 0.3724929094314575, Val Acc: 0.8902\n","Epoch [120/500], Loss: 0.3156038820743561, Train Acc: 0.8894,train F1-score:0.8857 Val Loss: 0.37068766355514526, Val Acc: 0.8916\n","Epoch [121/500], Loss: 0.3187682628631592, Train Acc: 0.8893,train F1-score:0.8865 Val Loss: 0.3759169280529022, Val Acc: 0.8882\n","Epoch [122/500], Loss: 0.32524773478507996, Train Acc: 0.8845,train F1-score:0.8812 Val Loss: 0.3800380229949951, Val Acc: 0.8882\n","Epoch [123/500], Loss: 0.3083958327770233, Train Acc: 0.8941,train F1-score:0.8906 Val Loss: 0.40335389971733093, Val Acc: 0.8855\n","Epoch [124/500], Loss: 0.31658369302749634, Train Acc: 0.8913,train F1-score:0.8878 Val Loss: 0.4123351275920868, Val Acc: 0.8909\n","Epoch [125/500], Loss: 0.3226635754108429, Train Acc: 0.8906,train F1-score:0.8867 Val Loss: 0.37601301074028015, Val Acc: 0.8922\n","Epoch [126/500], Loss: 0.31175780296325684, Train Acc: 0.8939,train F1-score:0.8909 Val Loss: 0.3754032254219055, Val Acc: 0.8916\n","Epoch [127/500], Loss: 0.31635740399360657, Train Acc: 0.8889,train F1-score:0.8856 Val Loss: 0.378108948469162, Val Acc: 0.8882\n","Epoch [128/500], Loss: 0.3148549795150757, Train Acc: 0.8900,train F1-score:0.8863 Val Loss: 0.37825536727905273, Val Acc: 0.8889\n","Epoch [129/500], Loss: 0.30700936913490295, Train Acc: 0.8932,train F1-score:0.8904 Val Loss: 0.3809424638748169, Val Acc: 0.8869\n","Epoch [130/500], Loss: 0.31605032086372375, Train Acc: 0.8935,train F1-score:0.8910 Val Loss: 0.38249242305755615, Val Acc: 0.8909\n","Epoch [131/500], Loss: 0.33377209305763245, Train Acc: 0.8916,train F1-score:0.8886 Val Loss: 0.38124340772628784, Val Acc: 0.8896\n","Epoch [132/500], Loss: 0.31648263335227966, Train Acc: 0.8904,train F1-score:0.8870 Val Loss: 0.3857375979423523, Val Acc: 0.8909\n","Epoch [133/500], Loss: 0.3118214011192322, Train Acc: 0.8907,train F1-score:0.8874 Val Loss: 0.38537970185279846, Val Acc: 0.8909\n","Epoch [134/500], Loss: 0.3072253465652466, Train Acc: 0.8916,train F1-score:0.8886 Val Loss: 0.3851971924304962, Val Acc: 0.8909\n","Epoch [135/500], Loss: 0.3029610216617584, Train Acc: 0.8962,train F1-score:0.8930 Val Loss: 0.38855233788490295, Val Acc: 0.8889\n","Epoch [136/500], Loss: 0.3025239109992981, Train Acc: 0.8973,train F1-score:0.8943 Val Loss: 0.39093515276908875, Val Acc: 0.8922\n","Epoch [137/500], Loss: 0.3074755072593689, Train Acc: 0.8956,train F1-score:0.8929 Val Loss: 0.38337257504463196, Val Acc: 0.8882\n","Epoch [138/500], Loss: 0.3030512034893036, Train Acc: 0.8942,train F1-score:0.8919 Val Loss: 0.38855594396591187, Val Acc: 0.8929\n","Epoch [139/500], Loss: 0.2951205372810364, Train Acc: 0.8948,train F1-score:0.8925 Val Loss: 0.3992997407913208, Val Acc: 0.8929\n","Epoch [140/500], Loss: 0.3024846911430359, Train Acc: 0.8942,train F1-score:0.8909 Val Loss: 0.40243130922317505, Val Acc: 0.8909\n","Epoch [141/500], Loss: 0.30973583459854126, Train Acc: 0.8922,train F1-score:0.8888 Val Loss: 0.4045834541320801, Val Acc: 0.8922\n","Epoch [142/500], Loss: 0.30641064047813416, Train Acc: 0.8953,train F1-score:0.8920 Val Loss: 0.40799951553344727, Val Acc: 0.8922\n","Epoch [143/500], Loss: 0.299645334482193, Train Acc: 0.8990,train F1-score:0.8961 Val Loss: 0.4055919051170349, Val Acc: 0.8922\n","Epoch [144/500], Loss: 0.3054831922054291, Train Acc: 0.8967,train F1-score:0.8938 Val Loss: 0.3919738531112671, Val Acc: 0.8969\n","Epoch [145/500], Loss: 0.3035793900489807, Train Acc: 0.8932,train F1-score:0.8903 Val Loss: 0.38477322459220886, Val Acc: 0.8983\n","Epoch [146/500], Loss: 0.2941722273826599, Train Acc: 0.9010,train F1-score:0.8985 Val Loss: 0.3958039879798889, Val Acc: 0.8969\n","Epoch [147/500], Loss: 0.29424530267715454, Train Acc: 0.8979,train F1-score:0.8953 Val Loss: 0.39854875206947327, Val Acc: 0.8969\n","Epoch [148/500], Loss: 0.29603317379951477, Train Acc: 0.8950,train F1-score:0.8921 Val Loss: 0.4029127061367035, Val Acc: 0.8936\n","Epoch [149/500], Loss: 0.3003857731819153, Train Acc: 0.8963,train F1-score:0.8940 Val Loss: 0.41528433561325073, Val Acc: 0.8942\n","Epoch [150/500], Loss: 0.3033628761768341, Train Acc: 0.8973,train F1-score:0.8944 Val Loss: 0.4059147536754608, Val Acc: 0.8949\n","Epoch [151/500], Loss: 0.29916781187057495, Train Acc: 0.9024,train F1-score:0.9001 Val Loss: 0.3810734450817108, Val Acc: 0.8963\n","Epoch [152/500], Loss: 0.286690354347229, Train Acc: 0.8994,train F1-score:0.8967 Val Loss: 0.3821812868118286, Val Acc: 0.8942\n","Epoch [153/500], Loss: 0.2986704111099243, Train Acc: 0.8981,train F1-score:0.8949 Val Loss: 0.380350261926651, Val Acc: 0.8963\n","Epoch [154/500], Loss: 0.2897658944129944, Train Acc: 0.8998,train F1-score:0.8972 Val Loss: 0.37912431359291077, Val Acc: 0.8963\n","Epoch [155/500], Loss: 0.2893112897872925, Train Acc: 0.8960,train F1-score:0.8928 Val Loss: 0.37903261184692383, Val Acc: 0.8969\n","Epoch [156/500], Loss: 0.2881491482257843, Train Acc: 0.9029,train F1-score:0.9002 Val Loss: 0.3793845772743225, Val Acc: 0.8989\n","Epoch [157/500], Loss: 0.2850961983203888, Train Acc: 0.9023,train F1-score:0.8999 Val Loss: 0.38046741485595703, Val Acc: 0.8989\n","Epoch [158/500], Loss: 0.28321802616119385, Train Acc: 0.9013,train F1-score:0.8988 Val Loss: 0.3856342136859894, Val Acc: 0.8983\n","Epoch [159/500], Loss: 0.2926284372806549, Train Acc: 0.8979,train F1-score:0.8954 Val Loss: 0.3866680860519409, Val Acc: 0.8996\n","Epoch [160/500], Loss: 0.281097948551178, Train Acc: 0.9029,train F1-score:0.9005 Val Loss: 0.3881702423095703, Val Acc: 0.8983\n","Epoch [161/500], Loss: 0.3028450906276703, Train Acc: 0.9019,train F1-score:0.8994 Val Loss: 0.3944045603275299, Val Acc: 0.8976\n","Epoch [162/500], Loss: 0.29027047753334045, Train Acc: 0.8990,train F1-score:0.8956 Val Loss: 0.3914755582809448, Val Acc: 0.8949\n","Epoch [163/500], Loss: 0.2909766435623169, Train Acc: 0.8999,train F1-score:0.8970 Val Loss: 0.3939118981361389, Val Acc: 0.8969\n","Epoch [164/500], Loss: 0.2928447723388672, Train Acc: 0.9020,train F1-score:0.8987 Val Loss: 0.39397183060646057, Val Acc: 0.8956\n","Epoch [165/500], Loss: 0.28259214758872986, Train Acc: 0.9038,train F1-score:0.9016 Val Loss: 0.3926602005958557, Val Acc: 0.8969\n","Epoch [166/500], Loss: 0.2803214490413666, Train Acc: 0.9029,train F1-score:0.9010 Val Loss: 0.39869508147239685, Val Acc: 0.8929\n","Epoch [167/500], Loss: 0.28016868233680725, Train Acc: 0.9009,train F1-score:0.8985 Val Loss: 0.39976608753204346, Val Acc: 0.8989\n","Epoch [168/500], Loss: 0.28566259145736694, Train Acc: 0.9013,train F1-score:0.8989 Val Loss: 0.3977276682853699, Val Acc: 0.8983\n","Epoch [169/500], Loss: 0.28366076946258545, Train Acc: 0.8999,train F1-score:0.8980 Val Loss: 0.40370312333106995, Val Acc: 0.8969\n","Epoch [170/500], Loss: 0.285566508769989, Train Acc: 0.8995,train F1-score:0.8963 Val Loss: 0.39784345030784607, Val Acc: 0.8983\n","Epoch [171/500], Loss: 0.2790403664112091, Train Acc: 0.9041,train F1-score:0.9021 Val Loss: 0.3961200416088104, Val Acc: 0.9016\n","Epoch [172/500], Loss: 0.27613192796707153, Train Acc: 0.9076,train F1-score:0.9047 Val Loss: 0.3992477357387543, Val Acc: 0.8942\n","Epoch [173/500], Loss: 0.2780267000198364, Train Acc: 0.9035,train F1-score:0.9008 Val Loss: 0.39882442355155945, Val Acc: 0.8902\n","Epoch [174/500], Loss: 0.2726590633392334, Train Acc: 0.9041,train F1-score:0.9026 Val Loss: 0.4007762372493744, Val Acc: 0.8916\n","Epoch [175/500], Loss: 0.28211426734924316, Train Acc: 0.9032,train F1-score:0.9008 Val Loss: 0.40579256415367126, Val Acc: 0.8916\n","Epoch [176/500], Loss: 0.27384716272354126, Train Acc: 0.9007,train F1-score:0.8979 Val Loss: 0.4053322970867157, Val Acc: 0.8929\n","Epoch [177/500], Loss: 0.27309343218803406, Train Acc: 0.9060,train F1-score:0.9044 Val Loss: 0.4058406352996826, Val Acc: 0.8989\n","Epoch [178/500], Loss: 0.27114978432655334, Train Acc: 0.9019,train F1-score:0.8988 Val Loss: 0.4050436317920685, Val Acc: 0.8936\n","Epoch [179/500], Loss: 0.2724987864494324, Train Acc: 0.9055,train F1-score:0.9031 Val Loss: 0.4095683693885803, Val Acc: 0.8949\n","Epoch [180/500], Loss: 0.2712283730506897, Train Acc: 0.9059,train F1-score:0.9039 Val Loss: 0.4174540638923645, Val Acc: 0.8956\n","Epoch [181/500], Loss: 0.27050018310546875, Train Acc: 0.9044,train F1-score:0.9019 Val Loss: 0.41651979088783264, Val Acc: 0.8996\n","Epoch [182/500], Loss: 0.2693825364112854, Train Acc: 0.9072,train F1-score:0.9055 Val Loss: 0.4290698766708374, Val Acc: 0.8983\n","Epoch [183/500], Loss: 0.26639920473098755, Train Acc: 0.9051,train F1-score:0.9030 Val Loss: 0.4281127154827118, Val Acc: 0.9023\n","Epoch [184/500], Loss: 0.27310654520988464, Train Acc: 0.9054,train F1-score:0.9031 Val Loss: 0.4273308515548706, Val Acc: 0.9003\n","Epoch [185/500], Loss: 0.2601492404937744, Train Acc: 0.9062,train F1-score:0.9043 Val Loss: 0.4349878430366516, Val Acc: 0.8996\n","Epoch [186/500], Loss: 0.2679447531700134, Train Acc: 0.9063,train F1-score:0.9038 Val Loss: 0.4400050938129425, Val Acc: 0.8976\n","Epoch [187/500], Loss: 0.256684273481369, Train Acc: 0.9109,train F1-score:0.9086 Val Loss: 0.43618759512901306, Val Acc: 0.9009\n","Epoch [188/500], Loss: 0.2705356180667877, Train Acc: 0.9038,train F1-score:0.9014 Val Loss: 0.43326905369758606, Val Acc: 0.9009\n","Epoch [189/500], Loss: 0.25923410058021545, Train Acc: 0.9102,train F1-score:0.9084 Val Loss: 0.4383962154388428, Val Acc: 0.8976\n","Epoch [190/500], Loss: 0.2780863344669342, Train Acc: 0.9040,train F1-score:0.9014 Val Loss: 0.43104273080825806, Val Acc: 0.9029\n","Epoch [191/500], Loss: 0.25724169611930847, Train Acc: 0.9106,train F1-score:0.9087 Val Loss: 0.425399512052536, Val Acc: 0.9063\n","Epoch [192/500], Loss: 0.2586756944656372, Train Acc: 0.9091,train F1-score:0.9068 Val Loss: 0.43278926610946655, Val Acc: 0.8949\n","Epoch [193/500], Loss: 0.2647051513195038, Train Acc: 0.9090,train F1-score:0.9074 Val Loss: 0.4268447756767273, Val Acc: 0.8989\n","Epoch [194/500], Loss: 0.27147549390792847, Train Acc: 0.9088,train F1-score:0.9070 Val Loss: 0.42842650413513184, Val Acc: 0.9029\n","Epoch [195/500], Loss: 0.2586313486099243, Train Acc: 0.9051,train F1-score:0.9023 Val Loss: 0.4329780340194702, Val Acc: 0.8989\n","Epoch [196/500], Loss: 0.258810430765152, Train Acc: 0.9109,train F1-score:0.9084 Val Loss: 0.4273637533187866, Val Acc: 0.9003\n","Epoch [197/500], Loss: 0.2676890790462494, Train Acc: 0.9065,train F1-score:0.9051 Val Loss: 0.42278897762298584, Val Acc: 0.9003\n","Epoch [198/500], Loss: 0.27140116691589355, Train Acc: 0.9072,train F1-score:0.9046 Val Loss: 0.41510531306266785, Val Acc: 0.8989\n","Epoch [199/500], Loss: 0.26031699776649475, Train Acc: 0.9094,train F1-score:0.9071 Val Loss: 0.41399702429771423, Val Acc: 0.8936\n","Epoch [200/500], Loss: 0.257753849029541, Train Acc: 0.9079,train F1-score:0.9064 Val Loss: 0.4104965925216675, Val Acc: 0.9023\n","Epoch [201/500], Loss: 0.25800663232803345, Train Acc: 0.9071,train F1-score:0.9044 Val Loss: 0.4048575758934021, Val Acc: 0.9009\n","Epoch [202/500], Loss: 0.26346179842948914, Train Acc: 0.9109,train F1-score:0.9086 Val Loss: 0.3976885676383972, Val Acc: 0.9023\n","Epoch [203/500], Loss: 0.25697535276412964, Train Acc: 0.9107,train F1-score:0.9092 Val Loss: 0.40094852447509766, Val Acc: 0.8996\n","Epoch [204/500], Loss: 0.2423664927482605, Train Acc: 0.9144,train F1-score:0.9125 Val Loss: 0.39660435914993286, Val Acc: 0.9056\n","Epoch [205/500], Loss: 0.25312891602516174, Train Acc: 0.9111,train F1-score:0.9088 Val Loss: 0.40325862169265747, Val Acc: 0.9043\n","Epoch [206/500], Loss: 0.2633284628391266, Train Acc: 0.9117,train F1-score:0.9104 Val Loss: 0.4121530055999756, Val Acc: 0.8976\n","Epoch [207/500], Loss: 0.25295644998550415, Train Acc: 0.9096,train F1-score:0.9068 Val Loss: 0.38915589451789856, Val Acc: 0.9029\n","Epoch [208/500], Loss: 0.24869249761104584, Train Acc: 0.9120,train F1-score:0.9102 Val Loss: 0.3881492018699646, Val Acc: 0.9043\n","Epoch [209/500], Loss: 0.24442549049854279, Train Acc: 0.9158,train F1-score:0.9144 Val Loss: 0.40504661202430725, Val Acc: 0.9009\n","Epoch [210/500], Loss: 0.25621578097343445, Train Acc: 0.9103,train F1-score:0.9081 Val Loss: 0.4057620167732239, Val Acc: 0.9016\n","Epoch [211/500], Loss: 0.2515634298324585, Train Acc: 0.9137,train F1-score:0.9123 Val Loss: 0.40865010023117065, Val Acc: 0.9036\n","Epoch [212/500], Loss: 0.24926988780498505, Train Acc: 0.9130,train F1-score:0.9107 Val Loss: 0.41274526715278625, Val Acc: 0.9023\n","Epoch [213/500], Loss: 0.24818241596221924, Train Acc: 0.9114,train F1-score:0.9088 Val Loss: 0.42115986347198486, Val Acc: 0.8976\n","Epoch [214/500], Loss: 0.25067079067230225, Train Acc: 0.9148,train F1-score:0.9131 Val Loss: 0.42351895570755005, Val Acc: 0.9029\n","Epoch [215/500], Loss: 0.24898864328861237, Train Acc: 0.9142,train F1-score:0.9125 Val Loss: 0.4327583909034729, Val Acc: 0.9009\n","Epoch [216/500], Loss: 0.24386687576770782, Train Acc: 0.9117,train F1-score:0.9094 Val Loss: 0.43215009570121765, Val Acc: 0.9029\n","Epoch [217/500], Loss: 0.2598082423210144, Train Acc: 0.9158,train F1-score:0.9141 Val Loss: 0.4340853691101074, Val Acc: 0.8983\n","Epoch [218/500], Loss: 0.24455751478672028, Train Acc: 0.9147,train F1-score:0.9133 Val Loss: 0.443931519985199, Val Acc: 0.8936\n","Epoch [219/500], Loss: 0.2540648281574249, Train Acc: 0.9127,train F1-score:0.9103 Val Loss: 0.43195486068725586, Val Acc: 0.9050\n","Early stopping at epoch 219\n","Test Loss: 0.42852815985679626, Test Accuracy: 0.8967828418230563\n","Precision: 0.8923, Recall: 0.8968, F1-score: 0.8931\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["F1-score saved to file.\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GATNet(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_dim, out_channels, num_layers):\n","        super(GATNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GATConv(in_channels, hidden_dim, heads=8))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GATConv(hidden_dim * 8, hidden_dim, heads=8))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim * 8, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.fc(x)\n","        return x\n","\n","# Define model\n","model = GATNet(in_channels=X_train.shape[1], hidden_dim=128, out_channels=13, num_layers=1).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model1.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer1/train_f1_scores_GAT.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer1/train_loss_GAT.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer1/epochs_GAT.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/layer1/f1_score_GAT.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60294,"status":"ok","timestamp":1714454516199,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"OEEydQ-iTBHm","outputId":"6e531b2f-c8b3-4067-cb39-c3c43c9e2116"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Epoch [1/500], Loss: 2.652108669281006, Train Acc: 0.0450,train F1-score:0.0674 Val Loss: 18.934040069580078, Val Acc: 0.6787\n","Epoch [2/500], Loss: 23.0024471282959, Train Acc: 0.6441,train F1-score:0.5047 Val Loss: 2.8651838302612305, Val Acc: 0.1104\n","Epoch [3/500], Loss: 3.1894607543945312, Train Acc: 0.1241,train F1-score:0.0320 Val Loss: 2.46151065826416, Val Acc: 0.0944\n","Epoch [4/500], Loss: 2.3673789501190186, Train Acc: 0.1112,train F1-score:0.0364 Val Loss: 2.195315361022949, Val Acc: 0.0937\n","Epoch [5/500], Loss: 2.233876943588257, Train Acc: 0.1043,train F1-score:0.0247 Val Loss: 2.3424692153930664, Val Acc: 0.0937\n","Epoch [6/500], Loss: 2.442516565322876, Train Acc: 0.1084,train F1-score:0.0308 Val Loss: 2.241842269897461, Val Acc: 0.0937\n","Epoch [7/500], Loss: 2.2268707752227783, Train Acc: 0.1068,train F1-score:0.0303 Val Loss: 2.2014408111572266, Val Acc: 0.0937\n","Epoch [8/500], Loss: 2.1879382133483887, Train Acc: 0.1061,train F1-score:0.0379 Val Loss: 1.8885807991027832, Val Acc: 0.1131\n","Epoch [9/500], Loss: 1.9555753469467163, Train Acc: 0.1728,train F1-score:0.1689 Val Loss: 1.4013588428497314, Val Acc: 0.6780\n","Epoch [10/500], Loss: 1.584688425064087, Train Acc: 0.4518,train F1-score:0.4527 Val Loss: 1.2976646423339844, Val Acc: 0.6787\n","Epoch [11/500], Loss: 1.5590925216674805, Train Acc: 0.6301,train F1-score:0.5180 Val Loss: 1.3717904090881348, Val Acc: 0.6928\n","Epoch [12/500], Loss: 1.6698777675628662, Train Acc: 0.6365,train F1-score:0.5303 Val Loss: 1.2405389547348022, Val Acc: 0.6627\n","Epoch [13/500], Loss: 1.6129248142242432, Train Acc: 0.5733,train F1-score:0.5106 Val Loss: 1.09840989112854, Val Acc: 0.6975\n","Epoch [14/500], Loss: 1.3233311176300049, Train Acc: 0.5960,train F1-score:0.5370 Val Loss: 1.2239243984222412, Val Acc: 0.6807\n","Epoch [15/500], Loss: 1.3642302751541138, Train Acc: 0.5917,train F1-score:0.5207 Val Loss: 1.2057883739471436, Val Acc: 0.6807\n","Epoch [16/500], Loss: 1.343542218208313, Train Acc: 0.6239,train F1-score:0.5201 Val Loss: 1.1223212480545044, Val Acc: 0.6821\n","Epoch [17/500], Loss: 1.3114725351333618, Train Acc: 0.6418,train F1-score:0.5131 Val Loss: 1.0825862884521484, Val Acc: 0.6821\n","Epoch [18/500], Loss: 1.287672758102417, Train Acc: 0.6472,train F1-score:0.5162 Val Loss: 1.0720781087875366, Val Acc: 0.6854\n","Epoch [19/500], Loss: 1.2677487134933472, Train Acc: 0.6468,train F1-score:0.5215 Val Loss: 1.0804126262664795, Val Acc: 0.6867\n","Epoch [20/500], Loss: 1.2195589542388916, Train Acc: 0.6454,train F1-score:0.5234 Val Loss: 1.0746997594833374, Val Acc: 0.6881\n","Epoch [21/500], Loss: 1.2035164833068848, Train Acc: 0.6462,train F1-score:0.5242 Val Loss: 1.054477334022522, Val Acc: 0.6881\n","Epoch [22/500], Loss: 1.187565565109253, Train Acc: 0.6466,train F1-score:0.5268 Val Loss: 1.0333735942840576, Val Acc: 0.6914\n","Epoch [23/500], Loss: 1.165681004524231, Train Acc: 0.6504,train F1-score:0.5324 Val Loss: 1.014006495475769, Val Acc: 0.7015\n","Epoch [24/500], Loss: 1.1448709964752197, Train Acc: 0.6519,train F1-score:0.5380 Val Loss: 1.00391685962677, Val Acc: 0.7035\n","Epoch [25/500], Loss: 1.1376959085464478, Train Acc: 0.6558,train F1-score:0.5463 Val Loss: 1.006926417350769, Val Acc: 0.7035\n","Epoch [26/500], Loss: 1.121324896812439, Train Acc: 0.6554,train F1-score:0.5443 Val Loss: 1.0132745504379272, Val Acc: 0.7035\n","Epoch [27/500], Loss: 1.1082698106765747, Train Acc: 0.6611,train F1-score:0.5505 Val Loss: 1.016058087348938, Val Acc: 0.7062\n","Epoch [28/500], Loss: 1.0929875373840332, Train Acc: 0.6629,train F1-score:0.5483 Val Loss: 1.0060499906539917, Val Acc: 0.7068\n","Epoch [29/500], Loss: 1.0960708856582642, Train Acc: 0.6635,train F1-score:0.5501 Val Loss: 0.9930053353309631, Val Acc: 0.7068\n","Epoch [30/500], Loss: 1.0804407596588135, Train Acc: 0.6645,train F1-score:0.5510 Val Loss: 0.9791858792304993, Val Acc: 0.7082\n","Epoch [31/500], Loss: 1.0792425870895386, Train Acc: 0.6660,train F1-score:0.5537 Val Loss: 0.9710443615913391, Val Acc: 0.7088\n","Epoch [32/500], Loss: 1.062528133392334, Train Acc: 0.6657,train F1-score:0.5534 Val Loss: 0.96320641040802, Val Acc: 0.7088\n","Epoch [33/500], Loss: 1.0556299686431885, Train Acc: 0.6672,train F1-score:0.5579 Val Loss: 0.9526085257530212, Val Acc: 0.7088\n","Epoch [34/500], Loss: 1.052708387374878, Train Acc: 0.6656,train F1-score:0.5583 Val Loss: 0.9374114871025085, Val Acc: 0.7088\n","Epoch [35/500], Loss: 1.0470417737960815, Train Acc: 0.6668,train F1-score:0.5649 Val Loss: 0.9235250949859619, Val Acc: 0.7095\n","Epoch [36/500], Loss: 1.044044017791748, Train Acc: 0.6682,train F1-score:0.5692 Val Loss: 0.9145075678825378, Val Acc: 0.7095\n","Epoch [37/500], Loss: 1.0354403257369995, Train Acc: 0.6719,train F1-score:0.5710 Val Loss: 0.906319797039032, Val Acc: 0.7095\n","Epoch [38/500], Loss: 1.0264660120010376, Train Acc: 0.6725,train F1-score:0.5738 Val Loss: 0.8966390490531921, Val Acc: 0.7095\n","Epoch [39/500], Loss: 1.0218074321746826, Train Acc: 0.6733,train F1-score:0.5786 Val Loss: 0.8863192200660706, Val Acc: 0.7108\n","Epoch [40/500], Loss: 1.0173836946487427, Train Acc: 0.6769,train F1-score:0.5846 Val Loss: 0.8727855086326599, Val Acc: 0.7115\n","Epoch [41/500], Loss: 1.0068011283874512, Train Acc: 0.6750,train F1-score:0.5853 Val Loss: 0.859893262386322, Val Acc: 0.7115\n","Epoch [42/500], Loss: 1.0103014707565308, Train Acc: 0.6771,train F1-score:0.5863 Val Loss: 0.850062370300293, Val Acc: 0.7162\n","Epoch [43/500], Loss: 1.003258466720581, Train Acc: 0.6804,train F1-score:0.5933 Val Loss: 0.852401614189148, Val Acc: 0.7135\n","Epoch [44/500], Loss: 0.9989297986030579, Train Acc: 0.6795,train F1-score:0.5916 Val Loss: 0.8439408540725708, Val Acc: 0.7162\n","Epoch [45/500], Loss: 0.9847109317779541, Train Acc: 0.6821,train F1-score:0.5986 Val Loss: 0.8256111145019531, Val Acc: 0.7436\n","Epoch [46/500], Loss: 0.9774255752563477, Train Acc: 0.6859,train F1-score:0.6077 Val Loss: 0.8242694735527039, Val Acc: 0.7349\n","Epoch [47/500], Loss: 0.9673042297363281, Train Acc: 0.6895,train F1-score:0.6120 Val Loss: 0.8184125423431396, Val Acc: 0.7430\n","Epoch [48/500], Loss: 0.9731979966163635, Train Acc: 0.6927,train F1-score:0.6153 Val Loss: 0.8087385892868042, Val Acc: 0.7517\n","Epoch [49/500], Loss: 0.9593895077705383, Train Acc: 0.6954,train F1-score:0.6189 Val Loss: 0.8069777488708496, Val Acc: 0.7470\n","Epoch [50/500], Loss: 0.9611130952835083, Train Acc: 0.6934,train F1-score:0.6164 Val Loss: 0.8029553294181824, Val Acc: 0.7503\n","Epoch [51/500], Loss: 0.9438940286636353, Train Acc: 0.6959,train F1-score:0.6210 Val Loss: 0.7972664833068848, Val Acc: 0.7610\n","Epoch [52/500], Loss: 0.9425017237663269, Train Acc: 0.6937,train F1-score:0.6221 Val Loss: 0.7898024916648865, Val Acc: 0.7631\n","Epoch [53/500], Loss: 0.9464990496635437, Train Acc: 0.6938,train F1-score:0.6225 Val Loss: 0.7839233875274658, Val Acc: 0.7510\n","Epoch [54/500], Loss: 0.9303434491157532, Train Acc: 0.6980,train F1-score:0.6251 Val Loss: 0.7765287756919861, Val Acc: 0.7510\n","Epoch [55/500], Loss: 0.9256534576416016, Train Acc: 0.7024,train F1-score:0.6337 Val Loss: 0.7747177481651306, Val Acc: 0.7604\n","Epoch [56/500], Loss: 0.9393274188041687, Train Acc: 0.7015,train F1-score:0.6368 Val Loss: 0.7590900659561157, Val Acc: 0.7657\n","Epoch [57/500], Loss: 0.9219414591789246, Train Acc: 0.7078,train F1-score:0.6415 Val Loss: 0.7603144645690918, Val Acc: 0.7624\n","Epoch [58/500], Loss: 0.9131066799163818, Train Acc: 0.7052,train F1-score:0.6393 Val Loss: 0.7643051147460938, Val Acc: 0.7644\n","Epoch [59/500], Loss: 0.9038798213005066, Train Acc: 0.7061,train F1-score:0.6403 Val Loss: 0.7651569843292236, Val Acc: 0.7677\n","Epoch [60/500], Loss: 0.9130322933197021, Train Acc: 0.7062,train F1-score:0.6433 Val Loss: 0.7610341310501099, Val Acc: 0.7684\n","Epoch [61/500], Loss: 0.9021354913711548, Train Acc: 0.7128,train F1-score:0.6517 Val Loss: 0.75310218334198, Val Acc: 0.7751\n","Epoch [62/500], Loss: 0.9018104076385498, Train Acc: 0.7121,train F1-score:0.6505 Val Loss: 0.7443279027938843, Val Acc: 0.7751\n","Epoch [63/500], Loss: 0.8993757963180542, Train Acc: 0.7100,train F1-score:0.6501 Val Loss: 0.7384324073791504, Val Acc: 0.7758\n","Epoch [64/500], Loss: 0.8917810320854187, Train Acc: 0.7104,train F1-score:0.6521 Val Loss: 0.7334316372871399, Val Acc: 0.7724\n","Epoch [65/500], Loss: 0.8902726173400879, Train Acc: 0.7104,train F1-score:0.6497 Val Loss: 0.733976423740387, Val Acc: 0.7718\n","Epoch [66/500], Loss: 0.8767059445381165, Train Acc: 0.7140,train F1-score:0.6531 Val Loss: 0.7345770597457886, Val Acc: 0.7711\n","Epoch [67/500], Loss: 0.8806520700454712, Train Acc: 0.7093,train F1-score:0.6477 Val Loss: 0.7330366969108582, Val Acc: 0.7764\n","Epoch [68/500], Loss: 0.8683850169181824, Train Acc: 0.7158,train F1-score:0.6574 Val Loss: 0.7299579381942749, Val Acc: 0.7811\n","Epoch [69/500], Loss: 0.8696427941322327, Train Acc: 0.7171,train F1-score:0.6625 Val Loss: 0.7205556631088257, Val Acc: 0.7838\n","Epoch [70/500], Loss: 0.8541814684867859, Train Acc: 0.7201,train F1-score:0.6670 Val Loss: 0.7072478532791138, Val Acc: 0.7798\n","Epoch [71/500], Loss: 0.8576512336730957, Train Acc: 0.7214,train F1-score:0.6676 Val Loss: 0.6998541951179504, Val Acc: 0.7778\n","Epoch [72/500], Loss: 0.8582757115364075, Train Acc: 0.7203,train F1-score:0.6658 Val Loss: 0.6972973942756653, Val Acc: 0.7784\n","Epoch [73/500], Loss: 0.846342921257019, Train Acc: 0.7217,train F1-score:0.6691 Val Loss: 0.6972095966339111, Val Acc: 0.7771\n","Epoch [74/500], Loss: 0.8466451168060303, Train Acc: 0.7214,train F1-score:0.6680 Val Loss: 0.69953453540802, Val Acc: 0.7758\n","Epoch [75/500], Loss: 0.8497252464294434, Train Acc: 0.7243,train F1-score:0.6729 Val Loss: 0.698649525642395, Val Acc: 0.7744\n","Epoch [76/500], Loss: 0.8501983284950256, Train Acc: 0.7185,train F1-score:0.6641 Val Loss: 0.6928797364234924, Val Acc: 0.7744\n","Epoch [77/500], Loss: 0.8365911245346069, Train Acc: 0.7258,train F1-score:0.6718 Val Loss: 0.687844455242157, Val Acc: 0.7784\n","Epoch [78/500], Loss: 0.8426849842071533, Train Acc: 0.7236,train F1-score:0.6691 Val Loss: 0.686049222946167, Val Acc: 0.7764\n","Epoch [79/500], Loss: 0.8265844583511353, Train Acc: 0.7282,train F1-score:0.6753 Val Loss: 0.6701441407203674, Val Acc: 0.7932\n","Epoch [80/500], Loss: 0.8220020532608032, Train Acc: 0.7276,train F1-score:0.6808 Val Loss: 0.6695866584777832, Val Acc: 0.7878\n","Epoch [81/500], Loss: 0.8183957934379578, Train Acc: 0.7301,train F1-score:0.6819 Val Loss: 0.6701111793518066, Val Acc: 0.7905\n","Epoch [82/500], Loss: 0.8133382797241211, Train Acc: 0.7312,train F1-score:0.6833 Val Loss: 0.6677247881889343, Val Acc: 0.7938\n","Epoch [83/500], Loss: 0.8100125789642334, Train Acc: 0.7348,train F1-score:0.6878 Val Loss: 0.6612299680709839, Val Acc: 0.8005\n","Epoch [84/500], Loss: 0.8118628859519958, Train Acc: 0.7336,train F1-score:0.6840 Val Loss: 0.6553440093994141, Val Acc: 0.8019\n","Epoch [85/500], Loss: 0.8035867810249329, Train Acc: 0.7344,train F1-score:0.6855 Val Loss: 0.6470199823379517, Val Acc: 0.8039\n","Epoch [86/500], Loss: 0.7943309545516968, Train Acc: 0.7387,train F1-score:0.6918 Val Loss: 0.6396715044975281, Val Acc: 0.8079\n","Epoch [87/500], Loss: 0.79155033826828, Train Acc: 0.7403,train F1-score:0.6955 Val Loss: 0.6314558982849121, Val Acc: 0.8099\n","Epoch [88/500], Loss: 0.7954450845718384, Train Acc: 0.7383,train F1-score:0.6949 Val Loss: 0.6281384229660034, Val Acc: 0.8119\n","Epoch [89/500], Loss: 0.793001651763916, Train Acc: 0.7371,train F1-score:0.6955 Val Loss: 0.6308689117431641, Val Acc: 0.8092\n","Epoch [90/500], Loss: 0.7803957462310791, Train Acc: 0.7458,train F1-score:0.7046 Val Loss: 0.6368299722671509, Val Acc: 0.8032\n","Epoch [91/500], Loss: 0.7863407731056213, Train Acc: 0.7411,train F1-score:0.6963 Val Loss: 0.6394414305686951, Val Acc: 0.7959\n","Epoch [92/500], Loss: 0.7813733220100403, Train Acc: 0.7402,train F1-score:0.6961 Val Loss: 0.6356841325759888, Val Acc: 0.7959\n","Epoch [93/500], Loss: 0.7824069857597351, Train Acc: 0.7387,train F1-score:0.6920 Val Loss: 0.6317514777183533, Val Acc: 0.7999\n","Epoch [94/500], Loss: 0.7746049165725708, Train Acc: 0.7421,train F1-score:0.6945 Val Loss: 0.6263090372085571, Val Acc: 0.8039\n","Epoch [95/500], Loss: 0.7632550001144409, Train Acc: 0.7469,train F1-score:0.7049 Val Loss: 0.6185558438301086, Val Acc: 0.8119\n","Epoch [96/500], Loss: 0.7660672664642334, Train Acc: 0.7459,train F1-score:0.7062 Val Loss: 0.6124672293663025, Val Acc: 0.8193\n","Epoch [97/500], Loss: 0.7706410884857178, Train Acc: 0.7445,train F1-score:0.7052 Val Loss: 0.610116183757782, Val Acc: 0.8186\n","Epoch [98/500], Loss: 0.7680723667144775, Train Acc: 0.7485,train F1-score:0.7102 Val Loss: 0.6097774505615234, Val Acc: 0.8199\n","Epoch [99/500], Loss: 0.7542039155960083, Train Acc: 0.7494,train F1-score:0.7093 Val Loss: 0.6098809838294983, Val Acc: 0.8186\n","Epoch [100/500], Loss: 0.7537195682525635, Train Acc: 0.7500,train F1-score:0.7079 Val Loss: 0.6085386276245117, Val Acc: 0.8226\n","Epoch [101/500], Loss: 0.7513332962989807, Train Acc: 0.7535,train F1-score:0.7131 Val Loss: 0.6056174039840698, Val Acc: 0.8220\n","Epoch [102/500], Loss: 0.7515360713005066, Train Acc: 0.7527,train F1-score:0.7134 Val Loss: 0.6038575768470764, Val Acc: 0.8193\n","Epoch [103/500], Loss: 0.7499824166297913, Train Acc: 0.7557,train F1-score:0.7177 Val Loss: 0.6015706062316895, Val Acc: 0.8179\n","Epoch [104/500], Loss: 0.7369738221168518, Train Acc: 0.7562,train F1-score:0.7179 Val Loss: 0.5980960130691528, Val Acc: 0.8199\n","Epoch [105/500], Loss: 0.7508893609046936, Train Acc: 0.7495,train F1-score:0.7105 Val Loss: 0.5985775589942932, Val Acc: 0.8206\n","Epoch [106/500], Loss: 0.7343518733978271, Train Acc: 0.7545,train F1-score:0.7157 Val Loss: 0.5970100164413452, Val Acc: 0.8226\n","Epoch [107/500], Loss: 0.7414973974227905, Train Acc: 0.7541,train F1-score:0.7164 Val Loss: 0.5967278480529785, Val Acc: 0.8240\n","Epoch [108/500], Loss: 0.7345210313796997, Train Acc: 0.7571,train F1-score:0.7203 Val Loss: 0.5958124399185181, Val Acc: 0.8233\n","Epoch [109/500], Loss: 0.7320572137832642, Train Acc: 0.7560,train F1-score:0.7184 Val Loss: 0.5923431515693665, Val Acc: 0.8206\n","Epoch [110/500], Loss: 0.7350326180458069, Train Acc: 0.7609,train F1-score:0.7236 Val Loss: 0.5885968208312988, Val Acc: 0.8246\n","Epoch [111/500], Loss: 0.7267923951148987, Train Acc: 0.7605,train F1-score:0.7241 Val Loss: 0.5837181210517883, Val Acc: 0.8233\n","Epoch [112/500], Loss: 0.7338047027587891, Train Acc: 0.7533,train F1-score:0.7167 Val Loss: 0.5781561136245728, Val Acc: 0.8260\n","Epoch [113/500], Loss: 0.7213175892829895, Train Acc: 0.7608,train F1-score:0.7255 Val Loss: 0.5776268839836121, Val Acc: 0.8233\n","Epoch [114/500], Loss: 0.7143324017524719, Train Acc: 0.7634,train F1-score:0.7274 Val Loss: 0.5822678208351135, Val Acc: 0.8240\n","Epoch [115/500], Loss: 0.7200987935066223, Train Acc: 0.7622,train F1-score:0.7259 Val Loss: 0.5842005014419556, Val Acc: 0.8213\n","Epoch [116/500], Loss: 0.7117909789085388, Train Acc: 0.7675,train F1-score:0.7319 Val Loss: 0.5782238841056824, Val Acc: 0.8226\n","Epoch [117/500], Loss: 0.710196316242218, Train Acc: 0.7644,train F1-score:0.7298 Val Loss: 0.5728316903114319, Val Acc: 0.8266\n","Epoch [118/500], Loss: 0.7068345546722412, Train Acc: 0.7639,train F1-score:0.7284 Val Loss: 0.571370005607605, Val Acc: 0.8260\n","Epoch [119/500], Loss: 0.7007315754890442, Train Acc: 0.7661,train F1-score:0.7312 Val Loss: 0.576747715473175, Val Acc: 0.8213\n","Epoch [120/500], Loss: 0.6983859539031982, Train Acc: 0.7645,train F1-score:0.7301 Val Loss: 0.5800977349281311, Val Acc: 0.8213\n","Epoch [121/500], Loss: 0.6972494125366211, Train Acc: 0.7654,train F1-score:0.7301 Val Loss: 0.585221529006958, Val Acc: 0.8233\n","Epoch [122/500], Loss: 0.7047327756881714, Train Acc: 0.7629,train F1-score:0.7295 Val Loss: 0.5934174656867981, Val Acc: 0.8193\n","Epoch [123/500], Loss: 0.6946001648902893, Train Acc: 0.7668,train F1-score:0.7327 Val Loss: 0.591216504573822, Val Acc: 0.8226\n","Epoch [124/500], Loss: 0.6888960003852844, Train Acc: 0.7683,train F1-score:0.7334 Val Loss: 0.5868391394615173, Val Acc: 0.8220\n","Epoch [125/500], Loss: 0.6889914274215698, Train Acc: 0.7706,train F1-score:0.7361 Val Loss: 0.5771173238754272, Val Acc: 0.8273\n","Epoch [126/500], Loss: 0.6915476322174072, Train Acc: 0.7700,train F1-score:0.7346 Val Loss: 0.5645749568939209, Val Acc: 0.8286\n","Epoch [127/500], Loss: 0.6873735189437866, Train Acc: 0.7689,train F1-score:0.7360 Val Loss: 0.5524647235870361, Val Acc: 0.8333\n","Epoch [128/500], Loss: 0.6862301826477051, Train Acc: 0.7658,train F1-score:0.7325 Val Loss: 0.5513087511062622, Val Acc: 0.8320\n","Epoch [129/500], Loss: 0.6774419546127319, Train Acc: 0.7745,train F1-score:0.7432 Val Loss: 0.5556378960609436, Val Acc: 0.8327\n","Epoch [130/500], Loss: 0.6803110837936401, Train Acc: 0.7718,train F1-score:0.7389 Val Loss: 0.560712456703186, Val Acc: 0.8266\n","Epoch [131/500], Loss: 0.6773340702056885, Train Acc: 0.7731,train F1-score:0.7393 Val Loss: 0.564266562461853, Val Acc: 0.8260\n","Epoch [132/500], Loss: 0.6717067956924438, Train Acc: 0.7771,train F1-score:0.7430 Val Loss: 0.5615884065628052, Val Acc: 0.8253\n","Epoch [133/500], Loss: 0.6729263663291931, Train Acc: 0.7737,train F1-score:0.7388 Val Loss: 0.5612791180610657, Val Acc: 0.8260\n","Epoch [134/500], Loss: 0.6809195876121521, Train Acc: 0.7726,train F1-score:0.7379 Val Loss: 0.5637472867965698, Val Acc: 0.8307\n","Epoch [135/500], Loss: 0.6656767725944519, Train Acc: 0.7728,train F1-score:0.7405 Val Loss: 0.5628883242607117, Val Acc: 0.8320\n","Epoch [136/500], Loss: 0.6688323020935059, Train Acc: 0.7790,train F1-score:0.7482 Val Loss: 0.5590728521347046, Val Acc: 0.8307\n","Epoch [137/500], Loss: 0.6645758748054504, Train Acc: 0.7768,train F1-score:0.7451 Val Loss: 0.5546966791152954, Val Acc: 0.8320\n","Epoch [138/500], Loss: 0.6592492461204529, Train Acc: 0.7788,train F1-score:0.7482 Val Loss: 0.5520105957984924, Val Acc: 0.8347\n","Epoch [139/500], Loss: 0.6586198806762695, Train Acc: 0.7804,train F1-score:0.7500 Val Loss: 0.5532187223434448, Val Acc: 0.8340\n","Epoch [140/500], Loss: 0.6640951037406921, Train Acc: 0.7774,train F1-score:0.7469 Val Loss: 0.5554744601249695, Val Acc: 0.8340\n","Epoch [141/500], Loss: 0.6617253422737122, Train Acc: 0.7807,train F1-score:0.7503 Val Loss: 0.5561069846153259, Val Acc: 0.8307\n","Epoch [142/500], Loss: 0.6600497961044312, Train Acc: 0.7782,train F1-score:0.7466 Val Loss: 0.556943416595459, Val Acc: 0.8333\n","Epoch [143/500], Loss: 0.6533633470535278, Train Acc: 0.7781,train F1-score:0.7477 Val Loss: 0.5562500357627869, Val Acc: 0.8320\n","Epoch [144/500], Loss: 0.6511777639389038, Train Acc: 0.7763,train F1-score:0.7445 Val Loss: 0.5568886995315552, Val Acc: 0.8353\n","Epoch [145/500], Loss: 0.6513286828994751, Train Acc: 0.7819,train F1-score:0.7519 Val Loss: 0.554372251033783, Val Acc: 0.8340\n","Epoch [146/500], Loss: 0.6497508883476257, Train Acc: 0.7812,train F1-score:0.7502 Val Loss: 0.549773633480072, Val Acc: 0.8353\n","Epoch [147/500], Loss: 0.6481661796569824, Train Acc: 0.7814,train F1-score:0.7510 Val Loss: 0.5480937957763672, Val Acc: 0.8353\n","Epoch [148/500], Loss: 0.640258252620697, Train Acc: 0.7834,train F1-score:0.7548 Val Loss: 0.5475059151649475, Val Acc: 0.8360\n","Epoch [149/500], Loss: 0.6463354825973511, Train Acc: 0.7832,train F1-score:0.7558 Val Loss: 0.5475260615348816, Val Acc: 0.8387\n","Epoch [150/500], Loss: 0.639682412147522, Train Acc: 0.7857,train F1-score:0.7581 Val Loss: 0.5455620884895325, Val Acc: 0.8340\n","Epoch [151/500], Loss: 0.6510418057441711, Train Acc: 0.7828,train F1-score:0.7528 Val Loss: 0.5425892472267151, Val Acc: 0.8307\n","Epoch [152/500], Loss: 0.6579415798187256, Train Acc: 0.7833,train F1-score:0.7521 Val Loss: 0.5422099828720093, Val Acc: 0.8320\n","Epoch [153/500], Loss: 0.6355867981910706, Train Acc: 0.7849,train F1-score:0.7539 Val Loss: 0.5436224937438965, Val Acc: 0.8353\n","Epoch [154/500], Loss: 0.6392609477043152, Train Acc: 0.7831,train F1-score:0.7535 Val Loss: 0.5430458188056946, Val Acc: 0.8380\n","Epoch [155/500], Loss: 0.63606196641922, Train Acc: 0.7848,train F1-score:0.7569 Val Loss: 0.5366405248641968, Val Acc: 0.8414\n","Epoch [156/500], Loss: 0.6347693204879761, Train Acc: 0.7858,train F1-score:0.7585 Val Loss: 0.5342744588851929, Val Acc: 0.8407\n","Epoch [157/500], Loss: 0.6442113518714905, Train Acc: 0.7887,train F1-score:0.7613 Val Loss: 0.537211537361145, Val Acc: 0.8360\n","Epoch [158/500], Loss: 0.6343814134597778, Train Acc: 0.7884,train F1-score:0.7598 Val Loss: 0.5402358770370483, Val Acc: 0.8394\n","Epoch [159/500], Loss: 0.6225656867027283, Train Acc: 0.7907,train F1-score:0.7621 Val Loss: 0.5423429012298584, Val Acc: 0.8400\n","Epoch [160/500], Loss: 0.6273811459541321, Train Acc: 0.7909,train F1-score:0.7640 Val Loss: 0.5401281118392944, Val Acc: 0.8387\n","Epoch [161/500], Loss: 0.6267377734184265, Train Acc: 0.7922,train F1-score:0.7649 Val Loss: 0.5365734100341797, Val Acc: 0.8373\n","Epoch [162/500], Loss: 0.6271964311599731, Train Acc: 0.7896,train F1-score:0.7621 Val Loss: 0.5347673892974854, Val Acc: 0.8380\n","Epoch [163/500], Loss: 0.623755693435669, Train Acc: 0.7908,train F1-score:0.7624 Val Loss: 0.5349745750427246, Val Acc: 0.8360\n","Epoch [164/500], Loss: 0.6232766509056091, Train Acc: 0.7897,train F1-score:0.7614 Val Loss: 0.5356214046478271, Val Acc: 0.8347\n","Epoch [165/500], Loss: 0.6194800138473511, Train Acc: 0.7903,train F1-score:0.7617 Val Loss: 0.5342089533805847, Val Acc: 0.8360\n","Epoch [166/500], Loss: 0.6150734424591064, Train Acc: 0.7923,train F1-score:0.7637 Val Loss: 0.5322961807250977, Val Acc: 0.8360\n","Epoch [167/500], Loss: 0.6143552660942078, Train Acc: 0.7907,train F1-score:0.7641 Val Loss: 0.5259394645690918, Val Acc: 0.8380\n","Epoch [168/500], Loss: 0.6193149089813232, Train Acc: 0.7887,train F1-score:0.7611 Val Loss: 0.5232295393943787, Val Acc: 0.8387\n","Epoch [169/500], Loss: 0.6241839528083801, Train Acc: 0.7898,train F1-score:0.7632 Val Loss: 0.5236810445785522, Val Acc: 0.8407\n","Epoch [170/500], Loss: 0.6156439185142517, Train Acc: 0.7937,train F1-score:0.7670 Val Loss: 0.5266022682189941, Val Acc: 0.8407\n","Epoch [171/500], Loss: 0.6119567155838013, Train Acc: 0.7927,train F1-score:0.7664 Val Loss: 0.5291367173194885, Val Acc: 0.8394\n","Epoch [172/500], Loss: 0.6074042916297913, Train Acc: 0.7967,train F1-score:0.7718 Val Loss: 0.5306219458580017, Val Acc: 0.8394\n","Epoch [173/500], Loss: 0.6093811392784119, Train Acc: 0.7916,train F1-score:0.7646 Val Loss: 0.530861496925354, Val Acc: 0.8380\n","Epoch [174/500], Loss: 0.6102103590965271, Train Acc: 0.7932,train F1-score:0.7646 Val Loss: 0.5275813341140747, Val Acc: 0.8387\n","Epoch [175/500], Loss: 0.6027966141700745, Train Acc: 0.7927,train F1-score:0.7640 Val Loss: 0.5224737524986267, Val Acc: 0.8427\n","Epoch [176/500], Loss: 0.6149394512176514, Train Acc: 0.7936,train F1-score:0.7674 Val Loss: 0.5184030532836914, Val Acc: 0.8434\n","Epoch [177/500], Loss: 0.6071116924285889, Train Acc: 0.7923,train F1-score:0.7667 Val Loss: 0.5148970484733582, Val Acc: 0.8440\n","Epoch [178/500], Loss: 0.6038000583648682, Train Acc: 0.7953,train F1-score:0.7699 Val Loss: 0.5139134526252747, Val Acc: 0.8434\n","Epoch [179/500], Loss: 0.6119567155838013, Train Acc: 0.7933,train F1-score:0.7669 Val Loss: 0.514352560043335, Val Acc: 0.8407\n","Epoch [180/500], Loss: 0.6029459238052368, Train Acc: 0.7997,train F1-score:0.7758 Val Loss: 0.5181849002838135, Val Acc: 0.8360\n","Epoch [181/500], Loss: 0.6000674366950989, Train Acc: 0.7999,train F1-score:0.7744 Val Loss: 0.5204512476921082, Val Acc: 0.8320\n","Epoch [182/500], Loss: 0.6026996970176697, Train Acc: 0.7969,train F1-score:0.7709 Val Loss: 0.524228036403656, Val Acc: 0.8327\n","Epoch [183/500], Loss: 0.6017146706581116, Train Acc: 0.7974,train F1-score:0.7718 Val Loss: 0.5275043249130249, Val Acc: 0.8420\n","Epoch [184/500], Loss: 0.604382336139679, Train Acc: 0.7994,train F1-score:0.7742 Val Loss: 0.5275890827178955, Val Acc: 0.8394\n","Epoch [185/500], Loss: 0.5916257500648499, Train Acc: 0.7989,train F1-score:0.7742 Val Loss: 0.524894118309021, Val Acc: 0.8373\n","Epoch [186/500], Loss: 0.586341917514801, Train Acc: 0.7990,train F1-score:0.7735 Val Loss: 0.5164121389389038, Val Acc: 0.8387\n","Epoch [187/500], Loss: 0.5941446423530579, Train Acc: 0.7943,train F1-score:0.7685 Val Loss: 0.5130473375320435, Val Acc: 0.8427\n","Epoch [188/500], Loss: 0.5910220146179199, Train Acc: 0.7999,train F1-score:0.7746 Val Loss: 0.5085718631744385, Val Acc: 0.8427\n","Epoch [189/500], Loss: 0.5902531147003174, Train Acc: 0.7994,train F1-score:0.7734 Val Loss: 0.5081112384796143, Val Acc: 0.8434\n","Epoch [190/500], Loss: 0.5895775556564331, Train Acc: 0.8005,train F1-score:0.7763 Val Loss: 0.5106130838394165, Val Acc: 0.8387\n","Epoch [191/500], Loss: 0.5907200574874878, Train Acc: 0.8009,train F1-score:0.7775 Val Loss: 0.5131269097328186, Val Acc: 0.8367\n","Epoch [192/500], Loss: 0.5906134247779846, Train Acc: 0.7979,train F1-score:0.7743 Val Loss: 0.5123865008354187, Val Acc: 0.8373\n","Epoch [193/500], Loss: 0.5892024040222168, Train Acc: 0.8023,train F1-score:0.7761 Val Loss: 0.5091484189033508, Val Acc: 0.8387\n","Epoch [194/500], Loss: 0.5796389579772949, Train Acc: 0.8015,train F1-score:0.7759 Val Loss: 0.5044174194335938, Val Acc: 0.8387\n","Epoch [195/500], Loss: 0.5835254788398743, Train Acc: 0.8025,train F1-score:0.7769 Val Loss: 0.5005332827568054, Val Acc: 0.8394\n","Epoch [196/500], Loss: 0.5912303924560547, Train Acc: 0.8034,train F1-score:0.7792 Val Loss: 0.4981960356235504, Val Acc: 0.8380\n","Epoch [197/500], Loss: 0.5887998938560486, Train Acc: 0.8010,train F1-score:0.7754 Val Loss: 0.5019583702087402, Val Acc: 0.8440\n","Epoch [198/500], Loss: 0.5807703137397766, Train Acc: 0.8056,train F1-score:0.7825 Val Loss: 0.5057525634765625, Val Acc: 0.8414\n","Epoch [199/500], Loss: 0.5764105319976807, Train Acc: 0.8060,train F1-score:0.7814 Val Loss: 0.5098201036453247, Val Acc: 0.8380\n","Epoch [200/500], Loss: 0.5863211750984192, Train Acc: 0.8019,train F1-score:0.7756 Val Loss: 0.5135473608970642, Val Acc: 0.8347\n","Epoch [201/500], Loss: 0.5859420299530029, Train Acc: 0.8025,train F1-score:0.7756 Val Loss: 0.5129243731498718, Val Acc: 0.8367\n","Epoch [202/500], Loss: 0.579797089099884, Train Acc: 0.8021,train F1-score:0.7750 Val Loss: 0.5091166496276855, Val Acc: 0.8387\n","Epoch [203/500], Loss: 0.5789198279380798, Train Acc: 0.8026,train F1-score:0.7771 Val Loss: 0.5041517615318298, Val Acc: 0.8400\n","Epoch [204/500], Loss: 0.5713761448860168, Train Acc: 0.8065,train F1-score:0.7831 Val Loss: 0.5029709339141846, Val Acc: 0.8394\n","Epoch [205/500], Loss: 0.5907756090164185, Train Acc: 0.8030,train F1-score:0.7790 Val Loss: 0.503646194934845, Val Acc: 0.8400\n","Epoch [206/500], Loss: 0.5663067102432251, Train Acc: 0.8074,train F1-score:0.7848 Val Loss: 0.5006400942802429, Val Acc: 0.8394\n","Epoch [207/500], Loss: 0.5731664299964905, Train Acc: 0.8036,train F1-score:0.7798 Val Loss: 0.49946460127830505, Val Acc: 0.8400\n","Epoch [208/500], Loss: 0.5634808540344238, Train Acc: 0.8071,train F1-score:0.7838 Val Loss: 0.5018571615219116, Val Acc: 0.8414\n","Epoch [209/500], Loss: 0.5770951509475708, Train Acc: 0.8050,train F1-score:0.7813 Val Loss: 0.5025148391723633, Val Acc: 0.8440\n","Epoch [210/500], Loss: 0.5692340731620789, Train Acc: 0.8068,train F1-score:0.7828 Val Loss: 0.5031868815422058, Val Acc: 0.8447\n","Epoch [211/500], Loss: 0.5681390166282654, Train Acc: 0.8047,train F1-score:0.7836 Val Loss: 0.5022510886192322, Val Acc: 0.8440\n","Epoch [212/500], Loss: 0.5696074962615967, Train Acc: 0.8077,train F1-score:0.7871 Val Loss: 0.5016534924507141, Val Acc: 0.8407\n","Epoch [213/500], Loss: 0.5738539695739746, Train Acc: 0.8047,train F1-score:0.7826 Val Loss: 0.49736931920051575, Val Acc: 0.8420\n","Epoch [214/500], Loss: 0.5752373337745667, Train Acc: 0.8061,train F1-score:0.7821 Val Loss: 0.49109381437301636, Val Acc: 0.8420\n","Epoch [215/500], Loss: 0.5673865079879761, Train Acc: 0.8057,train F1-score:0.7802 Val Loss: 0.4862499237060547, Val Acc: 0.8440\n","Epoch [216/500], Loss: 0.5632498860359192, Train Acc: 0.8055,train F1-score:0.7802 Val Loss: 0.4823434352874756, Val Acc: 0.8507\n","Epoch [217/500], Loss: 0.5691143274307251, Train Acc: 0.8071,train F1-score:0.7836 Val Loss: 0.4824244976043701, Val Acc: 0.8514\n","Epoch [218/500], Loss: 0.5615770220756531, Train Acc: 0.8100,train F1-score:0.7874 Val Loss: 0.48322173953056335, Val Acc: 0.8507\n","Epoch [219/500], Loss: 0.5621182918548584, Train Acc: 0.8074,train F1-score:0.7861 Val Loss: 0.4845556616783142, Val Acc: 0.8461\n","Epoch [220/500], Loss: 0.5620669722557068, Train Acc: 0.8096,train F1-score:0.7887 Val Loss: 0.4854527711868286, Val Acc: 0.8400\n","Epoch [221/500], Loss: 0.5649400353431702, Train Acc: 0.8087,train F1-score:0.7859 Val Loss: 0.48471733927726746, Val Acc: 0.8407\n","Epoch [222/500], Loss: 0.5532475709915161, Train Acc: 0.8122,train F1-score:0.7897 Val Loss: 0.48380112648010254, Val Acc: 0.8434\n","Epoch [223/500], Loss: 0.5708988904953003, Train Acc: 0.8070,train F1-score:0.7826 Val Loss: 0.48361116647720337, Val Acc: 0.8434\n","Epoch [224/500], Loss: 0.5675289034843445, Train Acc: 0.8090,train F1-score:0.7871 Val Loss: 0.4863450527191162, Val Acc: 0.8467\n","Epoch [225/500], Loss: 0.5505842566490173, Train Acc: 0.8171,train F1-score:0.7968 Val Loss: 0.48625698685646057, Val Acc: 0.8467\n","Epoch [226/500], Loss: 0.5588929057121277, Train Acc: 0.8102,train F1-score:0.7867 Val Loss: 0.48692789673805237, Val Acc: 0.8440\n","Epoch [227/500], Loss: 0.5469693541526794, Train Acc: 0.8107,train F1-score:0.7872 Val Loss: 0.48544368147850037, Val Acc: 0.8467\n","Epoch [228/500], Loss: 0.5583929419517517, Train Acc: 0.8133,train F1-score:0.7921 Val Loss: 0.47712546586990356, Val Acc: 0.8481\n","Epoch [229/500], Loss: 0.5716832876205444, Train Acc: 0.8096,train F1-score:0.7872 Val Loss: 0.47366806864738464, Val Acc: 0.8461\n","Epoch [230/500], Loss: 0.5693536996841431, Train Acc: 0.8123,train F1-score:0.7919 Val Loss: 0.4830145239830017, Val Acc: 0.8427\n","Epoch [231/500], Loss: 0.5596806406974792, Train Acc: 0.8163,train F1-score:0.7962 Val Loss: 0.4866448938846588, Val Acc: 0.8440\n","Epoch [232/500], Loss: 0.605414628982544, Train Acc: 0.8110,train F1-score:0.7913 Val Loss: 0.49022039771080017, Val Acc: 0.8440\n","Epoch [233/500], Loss: 0.5592294931411743, Train Acc: 0.8119,train F1-score:0.7896 Val Loss: 0.49575984477996826, Val Acc: 0.8414\n","Epoch [234/500], Loss: 0.551468551158905, Train Acc: 0.8122,train F1-score:0.7899 Val Loss: 0.4927997291088104, Val Acc: 0.8414\n","Epoch [235/500], Loss: 0.550684928894043, Train Acc: 0.8096,train F1-score:0.7872 Val Loss: 0.49316293001174927, Val Acc: 0.8434\n","Epoch [236/500], Loss: 0.5961293578147888, Train Acc: 0.8094,train F1-score:0.7883 Val Loss: 0.49466270208358765, Val Acc: 0.8447\n","Epoch [237/500], Loss: 0.5645503401756287, Train Acc: 0.8082,train F1-score:0.7878 Val Loss: 0.49662458896636963, Val Acc: 0.8461\n","Epoch [238/500], Loss: 0.5591589212417603, Train Acc: 0.8115,train F1-score:0.7910 Val Loss: 0.4961329400539398, Val Acc: 0.8487\n","Epoch [239/500], Loss: 0.5603125691413879, Train Acc: 0.8148,train F1-score:0.7947 Val Loss: 0.49563127756118774, Val Acc: 0.8501\n","Epoch [240/500], Loss: 0.572211742401123, Train Acc: 0.8092,train F1-score:0.7880 Val Loss: 0.49349626898765564, Val Acc: 0.8507\n","Epoch [241/500], Loss: 0.567745566368103, Train Acc: 0.8098,train F1-score:0.7882 Val Loss: 0.4941008687019348, Val Acc: 0.8467\n","Epoch [242/500], Loss: 0.5655314326286316, Train Acc: 0.8053,train F1-score:0.7824 Val Loss: 0.4985235631465912, Val Acc: 0.8501\n","Epoch [243/500], Loss: 0.5725952386856079, Train Acc: 0.8097,train F1-score:0.7867 Val Loss: 0.5008724927902222, Val Acc: 0.8474\n","Epoch [244/500], Loss: 0.5678790807723999, Train Acc: 0.8041,train F1-score:0.7806 Val Loss: 0.49839818477630615, Val Acc: 0.8481\n","Epoch [245/500], Loss: 0.5656191110610962, Train Acc: 0.8076,train F1-score:0.7849 Val Loss: 0.49650701880455017, Val Acc: 0.8474\n","Epoch [246/500], Loss: 0.5581038594245911, Train Acc: 0.8126,train F1-score:0.7914 Val Loss: 0.4898070991039276, Val Acc: 0.8467\n","Epoch [247/500], Loss: 0.5575486421585083, Train Acc: 0.8139,train F1-score:0.7929 Val Loss: 0.4842933714389801, Val Acc: 0.8447\n","Epoch [248/500], Loss: 0.5696335434913635, Train Acc: 0.8146,train F1-score:0.7930 Val Loss: 0.48281654715538025, Val Acc: 0.8461\n","Epoch [249/500], Loss: 0.5549582242965698, Train Acc: 0.8158,train F1-score:0.7949 Val Loss: 0.48431551456451416, Val Acc: 0.8507\n","Epoch [250/500], Loss: 0.5639337301254272, Train Acc: 0.8135,train F1-score:0.7928 Val Loss: 0.48652172088623047, Val Acc: 0.8514\n","Epoch [251/500], Loss: 0.5658110976219177, Train Acc: 0.8107,train F1-score:0.7893 Val Loss: 0.4889791011810303, Val Acc: 0.8507\n","Epoch [252/500], Loss: 0.5661807656288147, Train Acc: 0.8121,train F1-score:0.7904 Val Loss: 0.4872788190841675, Val Acc: 0.8501\n","Epoch [253/500], Loss: 0.6000545620918274, Train Acc: 0.8128,train F1-score:0.7915 Val Loss: 0.48555073142051697, Val Acc: 0.8467\n","Epoch [254/500], Loss: 0.5545564293861389, Train Acc: 0.8165,train F1-score:0.7964 Val Loss: 0.48322874307632446, Val Acc: 0.8454\n","Epoch [255/500], Loss: 0.55576491355896, Train Acc: 0.8164,train F1-score:0.7957 Val Loss: 0.4845602214336395, Val Acc: 0.8407\n","Epoch [256/500], Loss: 0.5513580441474915, Train Acc: 0.8166,train F1-score:0.7956 Val Loss: 0.48335209488868713, Val Acc: 0.8407\n","Epoch [257/500], Loss: 0.5484756231307983, Train Acc: 0.8168,train F1-score:0.7951 Val Loss: 0.48459774255752563, Val Acc: 0.8420\n","Epoch [258/500], Loss: 0.5595663189888, Train Acc: 0.8154,train F1-score:0.7936 Val Loss: 0.48658713698387146, Val Acc: 0.8434\n","Epoch [259/500], Loss: 0.5530702471733093, Train Acc: 0.8159,train F1-score:0.7961 Val Loss: 0.4943479895591736, Val Acc: 0.8467\n","Epoch [260/500], Loss: 0.545732855796814, Train Acc: 0.8174,train F1-score:0.7976 Val Loss: 0.49402672052383423, Val Acc: 0.8467\n","Epoch [261/500], Loss: 0.5486457943916321, Train Acc: 0.8175,train F1-score:0.7977 Val Loss: 0.49022039771080017, Val Acc: 0.8474\n","Epoch [262/500], Loss: 0.5536165833473206, Train Acc: 0.8174,train F1-score:0.7966 Val Loss: 0.48699039220809937, Val Acc: 0.8454\n","Epoch [263/500], Loss: 0.5464188456535339, Train Acc: 0.8175,train F1-score:0.7967 Val Loss: 0.48508960008621216, Val Acc: 0.8474\n","Epoch [264/500], Loss: 0.5448113083839417, Train Acc: 0.8165,train F1-score:0.7964 Val Loss: 0.48157012462615967, Val Acc: 0.8501\n","Epoch [265/500], Loss: 0.5406919717788696, Train Acc: 0.8157,train F1-score:0.7966 Val Loss: 0.47586122155189514, Val Acc: 0.8514\n","Epoch [266/500], Loss: 0.5453069806098938, Train Acc: 0.8200,train F1-score:0.8012 Val Loss: 0.476749986410141, Val Acc: 0.8501\n","Epoch [267/500], Loss: 0.5336924195289612, Train Acc: 0.8229,train F1-score:0.8040 Val Loss: 0.4793386161327362, Val Acc: 0.8501\n","Epoch [268/500], Loss: 0.5438703894615173, Train Acc: 0.8216,train F1-score:0.8033 Val Loss: 0.4780890643596649, Val Acc: 0.8514\n","Epoch [269/500], Loss: 0.5496123433113098, Train Acc: 0.8201,train F1-score:0.8014 Val Loss: 0.4722159206867218, Val Acc: 0.8514\n","Epoch [270/500], Loss: 0.5427674055099487, Train Acc: 0.8169,train F1-score:0.7974 Val Loss: 0.46899908781051636, Val Acc: 0.8507\n","Epoch [271/500], Loss: 0.536570131778717, Train Acc: 0.8206,train F1-score:0.8014 Val Loss: 0.4684177041053772, Val Acc: 0.8521\n","Epoch [272/500], Loss: 0.5332987904548645, Train Acc: 0.8195,train F1-score:0.7999 Val Loss: 0.46806463599205017, Val Acc: 0.8521\n","Epoch [273/500], Loss: 0.5863120555877686, Train Acc: 0.8221,train F1-score:0.8034 Val Loss: 0.46602973341941833, Val Acc: 0.8507\n","Epoch [274/500], Loss: 0.5287123322486877, Train Acc: 0.8265,train F1-score:0.8078 Val Loss: 0.4688795804977417, Val Acc: 0.8487\n","Epoch [275/500], Loss: 0.5374141931533813, Train Acc: 0.8240,train F1-score:0.8054 Val Loss: 0.47218677401542664, Val Acc: 0.8447\n","Epoch [276/500], Loss: 0.5262672305107117, Train Acc: 0.8256,train F1-score:0.8066 Val Loss: 0.4790034890174866, Val Acc: 0.8454\n","Epoch [277/500], Loss: 0.535614013671875, Train Acc: 0.8226,train F1-score:0.8030 Val Loss: 0.4754317104816437, Val Acc: 0.8474\n","Epoch [278/500], Loss: 0.5264482498168945, Train Acc: 0.8258,train F1-score:0.8068 Val Loss: 0.47522449493408203, Val Acc: 0.8494\n","Epoch [279/500], Loss: 0.5318520069122314, Train Acc: 0.8232,train F1-score:0.8065 Val Loss: 0.47319722175598145, Val Acc: 0.8461\n","Epoch [280/500], Loss: 0.5323173403739929, Train Acc: 0.8247,train F1-score:0.8073 Val Loss: 0.4742644131183624, Val Acc: 0.8467\n","Epoch [281/500], Loss: 0.6885423064231873, Train Acc: 0.8252,train F1-score:0.8070 Val Loss: 0.4728236198425293, Val Acc: 0.8454\n","Epoch [282/500], Loss: 0.5156121253967285, Train Acc: 0.8289,train F1-score:0.8103 Val Loss: 0.4724636375904083, Val Acc: 0.8427\n","Epoch [283/500], Loss: 0.5286460518836975, Train Acc: 0.8211,train F1-score:0.8022 Val Loss: 0.46904832124710083, Val Acc: 0.8507\n","Epoch [284/500], Loss: 0.5257435441017151, Train Acc: 0.8286,train F1-score:0.8114 Val Loss: 0.46661576628685, Val Acc: 0.8514\n","Epoch [285/500], Loss: 0.5291511416435242, Train Acc: 0.8230,train F1-score:0.8069 Val Loss: 0.4619956612586975, Val Acc: 0.8521\n","Epoch [286/500], Loss: 0.5312414169311523, Train Acc: 0.8239,train F1-score:0.8081 Val Loss: 0.4580892324447632, Val Acc: 0.8541\n","Epoch [287/500], Loss: 0.524192750453949, Train Acc: 0.8262,train F1-score:0.8089 Val Loss: 0.458553284406662, Val Acc: 0.8548\n","Epoch [288/500], Loss: 0.549287736415863, Train Acc: 0.8222,train F1-score:0.8030 Val Loss: 0.4574567377567291, Val Acc: 0.8541\n","Epoch [289/500], Loss: 0.5233857035636902, Train Acc: 0.8267,train F1-score:0.8083 Val Loss: 0.45870622992515564, Val Acc: 0.8581\n","Epoch [290/500], Loss: 0.525689423084259, Train Acc: 0.8236,train F1-score:0.8063 Val Loss: 0.45469680428504944, Val Acc: 0.8581\n","Epoch [291/500], Loss: 0.5221569538116455, Train Acc: 0.8277,train F1-score:0.8117 Val Loss: 0.4491526186466217, Val Acc: 0.8554\n","Epoch [292/500], Loss: 0.5223610997200012, Train Acc: 0.8324,train F1-score:0.8167 Val Loss: 0.44918954372406006, Val Acc: 0.8561\n","Epoch [293/500], Loss: 0.5174013376235962, Train Acc: 0.8273,train F1-score:0.8107 Val Loss: 0.4534110128879547, Val Acc: 0.8514\n","Epoch [294/500], Loss: 0.5163503289222717, Train Acc: 0.8301,train F1-score:0.8126 Val Loss: 0.46158474683761597, Val Acc: 0.8521\n","Epoch [295/500], Loss: 0.511568546295166, Train Acc: 0.8274,train F1-score:0.8092 Val Loss: 0.4638761281967163, Val Acc: 0.8521\n","Epoch [296/500], Loss: 0.5174065828323364, Train Acc: 0.8288,train F1-score:0.8112 Val Loss: 0.4666602909564972, Val Acc: 0.8501\n","Epoch [297/500], Loss: 0.518909752368927, Train Acc: 0.8264,train F1-score:0.8093 Val Loss: 0.46470072865486145, Val Acc: 0.8487\n","Epoch [298/500], Loss: 0.5195690989494324, Train Acc: 0.8278,train F1-score:0.8102 Val Loss: 0.4643499553203583, Val Acc: 0.8507\n","Epoch [299/500], Loss: 0.5198163986206055, Train Acc: 0.8293,train F1-score:0.8114 Val Loss: 0.4647817313671112, Val Acc: 0.8440\n","Epoch [300/500], Loss: 0.5165277719497681, Train Acc: 0.8278,train F1-score:0.8090 Val Loss: 0.4612652361392975, Val Acc: 0.8447\n","Epoch [301/500], Loss: 0.5148817300796509, Train Acc: 0.8300,train F1-score:0.8122 Val Loss: 0.46190524101257324, Val Acc: 0.8514\n","Epoch [302/500], Loss: 0.5931798815727234, Train Acc: 0.8279,train F1-score:0.8112 Val Loss: 0.4610304534435272, Val Acc: 0.8534\n","Epoch [303/500], Loss: 0.5304110050201416, Train Acc: 0.8314,train F1-score:0.8151 Val Loss: 0.4635428190231323, Val Acc: 0.8534\n","Epoch [304/500], Loss: 0.5228516459465027, Train Acc: 0.8314,train F1-score:0.8140 Val Loss: 0.46189188957214355, Val Acc: 0.8594\n","Epoch [305/500], Loss: 0.5163064002990723, Train Acc: 0.8314,train F1-score:0.8139 Val Loss: 0.45821815729141235, Val Acc: 0.8588\n","Epoch [306/500], Loss: 0.5301690697669983, Train Acc: 0.8283,train F1-score:0.8108 Val Loss: 0.4551297724246979, Val Acc: 0.8581\n","Epoch [307/500], Loss: 0.5162451267242432, Train Acc: 0.8287,train F1-score:0.8112 Val Loss: 0.45339760184288025, Val Acc: 0.8601\n","Epoch [308/500], Loss: 0.5103069543838501, Train Acc: 0.8335,train F1-score:0.8176 Val Loss: 0.4526631832122803, Val Acc: 0.8594\n","Epoch [309/500], Loss: 0.5246070623397827, Train Acc: 0.8308,train F1-score:0.8163 Val Loss: 0.4522244334220886, Val Acc: 0.8554\n","Epoch [310/500], Loss: 0.5040870308876038, Train Acc: 0.8359,train F1-score:0.8190 Val Loss: 0.4529825448989868, Val Acc: 0.8554\n","Epoch [311/500], Loss: 0.5103610754013062, Train Acc: 0.8318,train F1-score:0.8145 Val Loss: 0.4540540874004364, Val Acc: 0.8568\n","Epoch [312/500], Loss: 0.501124918460846, Train Acc: 0.8355,train F1-score:0.8188 Val Loss: 0.4556174874305725, Val Acc: 0.8574\n","Epoch [313/500], Loss: 0.5103654265403748, Train Acc: 0.8297,train F1-score:0.8127 Val Loss: 0.4549800455570221, Val Acc: 0.8574\n","Epoch [314/500], Loss: 0.5161555409431458, Train Acc: 0.8334,train F1-score:0.8180 Val Loss: 0.45541632175445557, Val Acc: 0.8568\n","Epoch [315/500], Loss: 0.5064589977264404, Train Acc: 0.8330,train F1-score:0.8175 Val Loss: 0.4572322368621826, Val Acc: 0.8581\n","Epoch [316/500], Loss: 0.4961419999599457, Train Acc: 0.8355,train F1-score:0.8189 Val Loss: 0.46039479970932007, Val Acc: 0.8601\n","Epoch [317/500], Loss: 0.5085828900337219, Train Acc: 0.8319,train F1-score:0.8155 Val Loss: 0.460862934589386, Val Acc: 0.8568\n","Epoch [318/500], Loss: 0.509554922580719, Train Acc: 0.8335,train F1-score:0.8172 Val Loss: 0.45797839760780334, Val Acc: 0.8541\n","Epoch [319/500], Loss: 0.5020753741264343, Train Acc: 0.8336,train F1-score:0.8179 Val Loss: 0.45752227306365967, Val Acc: 0.8541\n","Epoch [320/500], Loss: 0.5058831572532654, Train Acc: 0.8305,train F1-score:0.8149 Val Loss: 0.45938101410865784, Val Acc: 0.8568\n","Epoch [321/500], Loss: 0.49984976649284363, Train Acc: 0.8340,train F1-score:0.8184 Val Loss: 0.4601341485977173, Val Acc: 0.8554\n","Epoch [322/500], Loss: 0.5079259872436523, Train Acc: 0.8329,train F1-score:0.8165 Val Loss: 0.45961993932724, Val Acc: 0.8554\n","Epoch [323/500], Loss: 0.5024005174636841, Train Acc: 0.8350,train F1-score:0.8178 Val Loss: 0.4568081796169281, Val Acc: 0.8554\n","Epoch [324/500], Loss: 0.500731885433197, Train Acc: 0.8381,train F1-score:0.8222 Val Loss: 0.4526735246181488, Val Acc: 0.8527\n","Epoch [325/500], Loss: 0.5023894906044006, Train Acc: 0.8365,train F1-score:0.8212 Val Loss: 0.4485289752483368, Val Acc: 0.8561\n","Epoch [326/500], Loss: 0.5034700632095337, Train Acc: 0.8358,train F1-score:0.8203 Val Loss: 0.4498937427997589, Val Acc: 0.8534\n","Epoch [327/500], Loss: 0.49332794547080994, Train Acc: 0.8364,train F1-score:0.8204 Val Loss: 0.44984397292137146, Val Acc: 0.8548\n","Epoch [328/500], Loss: 0.498695969581604, Train Acc: 0.8389,train F1-score:0.8234 Val Loss: 0.44642797112464905, Val Acc: 0.8601\n","Epoch [329/500], Loss: 0.5090517401695251, Train Acc: 0.8378,train F1-score:0.8230 Val Loss: 0.4490755498409271, Val Acc: 0.8621\n","Epoch [330/500], Loss: 0.5127159953117371, Train Acc: 0.8370,train F1-score:0.8223 Val Loss: 0.4543303847312927, Val Acc: 0.8581\n","Epoch [331/500], Loss: 0.49638691544532776, Train Acc: 0.8368,train F1-score:0.8204 Val Loss: 0.4596787691116333, Val Acc: 0.8514\n","Epoch [332/500], Loss: 0.512986421585083, Train Acc: 0.8319,train F1-score:0.8136 Val Loss: 0.4570489823818207, Val Acc: 0.8507\n","Epoch [333/500], Loss: 0.5009191632270813, Train Acc: 0.8396,train F1-score:0.8236 Val Loss: 0.45137569308280945, Val Acc: 0.8527\n","Epoch [334/500], Loss: 0.5014919638633728, Train Acc: 0.8377,train F1-score:0.8221 Val Loss: 0.44887346029281616, Val Acc: 0.8574\n","Epoch [335/500], Loss: 0.5506719350814819, Train Acc: 0.8348,train F1-score:0.8199 Val Loss: 0.44480398297309875, Val Acc: 0.8548\n","Epoch [336/500], Loss: 0.5002772808074951, Train Acc: 0.8367,train F1-score:0.8213 Val Loss: 0.43928101658821106, Val Acc: 0.8561\n","Epoch [337/500], Loss: 0.4959101974964142, Train Acc: 0.8401,train F1-score:0.8242 Val Loss: 0.4384285509586334, Val Acc: 0.8581\n","Epoch [338/500], Loss: 0.49484479427337646, Train Acc: 0.8381,train F1-score:0.8224 Val Loss: 0.441782683134079, Val Acc: 0.8608\n","Epoch [339/500], Loss: 0.494762122631073, Train Acc: 0.8386,train F1-score:0.8239 Val Loss: 0.4439167082309723, Val Acc: 0.8635\n","Epoch [340/500], Loss: 0.492816299200058, Train Acc: 0.8419,train F1-score:0.8272 Val Loss: 0.44290709495544434, Val Acc: 0.8601\n","Epoch [341/500], Loss: 0.5387498140335083, Train Acc: 0.8375,train F1-score:0.8224 Val Loss: 0.43999189138412476, Val Acc: 0.8541\n","Epoch [342/500], Loss: 0.502511739730835, Train Acc: 0.8370,train F1-score:0.8201 Val Loss: 0.4366888403892517, Val Acc: 0.8568\n","Epoch [343/500], Loss: 0.509357750415802, Train Acc: 0.8362,train F1-score:0.8213 Val Loss: 0.43842101097106934, Val Acc: 0.8621\n","Epoch [344/500], Loss: 0.4973614811897278, Train Acc: 0.8355,train F1-score:0.8206 Val Loss: 0.44139787554740906, Val Acc: 0.8648\n","Epoch [345/500], Loss: 0.5015250444412231, Train Acc: 0.8391,train F1-score:0.8239 Val Loss: 0.4397619068622589, Val Acc: 0.8635\n","Epoch [346/500], Loss: 0.488409161567688, Train Acc: 0.8411,train F1-score:0.8254 Val Loss: 0.4391787052154541, Val Acc: 0.8581\n","Epoch [347/500], Loss: 0.49314790964126587, Train Acc: 0.8401,train F1-score:0.8240 Val Loss: 0.4344913363456726, Val Acc: 0.8628\n","Epoch [348/500], Loss: 0.4872230887413025, Train Acc: 0.8412,train F1-score:0.8264 Val Loss: 0.43301504850387573, Val Acc: 0.8641\n","Epoch [349/500], Loss: 0.49104616045951843, Train Acc: 0.8391,train F1-score:0.8242 Val Loss: 0.4347323775291443, Val Acc: 0.8614\n","Epoch [350/500], Loss: 0.545095682144165, Train Acc: 0.8377,train F1-score:0.8234 Val Loss: 0.43897441029548645, Val Acc: 0.8601\n","Epoch [351/500], Loss: 0.6275375485420227, Train Acc: 0.8404,train F1-score:0.8247 Val Loss: 0.4432404935359955, Val Acc: 0.8527\n","Epoch [352/500], Loss: 0.496392160654068, Train Acc: 0.8369,train F1-score:0.8206 Val Loss: 0.43839776515960693, Val Acc: 0.8534\n","Epoch [353/500], Loss: 0.4936801493167877, Train Acc: 0.8385,train F1-score:0.8231 Val Loss: 0.4337095618247986, Val Acc: 0.8601\n","Epoch [354/500], Loss: 0.4894264042377472, Train Acc: 0.8407,train F1-score:0.8268 Val Loss: 0.43071243166923523, Val Acc: 0.8621\n","Epoch [355/500], Loss: 0.49262502789497375, Train Acc: 0.8355,train F1-score:0.8203 Val Loss: 0.4360171854496002, Val Acc: 0.8581\n","Epoch [356/500], Loss: 0.4895745813846588, Train Acc: 0.8387,train F1-score:0.8225 Val Loss: 0.4376898407936096, Val Acc: 0.8594\n","Epoch [357/500], Loss: 0.4827926456928253, Train Acc: 0.8427,train F1-score:0.8271 Val Loss: 0.43874067068099976, Val Acc: 0.8574\n","Epoch [358/500], Loss: 0.4899665117263794, Train Acc: 0.8409,train F1-score:0.8253 Val Loss: 0.43775418400764465, Val Acc: 0.8601\n","Epoch [359/500], Loss: 0.49484431743621826, Train Acc: 0.8396,train F1-score:0.8245 Val Loss: 0.4383537471294403, Val Acc: 0.8635\n","Epoch [360/500], Loss: 0.5090357661247253, Train Acc: 0.8431,train F1-score:0.8283 Val Loss: 0.4375210404396057, Val Acc: 0.8635\n","Epoch [361/500], Loss: 0.4937562048435211, Train Acc: 0.8396,train F1-score:0.8245 Val Loss: 0.43761250376701355, Val Acc: 0.8635\n","Epoch [362/500], Loss: 0.4922749698162079, Train Acc: 0.8388,train F1-score:0.8243 Val Loss: 0.43692997097969055, Val Acc: 0.8635\n","Epoch [363/500], Loss: 0.48328089714050293, Train Acc: 0.8428,train F1-score:0.8295 Val Loss: 0.4360675811767578, Val Acc: 0.8594\n","Epoch [364/500], Loss: 0.49145060777664185, Train Acc: 0.8401,train F1-score:0.8246 Val Loss: 0.43726545572280884, Val Acc: 0.8581\n","Epoch [365/500], Loss: 0.534622073173523, Train Acc: 0.8384,train F1-score:0.8224 Val Loss: 0.4385373592376709, Val Acc: 0.8561\n","Epoch [366/500], Loss: 0.5181735157966614, Train Acc: 0.8425,train F1-score:0.8265 Val Loss: 0.4389359652996063, Val Acc: 0.8594\n","Epoch [367/500], Loss: 0.48078885674476624, Train Acc: 0.8405,train F1-score:0.8260 Val Loss: 0.44212982058525085, Val Acc: 0.8628\n","Epoch [368/500], Loss: 0.49111002683639526, Train Acc: 0.8392,train F1-score:0.8249 Val Loss: 0.443431556224823, Val Acc: 0.8561\n","Epoch [369/500], Loss: 0.4857371747493744, Train Acc: 0.8410,train F1-score:0.8256 Val Loss: 0.445366233587265, Val Acc: 0.8554\n","Epoch [370/500], Loss: 0.4886402487754822, Train Acc: 0.8416,train F1-score:0.8259 Val Loss: 0.44325879216194153, Val Acc: 0.8568\n","Epoch [371/500], Loss: 0.48848962783813477, Train Acc: 0.8421,train F1-score:0.8268 Val Loss: 0.4356323778629303, Val Acc: 0.8581\n","Epoch [372/500], Loss: 0.5043909549713135, Train Acc: 0.8386,train F1-score:0.8234 Val Loss: 0.4312605559825897, Val Acc: 0.8608\n","Epoch [373/500], Loss: 0.4958147704601288, Train Acc: 0.8408,train F1-score:0.8264 Val Loss: 0.4320739209651947, Val Acc: 0.8561\n","Epoch [374/500], Loss: 0.4882935583591461, Train Acc: 0.8420,train F1-score:0.8270 Val Loss: 0.4304392337799072, Val Acc: 0.8561\n","Epoch [375/500], Loss: 0.4877792298793793, Train Acc: 0.8400,train F1-score:0.8245 Val Loss: 0.4299333691596985, Val Acc: 0.8588\n","Epoch [376/500], Loss: 0.4809987246990204, Train Acc: 0.8427,train F1-score:0.8276 Val Loss: 0.4289221167564392, Val Acc: 0.8621\n","Epoch [377/500], Loss: 0.4815242290496826, Train Acc: 0.8415,train F1-score:0.8270 Val Loss: 0.42994433641433716, Val Acc: 0.8661\n","Epoch [378/500], Loss: 0.6672123074531555, Train Acc: 0.8437,train F1-score:0.8300 Val Loss: 0.4355686902999878, Val Acc: 0.8628\n","Epoch [379/500], Loss: 0.4947368800640106, Train Acc: 0.8398,train F1-score:0.8248 Val Loss: 0.43492117524147034, Val Acc: 0.8574\n","Epoch [380/500], Loss: 0.5892416834831238, Train Acc: 0.8389,train F1-score:0.8228 Val Loss: 0.43296223878860474, Val Acc: 0.8574\n","Epoch [381/500], Loss: 0.4855085611343384, Train Acc: 0.8418,train F1-score:0.8263 Val Loss: 0.43336015939712524, Val Acc: 0.8641\n","Epoch [382/500], Loss: 0.4776885509490967, Train Acc: 0.8422,train F1-score:0.8278 Val Loss: 0.43438926339149475, Val Acc: 0.8668\n","Epoch [383/500], Loss: 0.4878028929233551, Train Acc: 0.8442,train F1-score:0.8316 Val Loss: 0.43636059761047363, Val Acc: 0.8608\n","Epoch [384/500], Loss: 0.4769412577152252, Train Acc: 0.8420,train F1-score:0.8279 Val Loss: 0.43689581751823425, Val Acc: 0.8574\n","Epoch [385/500], Loss: 0.4831429421901703, Train Acc: 0.8447,train F1-score:0.8290 Val Loss: 0.4341222643852234, Val Acc: 0.8548\n","Epoch [386/500], Loss: 0.48963719606399536, Train Acc: 0.8370,train F1-score:0.8212 Val Loss: 0.4322841465473175, Val Acc: 0.8581\n","Epoch [387/500], Loss: 0.48925477266311646, Train Acc: 0.8391,train F1-score:0.8241 Val Loss: 0.43351858854293823, Val Acc: 0.8608\n","Epoch [388/500], Loss: 0.5099970102310181, Train Acc: 0.8387,train F1-score:0.8241 Val Loss: 0.43653956055641174, Val Acc: 0.8621\n","Epoch [389/500], Loss: 0.4891470968723297, Train Acc: 0.8422,train F1-score:0.8275 Val Loss: 0.4363793134689331, Val Acc: 0.8588\n","Epoch [390/500], Loss: 0.5479880571365356, Train Acc: 0.8410,train F1-score:0.8269 Val Loss: 0.43704429268836975, Val Acc: 0.8568\n","Epoch [391/500], Loss: 0.49767497181892395, Train Acc: 0.8426,train F1-score:0.8277 Val Loss: 0.4367024302482605, Val Acc: 0.8581\n","Epoch [392/500], Loss: 0.4789429008960724, Train Acc: 0.8430,train F1-score:0.8282 Val Loss: 0.43263471126556396, Val Acc: 0.8635\n","Epoch [393/500], Loss: 0.4868197739124298, Train Acc: 0.8416,train F1-score:0.8274 Val Loss: 0.43307429552078247, Val Acc: 0.8568\n","Epoch [394/500], Loss: 0.4835241138935089, Train Acc: 0.8432,train F1-score:0.8287 Val Loss: 0.43429771065711975, Val Acc: 0.8561\n","Epoch [395/500], Loss: 0.4832889139652252, Train Acc: 0.8421,train F1-score:0.8273 Val Loss: 0.43550679087638855, Val Acc: 0.8628\n","Epoch [396/500], Loss: 0.48507073521614075, Train Acc: 0.8397,train F1-score:0.8255 Val Loss: 0.43741998076438904, Val Acc: 0.8614\n","Epoch [397/500], Loss: 0.4913818836212158, Train Acc: 0.8401,train F1-score:0.8258 Val Loss: 0.4382447600364685, Val Acc: 0.8588\n","Epoch [398/500], Loss: 0.47973379492759705, Train Acc: 0.8437,train F1-score:0.8291 Val Loss: 0.43931642174720764, Val Acc: 0.8628\n","Epoch [399/500], Loss: 0.4816838204860687, Train Acc: 0.8416,train F1-score:0.8273 Val Loss: 0.44048088788986206, Val Acc: 0.8608\n","Epoch [400/500], Loss: 0.4893787205219269, Train Acc: 0.8407,train F1-score:0.8255 Val Loss: 0.43954476714134216, Val Acc: 0.8608\n","Epoch [401/500], Loss: 0.48416855931282043, Train Acc: 0.8431,train F1-score:0.8280 Val Loss: 0.4365234076976776, Val Acc: 0.8641\n","Epoch [402/500], Loss: 0.4792798161506653, Train Acc: 0.8404,train F1-score:0.8261 Val Loss: 0.4321770668029785, Val Acc: 0.8621\n","Epoch [403/500], Loss: 0.4745645225048065, Train Acc: 0.8432,train F1-score:0.8292 Val Loss: 0.43273043632507324, Val Acc: 0.8614\n","Epoch [404/500], Loss: 0.47186192870140076, Train Acc: 0.8455,train F1-score:0.8307 Val Loss: 0.4328344166278839, Val Acc: 0.8588\n","Epoch [405/500], Loss: 0.490629106760025, Train Acc: 0.8460,train F1-score:0.8316 Val Loss: 0.4302750825881958, Val Acc: 0.8641\n","Epoch [406/500], Loss: 0.4779302477836609, Train Acc: 0.8454,train F1-score:0.8318 Val Loss: 0.4273446798324585, Val Acc: 0.8688\n","Epoch [407/500], Loss: 0.4834394156932831, Train Acc: 0.8472,train F1-score:0.8338 Val Loss: 0.4279125928878784, Val Acc: 0.8635\n","Epoch [408/500], Loss: 1.384047269821167, Train Acc: 0.8497,train F1-score:0.8359 Val Loss: 0.4266289472579956, Val Acc: 0.8648\n","Epoch [409/500], Loss: 0.6201165914535522, Train Acc: 0.8458,train F1-score:0.8314 Val Loss: 0.4267312288284302, Val Acc: 0.8648\n","Epoch [410/500], Loss: 0.7563644051551819, Train Acc: 0.8423,train F1-score:0.8280 Val Loss: 0.42829060554504395, Val Acc: 0.8628\n","Epoch [411/500], Loss: 0.4961397051811218, Train Acc: 0.8416,train F1-score:0.8264 Val Loss: 0.42977192997932434, Val Acc: 0.8661\n","Epoch [412/500], Loss: 0.48214191198349, Train Acc: 0.8425,train F1-score:0.8282 Val Loss: 0.4316690266132355, Val Acc: 0.8675\n","Epoch [413/500], Loss: 0.48123374581336975, Train Acc: 0.8463,train F1-score:0.8332 Val Loss: 0.4295527935028076, Val Acc: 0.8628\n","Epoch [414/500], Loss: 0.7842817306518555, Train Acc: 0.8416,train F1-score:0.8268 Val Loss: 0.428005188703537, Val Acc: 0.8635\n","Epoch [415/500], Loss: 0.4872523248195648, Train Acc: 0.8449,train F1-score:0.8303 Val Loss: 0.42797088623046875, Val Acc: 0.8661\n","Epoch [416/500], Loss: 0.48693370819091797, Train Acc: 0.8419,train F1-score:0.8266 Val Loss: 0.4275517463684082, Val Acc: 0.8628\n","Epoch [417/500], Loss: 0.5110112428665161, Train Acc: 0.8422,train F1-score:0.8275 Val Loss: 0.42652079463005066, Val Acc: 0.8621\n","Epoch [418/500], Loss: 0.4963509440422058, Train Acc: 0.8399,train F1-score:0.8253 Val Loss: 0.42443162202835083, Val Acc: 0.8641\n","Epoch [419/500], Loss: 0.5240969061851501, Train Acc: 0.8350,train F1-score:0.8207 Val Loss: 0.42382100224494934, Val Acc: 0.8608\n","Epoch [420/500], Loss: 0.6258730888366699, Train Acc: 0.8382,train F1-score:0.8241 Val Loss: 0.4256611764431, Val Acc: 0.8527\n","Epoch [421/500], Loss: 0.5554333329200745, Train Acc: 0.8386,train F1-score:0.8232 Val Loss: 0.42660585045814514, Val Acc: 0.8574\n","Epoch [422/500], Loss: 0.7143264412879944, Train Acc: 0.8350,train F1-score:0.8188 Val Loss: 0.42509910464286804, Val Acc: 0.8574\n","Epoch [423/500], Loss: 0.4944896101951599, Train Acc: 0.8378,train F1-score:0.8225 Val Loss: 0.4244416654109955, Val Acc: 0.8594\n","Epoch [424/500], Loss: 1.7766519784927368, Train Acc: 0.8366,train F1-score:0.8226 Val Loss: 0.42805442214012146, Val Acc: 0.8568\n","Epoch [425/500], Loss: 0.9005889296531677, Train Acc: 0.8358,train F1-score:0.8205 Val Loss: 0.4346665143966675, Val Acc: 0.8574\n","Epoch [426/500], Loss: 0.5163508653640747, Train Acc: 0.8355,train F1-score:0.8193 Val Loss: 0.43412575125694275, Val Acc: 0.8581\n","Epoch [427/500], Loss: 0.5107426643371582, Train Acc: 0.8376,train F1-score:0.8218 Val Loss: 0.43316128849983215, Val Acc: 0.8635\n","Epoch [428/500], Loss: 0.576389491558075, Train Acc: 0.8398,train F1-score:0.8252 Val Loss: 0.4321014881134033, Val Acc: 0.8675\n","Epoch [429/500], Loss: 0.5080310702323914, Train Acc: 0.8361,train F1-score:0.8229 Val Loss: 0.43647870421409607, Val Acc: 0.8661\n","Epoch [430/500], Loss: 0.5151510834693909, Train Acc: 0.8364,train F1-score:0.8218 Val Loss: 0.44224604964256287, Val Acc: 0.8648\n","Epoch [431/500], Loss: 0.5089477300643921, Train Acc: 0.8390,train F1-score:0.8252 Val Loss: 0.4452940821647644, Val Acc: 0.8601\n","Epoch [432/500], Loss: 0.5666522979736328, Train Acc: 0.8378,train F1-score:0.8225 Val Loss: 0.4478073716163635, Val Acc: 0.8554\n","Epoch [433/500], Loss: 0.92348313331604, Train Acc: 0.8344,train F1-score:0.8201 Val Loss: 0.4441455900669098, Val Acc: 0.8554\n","Epoch [434/500], Loss: 0.9025771617889404, Train Acc: 0.8364,train F1-score:0.8209 Val Loss: 0.4407182037830353, Val Acc: 0.8554\n","Epoch [435/500], Loss: 0.5958617925643921, Train Acc: 0.8350,train F1-score:0.8203 Val Loss: 0.4381042718887329, Val Acc: 0.8534\n","Epoch [436/500], Loss: 0.5073242783546448, Train Acc: 0.8371,train F1-score:0.8217 Val Loss: 0.4345059394836426, Val Acc: 0.8581\n","Epoch [437/500], Loss: 0.5646795034408569, Train Acc: 0.8350,train F1-score:0.8201 Val Loss: 0.4366970360279083, Val Acc: 0.8561\n","Epoch [438/500], Loss: 0.521974503993988, Train Acc: 0.8334,train F1-score:0.8165 Val Loss: 0.43756091594696045, Val Acc: 0.8574\n","Epoch [439/500], Loss: 2.3360204696655273, Train Acc: 0.8355,train F1-score:0.8198 Val Loss: 0.45363208651542664, Val Acc: 0.8514\n","Epoch [440/500], Loss: 0.5401214361190796, Train Acc: 0.8237,train F1-score:0.8044 Val Loss: 0.45935872197151184, Val Acc: 0.8481\n","Epoch [441/500], Loss: 0.5650092959403992, Train Acc: 0.8159,train F1-score:0.7932 Val Loss: 0.45333707332611084, Val Acc: 0.8554\n","Epoch [442/500], Loss: 1.216927409172058, Train Acc: 0.8265,train F1-score:0.8125 Val Loss: 0.46204376220703125, Val Acc: 0.8581\n","Epoch [443/500], Loss: 1.476486325263977, Train Acc: 0.8288,train F1-score:0.8167 Val Loss: 0.4608730375766754, Val Acc: 0.8581\n","Epoch [444/500], Loss: 0.5680239200592041, Train Acc: 0.8353,train F1-score:0.8203 Val Loss: 0.4727511405944824, Val Acc: 0.8494\n","Epoch [445/500], Loss: 0.649010419845581, Train Acc: 0.8330,train F1-score:0.8167 Val Loss: 0.45936399698257446, Val Acc: 0.8527\n","Epoch [446/500], Loss: 0.5400249361991882, Train Acc: 0.8323,train F1-score:0.8159 Val Loss: 0.4457850456237793, Val Acc: 0.8568\n","Epoch [447/500], Loss: 0.5375586152076721, Train Acc: 0.8342,train F1-score:0.8195 Val Loss: 0.44500356912612915, Val Acc: 0.8608\n","Epoch [448/500], Loss: 0.5437642335891724, Train Acc: 0.8278,train F1-score:0.8126 Val Loss: 0.4488934874534607, Val Acc: 0.8601\n","Epoch [449/500], Loss: 0.5445900559425354, Train Acc: 0.8300,train F1-score:0.8144 Val Loss: 0.4580317437648773, Val Acc: 0.8548\n","Epoch [450/500], Loss: 0.5481630563735962, Train Acc: 0.8308,train F1-score:0.8139 Val Loss: 0.4573909640312195, Val Acc: 0.8541\n","Epoch [451/500], Loss: 0.5418136715888977, Train Acc: 0.8362,train F1-score:0.8206 Val Loss: 0.45790502429008484, Val Acc: 0.8561\n","Epoch [452/500], Loss: 0.5355896353721619, Train Acc: 0.8300,train F1-score:0.8150 Val Loss: 0.46054166555404663, Val Acc: 0.8568\n","Epoch [453/500], Loss: 0.5486468076705933, Train Acc: 0.8283,train F1-score:0.8130 Val Loss: 0.4599417448043823, Val Acc: 0.8594\n","Epoch [454/500], Loss: 0.5352964401245117, Train Acc: 0.8320,train F1-score:0.8165 Val Loss: 0.4553139805793762, Val Acc: 0.8628\n","Epoch [455/500], Loss: 0.5362688899040222, Train Acc: 0.8320,train F1-score:0.8156 Val Loss: 0.4521283805370331, Val Acc: 0.8608\n","Epoch [456/500], Loss: 0.5327848196029663, Train Acc: 0.8378,train F1-score:0.8223 Val Loss: 0.4538145959377289, Val Acc: 0.8581\n","Epoch [457/500], Loss: 0.5316022038459778, Train Acc: 0.8317,train F1-score:0.8156 Val Loss: 0.4523470997810364, Val Acc: 0.8568\n","Epoch [458/500], Loss: 0.5213348269462585, Train Acc: 0.8339,train F1-score:0.8192 Val Loss: 0.4552333950996399, Val Acc: 0.8534\n","Epoch [459/500], Loss: 0.536193311214447, Train Acc: 0.8314,train F1-score:0.8164 Val Loss: 0.4526990056037903, Val Acc: 0.8514\n","Epoch [460/500], Loss: 0.5203277468681335, Train Acc: 0.8403,train F1-score:0.8253 Val Loss: 0.4453881084918976, Val Acc: 0.8574\n","Epoch [461/500], Loss: 0.5317705273628235, Train Acc: 0.8390,train F1-score:0.8237 Val Loss: 0.4410310983657837, Val Acc: 0.8621\n","Epoch [462/500], Loss: 0.5796757936477661, Train Acc: 0.8358,train F1-score:0.8216 Val Loss: 0.4405584931373596, Val Acc: 0.8641\n","Epoch [463/500], Loss: 0.5272709727287292, Train Acc: 0.8385,train F1-score:0.8238 Val Loss: 0.44238659739494324, Val Acc: 0.8668\n","Epoch [464/500], Loss: 0.5130913853645325, Train Acc: 0.8392,train F1-score:0.8249 Val Loss: 0.4455048739910126, Val Acc: 0.8628\n","Epoch [465/500], Loss: 0.5621897578239441, Train Acc: 0.8375,train F1-score:0.8225 Val Loss: 0.4492729902267456, Val Acc: 0.8594\n","Epoch [466/500], Loss: 0.5074244141578674, Train Acc: 0.8381,train F1-score:0.8226 Val Loss: 0.4511433243751526, Val Acc: 0.8594\n","Epoch [467/500], Loss: 0.5256486535072327, Train Acc: 0.8366,train F1-score:0.8203 Val Loss: 0.4454652965068817, Val Acc: 0.8581\n","Epoch [468/500], Loss: 0.5211486220359802, Train Acc: 0.8356,train F1-score:0.8203 Val Loss: 0.44377583265304565, Val Acc: 0.8655\n","Epoch [469/500], Loss: 0.5254672765731812, Train Acc: 0.8333,train F1-score:0.8179 Val Loss: 0.44589465856552124, Val Acc: 0.8648\n","Epoch [470/500], Loss: 0.5179882049560547, Train Acc: 0.8360,train F1-score:0.8208 Val Loss: 0.44554826617240906, Val Acc: 0.8641\n","Epoch [471/500], Loss: 0.5319890379905701, Train Acc: 0.8357,train F1-score:0.8203 Val Loss: 0.4410291612148285, Val Acc: 0.8635\n","Epoch [472/500], Loss: 0.5090306401252747, Train Acc: 0.8369,train F1-score:0.8210 Val Loss: 0.4331256151199341, Val Acc: 0.8635\n","Epoch [473/500], Loss: 0.5160959959030151, Train Acc: 0.8407,train F1-score:0.8244 Val Loss: 0.42689892649650574, Val Acc: 0.8621\n","Epoch [474/500], Loss: 0.5048495531082153, Train Acc: 0.8401,train F1-score:0.8249 Val Loss: 0.42531347274780273, Val Acc: 0.8635\n","Epoch [475/500], Loss: 0.4981350004673004, Train Acc: 0.8448,train F1-score:0.8311 Val Loss: 0.42419448494911194, Val Acc: 0.8628\n","Epoch [476/500], Loss: 0.5090930461883545, Train Acc: 0.8383,train F1-score:0.8246 Val Loss: 0.4221433997154236, Val Acc: 0.8635\n","Epoch [477/500], Loss: 0.5008386969566345, Train Acc: 0.8437,train F1-score:0.8298 Val Loss: 0.42380011081695557, Val Acc: 0.8588\n","Epoch [478/500], Loss: 0.5168108344078064, Train Acc: 0.8393,train F1-score:0.8235 Val Loss: 0.4272509217262268, Val Acc: 0.8588\n","Epoch [479/500], Loss: 0.5076888799667358, Train Acc: 0.8398,train F1-score:0.8236 Val Loss: 0.4264952838420868, Val Acc: 0.8601\n","Epoch [480/500], Loss: 0.5030550360679626, Train Acc: 0.8372,train F1-score:0.8215 Val Loss: 0.4269067347049713, Val Acc: 0.8601\n","Epoch [481/500], Loss: 0.4958418011665344, Train Acc: 0.8377,train F1-score:0.8233 Val Loss: 0.43140676617622375, Val Acc: 0.8574\n","Epoch [482/500], Loss: 0.5034891366958618, Train Acc: 0.8411,train F1-score:0.8272 Val Loss: 0.4363163113594055, Val Acc: 0.8574\n","Epoch [483/500], Loss: 0.4966065287590027, Train Acc: 0.8400,train F1-score:0.8250 Val Loss: 0.4363899230957031, Val Acc: 0.8574\n","Epoch [484/500], Loss: 0.49545982480049133, Train Acc: 0.8455,train F1-score:0.8302 Val Loss: 0.4367389380931854, Val Acc: 0.8594\n","Epoch [485/500], Loss: 0.4966147840023041, Train Acc: 0.8429,train F1-score:0.8278 Val Loss: 0.4387362003326416, Val Acc: 0.8581\n","Epoch [486/500], Loss: 0.49632829427719116, Train Acc: 0.8489,train F1-score:0.8348 Val Loss: 0.439432829618454, Val Acc: 0.8581\n","Epoch [487/500], Loss: 0.48529186844825745, Train Acc: 0.8446,train F1-score:0.8307 Val Loss: 0.44059085845947266, Val Acc: 0.8568\n","Epoch [488/500], Loss: 0.4896385967731476, Train Acc: 0.8440,train F1-score:0.8294 Val Loss: 0.4402683675289154, Val Acc: 0.8548\n","Epoch [489/500], Loss: 0.4897061586380005, Train Acc: 0.8448,train F1-score:0.8301 Val Loss: 0.43773898482322693, Val Acc: 0.8561\n","Epoch [490/500], Loss: 0.4910779297351837, Train Acc: 0.8458,train F1-score:0.8314 Val Loss: 0.4369097948074341, Val Acc: 0.8554\n","Epoch [491/500], Loss: 0.487636536359787, Train Acc: 0.8432,train F1-score:0.8278 Val Loss: 0.4347919821739197, Val Acc: 0.8574\n","Epoch [492/500], Loss: 0.4868122339248657, Train Acc: 0.8469,train F1-score:0.8322 Val Loss: 0.43352171778678894, Val Acc: 0.8601\n","Epoch [493/500], Loss: 0.48173609375953674, Train Acc: 0.8489,train F1-score:0.8351 Val Loss: 0.43209555745124817, Val Acc: 0.8614\n","Epoch [494/500], Loss: 0.48834526538848877, Train Acc: 0.8480,train F1-score:0.8343 Val Loss: 0.4323447644710541, Val Acc: 0.8601\n","Epoch [495/500], Loss: 0.4917484521865845, Train Acc: 0.8458,train F1-score:0.8316 Val Loss: 0.43258318305015564, Val Acc: 0.8608\n","Epoch [496/500], Loss: 0.4858805239200592, Train Acc: 0.8443,train F1-score:0.8301 Val Loss: 0.43029651045799255, Val Acc: 0.8641\n","Epoch [497/500], Loss: 0.4783744513988495, Train Acc: 0.8489,train F1-score:0.8346 Val Loss: 0.42932048439979553, Val Acc: 0.8668\n","Epoch [498/500], Loss: 0.4896487295627594, Train Acc: 0.8465,train F1-score:0.8325 Val Loss: 0.4277048408985138, Val Acc: 0.8661\n","Epoch [499/500], Loss: 0.48356395959854126, Train Acc: 0.8489,train F1-score:0.8351 Val Loss: 0.42655521631240845, Val Acc: 0.8648\n","Epoch [500/500], Loss: 0.4950534403324127, Train Acc: 0.8479,train F1-score:0.8342 Val Loss: 0.42724454402923584, Val Acc: 0.8635\n","Test Loss: 0.4288090765476227, Test Accuracy: 0.8666219839142091\n","Precision: 0.8552, Recall: 0.8666, F1-score: 0.8528\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["F1-score saved to file.\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GATNet(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_dim, out_channels, num_layers):\n","        super(GATNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GATConv(in_channels, hidden_dim, heads=8))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GATConv(hidden_dim * 8, hidden_dim, heads=8))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim * 8, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.fc(x)\n","        return x\n","\n","# Define model\n","model = GATNet(in_channels=X_train.shape[1], hidden_dim=128, out_channels=13, num_layers=3).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model3.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer3/train_f1_scores_GAT.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer3/train_loss_GAT.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer3/epochs_GAT.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model3.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/layer3/f1_score_GAT.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70126,"status":"ok","timestamp":1714454592271,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"lLugatBRcyvS","outputId":"d7c06675-aa7e-4e67-d823-ef10a2b66435"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Epoch [1/500], Loss: 2.67704439163208, Train Acc: 0.0536,train F1-score:0.0761 Val Loss: 51.17746353149414, Val Acc: 0.6787\n","Epoch [2/500], Loss: 65.8505630493164, Train Acc: 0.6441,train F1-score:0.5047 Val Loss: 1.9251996278762817, Val Acc: 0.1104\n","Epoch [3/500], Loss: 2.332005023956299, Train Acc: 0.1221,train F1-score:0.0395 Val Loss: 2.4797110557556152, Val Acc: 0.1104\n","Epoch [4/500], Loss: 2.3966269493103027, Train Acc: 0.1183,train F1-score:0.0399 Val Loss: 2.4414517879486084, Val Acc: 0.0529\n","Epoch [5/500], Loss: 2.404960870742798, Train Acc: 0.1093,train F1-score:0.0397 Val Loss: 2.3092100620269775, Val Acc: 0.1084\n","Epoch [6/500], Loss: 2.6688804626464844, Train Acc: 0.1089,train F1-score:0.0358 Val Loss: 2.3879055976867676, Val Acc: 0.1044\n","Epoch [7/500], Loss: 2.325636148452759, Train Acc: 0.1146,train F1-score:0.0435 Val Loss: 2.1743297576904297, Val Acc: 0.1191\n","Epoch [8/500], Loss: 2.153927803039551, Train Acc: 0.1431,train F1-score:0.0861 Val Loss: 1.6644775867462158, Val Acc: 0.6024\n","Epoch [9/500], Loss: 1.886311650276184, Train Acc: 0.4277,train F1-score:0.4283 Val Loss: 1.2923450469970703, Val Acc: 0.6774\n","Epoch [10/500], Loss: 1.777582049369812, Train Acc: 0.6071,train F1-score:0.5005 Val Loss: 1.477229356765747, Val Acc: 0.6319\n","Epoch [11/500], Loss: 1.6494284868240356, Train Acc: 0.5156,train F1-score:0.4783 Val Loss: 1.411665678024292, Val Acc: 0.6720\n","Epoch [12/500], Loss: 1.5412214994430542, Train Acc: 0.5330,train F1-score:0.4861 Val Loss: 1.2022618055343628, Val Acc: 0.6767\n","Epoch [13/500], Loss: 1.4362670183181763, Train Acc: 0.5883,train F1-score:0.5012 Val Loss: 1.1594990491867065, Val Acc: 0.6767\n","Epoch [14/500], Loss: 1.4706926345825195, Train Acc: 0.5975,train F1-score:0.4976 Val Loss: 1.2115250825881958, Val Acc: 0.6754\n","Epoch [15/500], Loss: 1.3855516910552979, Train Acc: 0.5921,train F1-score:0.4973 Val Loss: 1.2916538715362549, Val Acc: 0.6754\n","Epoch [16/500], Loss: 1.313569188117981, Train Acc: 0.5993,train F1-score:0.5028 Val Loss: 1.3287949562072754, Val Acc: 0.6740\n","Epoch [17/500], Loss: 1.2789040803909302, Train Acc: 0.6136,train F1-score:0.5116 Val Loss: 1.270999550819397, Val Acc: 0.6787\n","Epoch [18/500], Loss: 1.2431538105010986, Train Acc: 0.6284,train F1-score:0.5187 Val Loss: 1.1973216533660889, Val Acc: 0.6787\n","Epoch [19/500], Loss: 1.2220523357391357, Train Acc: 0.6348,train F1-score:0.5138 Val Loss: 1.1769483089447021, Val Acc: 0.6787\n","Epoch [20/500], Loss: 1.212601661682129, Train Acc: 0.6388,train F1-score:0.5092 Val Loss: 1.2401916980743408, Val Acc: 0.6787\n","Epoch [21/500], Loss: 1.196863055229187, Train Acc: 0.6388,train F1-score:0.5067 Val Loss: 1.2757383584976196, Val Acc: 0.6787\n","Epoch [22/500], Loss: 1.1896318197250366, Train Acc: 0.6383,train F1-score:0.5062 Val Loss: 1.237996220588684, Val Acc: 0.6787\n","Epoch [23/500], Loss: 1.1748923063278198, Train Acc: 0.6395,train F1-score:0.5037 Val Loss: 1.1693929433822632, Val Acc: 0.6787\n","Epoch [24/500], Loss: 1.1701600551605225, Train Acc: 0.6425,train F1-score:0.5045 Val Loss: 1.1297165155410767, Val Acc: 0.6787\n","Epoch [25/500], Loss: 1.1643595695495605, Train Acc: 0.6434,train F1-score:0.5046 Val Loss: 1.122846007347107, Val Acc: 0.6787\n","Epoch [26/500], Loss: 1.1502745151519775, Train Acc: 0.6444,train F1-score:0.5060 Val Loss: 1.118454933166504, Val Acc: 0.6787\n","Epoch [27/500], Loss: 1.147730827331543, Train Acc: 0.6457,train F1-score:0.5102 Val Loss: 1.1197971105575562, Val Acc: 0.6787\n","Epoch [28/500], Loss: 1.1436961889266968, Train Acc: 0.6467,train F1-score:0.5165 Val Loss: 1.1039798259735107, Val Acc: 0.6787\n","Epoch [29/500], Loss: 1.1395976543426514, Train Acc: 0.6470,train F1-score:0.5189 Val Loss: 1.0812032222747803, Val Acc: 0.6794\n","Epoch [30/500], Loss: 1.1338411569595337, Train Acc: 0.6484,train F1-score:0.5202 Val Loss: 1.0582830905914307, Val Acc: 0.6807\n","Epoch [31/500], Loss: 1.1145355701446533, Train Acc: 0.6520,train F1-score:0.5253 Val Loss: 1.0382369756698608, Val Acc: 0.6821\n","Epoch [32/500], Loss: 1.1092082262039185, Train Acc: 0.6519,train F1-score:0.5273 Val Loss: 1.0231167078018188, Val Acc: 0.6881\n","Epoch [33/500], Loss: 1.0974053144454956, Train Acc: 0.6535,train F1-score:0.5311 Val Loss: 1.0095164775848389, Val Acc: 0.6914\n","Epoch [34/500], Loss: 1.0901297330856323, Train Acc: 0.6533,train F1-score:0.5320 Val Loss: 0.9973815083503723, Val Acc: 0.6928\n","Epoch [35/500], Loss: 1.0973154306411743, Train Acc: 0.6549,train F1-score:0.5352 Val Loss: 0.9858724474906921, Val Acc: 0.6941\n","Epoch [36/500], Loss: 1.084194540977478, Train Acc: 0.6562,train F1-score:0.5373 Val Loss: 0.9746428728103638, Val Acc: 0.6975\n","Epoch [37/500], Loss: 1.0737781524658203, Train Acc: 0.6591,train F1-score:0.5438 Val Loss: 0.9620897173881531, Val Acc: 0.7008\n","Epoch [38/500], Loss: 1.080307126045227, Train Acc: 0.6576,train F1-score:0.5440 Val Loss: 0.9521726369857788, Val Acc: 0.7041\n","Epoch [39/500], Loss: 1.068143367767334, Train Acc: 0.6600,train F1-score:0.5469 Val Loss: 0.9448878765106201, Val Acc: 0.7068\n","Epoch [40/500], Loss: 1.0627241134643555, Train Acc: 0.6611,train F1-score:0.5502 Val Loss: 0.9442856311798096, Val Acc: 0.7082\n","Epoch [41/500], Loss: 1.0545424222946167, Train Acc: 0.6629,train F1-score:0.5523 Val Loss: 0.9426397085189819, Val Acc: 0.7115\n","Epoch [42/500], Loss: 1.0603108406066895, Train Acc: 0.6619,train F1-score:0.5515 Val Loss: 0.9422382116317749, Val Acc: 0.7129\n","Epoch [43/500], Loss: 1.0437943935394287, Train Acc: 0.6645,train F1-score:0.5552 Val Loss: 0.937636137008667, Val Acc: 0.7149\n","Epoch [44/500], Loss: 1.052943468093872, Train Acc: 0.6635,train F1-score:0.5542 Val Loss: 0.937523603439331, Val Acc: 0.7135\n","Epoch [45/500], Loss: 1.0374940633773804, Train Acc: 0.6672,train F1-score:0.5581 Val Loss: 0.9370433688163757, Val Acc: 0.7155\n","Epoch [46/500], Loss: 1.0298855304718018, Train Acc: 0.6669,train F1-score:0.5584 Val Loss: 0.9321672916412354, Val Acc: 0.7216\n","Epoch [47/500], Loss: 1.037968635559082, Train Acc: 0.6657,train F1-score:0.5570 Val Loss: 0.9288526773452759, Val Acc: 0.7222\n","Epoch [48/500], Loss: 1.025370478630066, Train Acc: 0.6701,train F1-score:0.5635 Val Loss: 0.9273423552513123, Val Acc: 0.7229\n","Epoch [49/500], Loss: 1.0308074951171875, Train Acc: 0.6676,train F1-score:0.5629 Val Loss: 0.9260097146034241, Val Acc: 0.7216\n","Epoch [50/500], Loss: 1.0196723937988281, Train Acc: 0.6678,train F1-score:0.5612 Val Loss: 0.9219976663589478, Val Acc: 0.7216\n","Epoch [51/500], Loss: 1.0170644521713257, Train Acc: 0.6705,train F1-score:0.5647 Val Loss: 0.9166558980941772, Val Acc: 0.7236\n","Epoch [52/500], Loss: 1.0112886428833008, Train Acc: 0.6722,train F1-score:0.5676 Val Loss: 0.9101632833480835, Val Acc: 0.7249\n","Epoch [53/500], Loss: 1.01535165309906, Train Acc: 0.6722,train F1-score:0.5681 Val Loss: 0.9037728309631348, Val Acc: 0.7249\n","Epoch [54/500], Loss: 1.0077264308929443, Train Acc: 0.6723,train F1-score:0.5689 Val Loss: 0.8982663154602051, Val Acc: 0.7256\n","Epoch [55/500], Loss: 1.0054665803909302, Train Acc: 0.6736,train F1-score:0.5710 Val Loss: 0.8943948149681091, Val Acc: 0.7256\n","Epoch [56/500], Loss: 0.9976474046707153, Train Acc: 0.6731,train F1-score:0.5706 Val Loss: 0.891463577747345, Val Acc: 0.7262\n","Epoch [57/500], Loss: 0.9918087124824524, Train Acc: 0.6740,train F1-score:0.5720 Val Loss: 0.8803686499595642, Val Acc: 0.7269\n","Epoch [58/500], Loss: 0.9914041757583618, Train Acc: 0.6737,train F1-score:0.5723 Val Loss: 0.8649345636367798, Val Acc: 0.7276\n","Epoch [59/500], Loss: 0.9803211688995361, Train Acc: 0.6788,train F1-score:0.5807 Val Loss: 0.8531437516212463, Val Acc: 0.7303\n","Epoch [60/500], Loss: 0.9784567356109619, Train Acc: 0.6818,train F1-score:0.5853 Val Loss: 0.8486771583557129, Val Acc: 0.7303\n","Epoch [61/500], Loss: 0.9834602475166321, Train Acc: 0.6784,train F1-score:0.5816 Val Loss: 0.8501813411712646, Val Acc: 0.7296\n","Epoch [62/500], Loss: 0.9740731716156006, Train Acc: 0.6822,train F1-score:0.5860 Val Loss: 0.8482112884521484, Val Acc: 0.7296\n","Epoch [63/500], Loss: 0.9676862359046936, Train Acc: 0.6828,train F1-score:0.5863 Val Loss: 0.8407115340232849, Val Acc: 0.7303\n","Epoch [64/500], Loss: 0.9657014012336731, Train Acc: 0.6835,train F1-score:0.5882 Val Loss: 0.8391911387443542, Val Acc: 0.7316\n","Epoch [65/500], Loss: 0.9661410450935364, Train Acc: 0.6823,train F1-score:0.5877 Val Loss: 0.8421009182929993, Val Acc: 0.7316\n","Epoch [66/500], Loss: 0.9575701355934143, Train Acc: 0.6835,train F1-score:0.5897 Val Loss: 0.8364006280899048, Val Acc: 0.7316\n","Epoch [67/500], Loss: 0.9536712765693665, Train Acc: 0.6852,train F1-score:0.5913 Val Loss: 0.827140748500824, Val Acc: 0.7309\n","Epoch [68/500], Loss: 0.9624254107475281, Train Acc: 0.6842,train F1-score:0.5894 Val Loss: 0.8258230686187744, Val Acc: 0.7303\n","Epoch [69/500], Loss: 0.9529168009757996, Train Acc: 0.6863,train F1-score:0.5931 Val Loss: 0.8279667496681213, Val Acc: 0.7296\n","Epoch [70/500], Loss: 0.95407634973526, Train Acc: 0.6849,train F1-score:0.5922 Val Loss: 0.8285704851150513, Val Acc: 0.7356\n","Epoch [71/500], Loss: 0.9419232606887817, Train Acc: 0.6871,train F1-score:0.5948 Val Loss: 0.816402792930603, Val Acc: 0.7390\n","Epoch [72/500], Loss: 0.9353760480880737, Train Acc: 0.6913,train F1-score:0.6010 Val Loss: 0.8029367923736572, Val Acc: 0.7463\n","Epoch [73/500], Loss: 0.9403213262557983, Train Acc: 0.6892,train F1-score:0.5980 Val Loss: 0.8056271076202393, Val Acc: 0.7510\n","Epoch [74/500], Loss: 0.9318714737892151, Train Acc: 0.6887,train F1-score:0.5981 Val Loss: 0.8141451478004456, Val Acc: 0.7517\n","Epoch [75/500], Loss: 0.9354743957519531, Train Acc: 0.6900,train F1-score:0.6002 Val Loss: 0.8101930618286133, Val Acc: 0.7530\n","Epoch [76/500], Loss: 0.9339186549186707, Train Acc: 0.6876,train F1-score:0.5980 Val Loss: 0.8017436861991882, Val Acc: 0.7517\n","Epoch [77/500], Loss: 0.9332175850868225, Train Acc: 0.6908,train F1-score:0.6001 Val Loss: 0.8009498715400696, Val Acc: 0.7510\n","Epoch [78/500], Loss: 0.9240487813949585, Train Acc: 0.6908,train F1-score:0.5997 Val Loss: 0.8048639297485352, Val Acc: 0.7510\n","Epoch [79/500], Loss: 0.920347273349762, Train Acc: 0.6902,train F1-score:0.5993 Val Loss: 0.8035247921943665, Val Acc: 0.7510\n","Epoch [80/500], Loss: 0.9212995171546936, Train Acc: 0.6905,train F1-score:0.5996 Val Loss: 0.7956000566482544, Val Acc: 0.7517\n","Epoch [81/500], Loss: 0.9240598082542419, Train Acc: 0.6894,train F1-score:0.5984 Val Loss: 0.789487361907959, Val Acc: 0.7517\n","Epoch [82/500], Loss: 0.916370689868927, Train Acc: 0.6907,train F1-score:0.6007 Val Loss: 0.7873068451881409, Val Acc: 0.7523\n","Epoch [83/500], Loss: 0.9134302735328674, Train Acc: 0.6933,train F1-score:0.6035 Val Loss: 0.7868665456771851, Val Acc: 0.7510\n","Epoch [84/500], Loss: 0.9042965769767761, Train Acc: 0.6919,train F1-score:0.6029 Val Loss: 0.7843648195266724, Val Acc: 0.7510\n","Epoch [85/500], Loss: 0.9020397067070007, Train Acc: 0.6951,train F1-score:0.6077 Val Loss: 0.7793753743171692, Val Acc: 0.7530\n","Epoch [86/500], Loss: 0.9018427729606628, Train Acc: 0.6959,train F1-score:0.6102 Val Loss: 0.7748232483863831, Val Acc: 0.7544\n","Epoch [87/500], Loss: 0.8943958282470703, Train Acc: 0.6961,train F1-score:0.6105 Val Loss: 0.7695929408073425, Val Acc: 0.7544\n","Epoch [88/500], Loss: 0.895301342010498, Train Acc: 0.6977,train F1-score:0.6130 Val Loss: 0.7704357504844666, Val Acc: 0.7544\n","Epoch [89/500], Loss: 0.8869746923446655, Train Acc: 0.6993,train F1-score:0.6154 Val Loss: 0.7716953158378601, Val Acc: 0.7463\n","Epoch [90/500], Loss: 0.8847016096115112, Train Acc: 0.6982,train F1-score:0.6137 Val Loss: 0.7737531065940857, Val Acc: 0.7477\n","Epoch [91/500], Loss: 0.8874215483665466, Train Acc: 0.6991,train F1-score:0.6126 Val Loss: 0.7747260332107544, Val Acc: 0.7537\n","Epoch [92/500], Loss: 0.8898255825042725, Train Acc: 0.6990,train F1-score:0.6156 Val Loss: 0.7709516286849976, Val Acc: 0.7537\n","Epoch [93/500], Loss: 0.8783913850784302, Train Acc: 0.7001,train F1-score:0.6197 Val Loss: 0.7657451629638672, Val Acc: 0.7537\n","Epoch [94/500], Loss: 0.8866345286369324, Train Acc: 0.6965,train F1-score:0.6135 Val Loss: 0.763573408126831, Val Acc: 0.7517\n","Epoch [95/500], Loss: 0.8794440627098083, Train Acc: 0.6995,train F1-score:0.6166 Val Loss: 0.76792973279953, Val Acc: 0.7463\n","Epoch [96/500], Loss: 0.8837103843688965, Train Acc: 0.6966,train F1-score:0.6141 Val Loss: 0.7746970057487488, Val Acc: 0.7530\n","Epoch [97/500], Loss: 0.8761561512947083, Train Acc: 0.7006,train F1-score:0.6203 Val Loss: 0.7756599187850952, Val Acc: 0.7570\n","Epoch [98/500], Loss: 0.8853452205657959, Train Acc: 0.6985,train F1-score:0.6178 Val Loss: 0.7677144408226013, Val Acc: 0.7590\n","Epoch [99/500], Loss: 0.876799464225769, Train Acc: 0.6981,train F1-score:0.6175 Val Loss: 0.7547775506973267, Val Acc: 0.7584\n","Epoch [100/500], Loss: 0.8664062023162842, Train Acc: 0.7017,train F1-score:0.6199 Val Loss: 0.743392825126648, Val Acc: 0.7597\n","Epoch [101/500], Loss: 0.8673152327537537, Train Acc: 0.6985,train F1-score:0.6178 Val Loss: 0.7416723966598511, Val Acc: 0.7577\n","Epoch [102/500], Loss: 0.858229398727417, Train Acc: 0.7031,train F1-score:0.6239 Val Loss: 0.7467283606529236, Val Acc: 0.7590\n","Epoch [103/500], Loss: 0.8570294380187988, Train Acc: 0.7020,train F1-score:0.6250 Val Loss: 0.7507583498954773, Val Acc: 0.7604\n","Epoch [104/500], Loss: 0.8523637652397156, Train Acc: 0.7014,train F1-score:0.6275 Val Loss: 0.7476341128349304, Val Acc: 0.7570\n","Epoch [105/500], Loss: 0.8512936234474182, Train Acc: 0.7047,train F1-score:0.6288 Val Loss: 0.7396901845932007, Val Acc: 0.7584\n","Epoch [106/500], Loss: 0.8586679100990295, Train Acc: 0.7000,train F1-score:0.6198 Val Loss: 0.7396388053894043, Val Acc: 0.7590\n","Epoch [107/500], Loss: 0.8525726795196533, Train Acc: 0.7047,train F1-score:0.6251 Val Loss: 0.7450896501541138, Val Acc: 0.7550\n","Epoch [108/500], Loss: 0.8601857423782349, Train Acc: 0.7035,train F1-score:0.6259 Val Loss: 0.74802565574646, Val Acc: 0.7570\n","Epoch [109/500], Loss: 0.8551832437515259, Train Acc: 0.7051,train F1-score:0.6321 Val Loss: 0.7437320947647095, Val Acc: 0.7584\n","Epoch [110/500], Loss: 0.8443452715873718, Train Acc: 0.7046,train F1-score:0.6376 Val Loss: 0.7315995693206787, Val Acc: 0.7577\n","Epoch [111/500], Loss: 0.8482591509819031, Train Acc: 0.7036,train F1-score:0.6360 Val Loss: 0.7244533896446228, Val Acc: 0.7584\n","Epoch [112/500], Loss: 0.8435460329055786, Train Acc: 0.7062,train F1-score:0.6396 Val Loss: 0.729176938533783, Val Acc: 0.7590\n","Epoch [113/500], Loss: 0.8366517424583435, Train Acc: 0.7042,train F1-score:0.6390 Val Loss: 0.7408319711685181, Val Acc: 0.7584\n","Epoch [114/500], Loss: 0.8377547860145569, Train Acc: 0.7066,train F1-score:0.6423 Val Loss: 0.7399565577507019, Val Acc: 0.7590\n","Epoch [115/500], Loss: 0.850796103477478, Train Acc: 0.7045,train F1-score:0.6360 Val Loss: 0.7340663075447083, Val Acc: 0.7577\n","Epoch [116/500], Loss: 0.8340396285057068, Train Acc: 0.7076,train F1-score:0.6383 Val Loss: 0.7275311350822449, Val Acc: 0.7570\n","Epoch [117/500], Loss: 0.8338533639907837, Train Acc: 0.7083,train F1-score:0.6346 Val Loss: 0.7216123342514038, Val Acc: 0.7577\n","Epoch [118/500], Loss: 0.8372434377670288, Train Acc: 0.7095,train F1-score:0.6403 Val Loss: 0.7207430601119995, Val Acc: 0.7604\n","Epoch [119/500], Loss: 0.8324964046478271, Train Acc: 0.7056,train F1-score:0.6375 Val Loss: 0.7245855927467346, Val Acc: 0.7584\n","Epoch [120/500], Loss: 0.8282309174537659, Train Acc: 0.7084,train F1-score:0.6445 Val Loss: 0.7180464267730713, Val Acc: 0.7577\n","Epoch [121/500], Loss: 0.8276178240776062, Train Acc: 0.7093,train F1-score:0.6431 Val Loss: 0.7132858633995056, Val Acc: 0.7584\n","Epoch [122/500], Loss: 0.8283520936965942, Train Acc: 0.7074,train F1-score:0.6439 Val Loss: 0.7132654786109924, Val Acc: 0.7584\n","Epoch [123/500], Loss: 0.823328971862793, Train Acc: 0.7113,train F1-score:0.6477 Val Loss: 0.7134227156639099, Val Acc: 0.7577\n","Epoch [124/500], Loss: 0.8161217570304871, Train Acc: 0.7099,train F1-score:0.6485 Val Loss: 0.7148162126541138, Val Acc: 0.7584\n","Epoch [125/500], Loss: 0.8298685550689697, Train Acc: 0.7062,train F1-score:0.6486 Val Loss: 0.7202818989753723, Val Acc: 0.7577\n","Epoch [126/500], Loss: 0.8192600607872009, Train Acc: 0.7078,train F1-score:0.6529 Val Loss: 0.7166429162025452, Val Acc: 0.7577\n","Epoch [127/500], Loss: 0.8238606452941895, Train Acc: 0.7114,train F1-score:0.6555 Val Loss: 0.7093809843063354, Val Acc: 0.7584\n","Epoch [128/500], Loss: 0.8135318160057068, Train Acc: 0.7130,train F1-score:0.6573 Val Loss: 0.6992701292037964, Val Acc: 0.7577\n","Epoch [129/500], Loss: 0.8111929893493652, Train Acc: 0.7137,train F1-score:0.6565 Val Loss: 0.6962458491325378, Val Acc: 0.7597\n","Epoch [130/500], Loss: 0.8068789839744568, Train Acc: 0.7125,train F1-score:0.6560 Val Loss: 0.6954596638679504, Val Acc: 0.7604\n","Epoch [131/500], Loss: 0.8019043803215027, Train Acc: 0.7129,train F1-score:0.6576 Val Loss: 0.6989465951919556, Val Acc: 0.7584\n","Epoch [132/500], Loss: 0.8098401427268982, Train Acc: 0.7134,train F1-score:0.6606 Val Loss: 0.7068405747413635, Val Acc: 0.7577\n","Epoch [133/500], Loss: 0.8021485209465027, Train Acc: 0.7134,train F1-score:0.6622 Val Loss: 0.7157467603683472, Val Acc: 0.7597\n","Epoch [134/500], Loss: 0.8141420483589172, Train Acc: 0.7150,train F1-score:0.6687 Val Loss: 0.7151170372962952, Val Acc: 0.7604\n","Epoch [135/500], Loss: 0.793961763381958, Train Acc: 0.7163,train F1-score:0.6705 Val Loss: 0.6978775858879089, Val Acc: 0.7597\n","Epoch [136/500], Loss: 0.8220950961112976, Train Acc: 0.7159,train F1-score:0.6709 Val Loss: 0.6899053454399109, Val Acc: 0.7617\n","Epoch [137/500], Loss: 0.7948787212371826, Train Acc: 0.7177,train F1-score:0.6702 Val Loss: 0.6880297064781189, Val Acc: 0.7610\n","Epoch [138/500], Loss: 0.8238406181335449, Train Acc: 0.7149,train F1-score:0.6724 Val Loss: 0.6940407156944275, Val Acc: 0.7584\n","Epoch [139/500], Loss: 0.7983627319335938, Train Acc: 0.7135,train F1-score:0.6691 Val Loss: 0.7019100189208984, Val Acc: 0.7677\n","Epoch [140/500], Loss: 0.7989420294761658, Train Acc: 0.7145,train F1-score:0.6727 Val Loss: 0.7075415849685669, Val Acc: 0.7758\n","Epoch [141/500], Loss: 0.7925978302955627, Train Acc: 0.7156,train F1-score:0.6738 Val Loss: 0.7051612734794617, Val Acc: 0.7590\n","Epoch [142/500], Loss: 0.7851674556732178, Train Acc: 0.7186,train F1-score:0.6763 Val Loss: 0.6949660778045654, Val Acc: 0.7617\n","Epoch [143/500], Loss: 0.7908554077148438, Train Acc: 0.7154,train F1-score:0.6700 Val Loss: 0.6870952248573303, Val Acc: 0.7610\n","Epoch [144/500], Loss: 0.7985605597496033, Train Acc: 0.7212,train F1-score:0.6735 Val Loss: 0.6866479516029358, Val Acc: 0.7671\n","Epoch [145/500], Loss: 0.7778705358505249, Train Acc: 0.7222,train F1-score:0.6723 Val Loss: 0.6900249123573303, Val Acc: 0.7871\n","Epoch [146/500], Loss: 0.7880668640136719, Train Acc: 0.7195,train F1-score:0.6731 Val Loss: 0.6953530311584473, Val Acc: 0.7878\n","Epoch [147/500], Loss: 0.7828659415245056, Train Acc: 0.7243,train F1-score:0.6823 Val Loss: 0.695564866065979, Val Acc: 0.7885\n","Epoch [148/500], Loss: 0.7817960977554321, Train Acc: 0.7240,train F1-score:0.6828 Val Loss: 0.6911618113517761, Val Acc: 0.7871\n","Epoch [149/500], Loss: 0.7743507027626038, Train Acc: 0.7217,train F1-score:0.6798 Val Loss: 0.6868568658828735, Val Acc: 0.7892\n","Epoch [150/500], Loss: 0.7822505235671997, Train Acc: 0.7236,train F1-score:0.6804 Val Loss: 0.6891331076622009, Val Acc: 0.7925\n","Epoch [151/500], Loss: 0.7815062999725342, Train Acc: 0.7210,train F1-score:0.6803 Val Loss: 0.6909047365188599, Val Acc: 0.7965\n","Epoch [152/500], Loss: 0.7927822470664978, Train Acc: 0.7210,train F1-score:0.6813 Val Loss: 0.6896183490753174, Val Acc: 0.7959\n","Epoch [153/500], Loss: 0.7801619172096252, Train Acc: 0.7243,train F1-score:0.6848 Val Loss: 0.6845600605010986, Val Acc: 0.7938\n","Epoch [154/500], Loss: 0.7792432904243469, Train Acc: 0.7221,train F1-score:0.6828 Val Loss: 0.6814711689949036, Val Acc: 0.7912\n","Epoch [155/500], Loss: 0.7743771076202393, Train Acc: 0.7221,train F1-score:0.6819 Val Loss: 0.6797969341278076, Val Acc: 0.7938\n","Epoch [156/500], Loss: 0.7763100862503052, Train Acc: 0.7203,train F1-score:0.6824 Val Loss: 0.6779403686523438, Val Acc: 0.7959\n","Epoch [157/500], Loss: 0.7601462006568909, Train Acc: 0.7281,train F1-score:0.6912 Val Loss: 0.6721158623695374, Val Acc: 0.7965\n","Epoch [158/500], Loss: 0.7685286998748779, Train Acc: 0.7237,train F1-score:0.6866 Val Loss: 0.666508674621582, Val Acc: 0.7878\n","Epoch [159/500], Loss: 0.771195650100708, Train Acc: 0.7264,train F1-score:0.6910 Val Loss: 0.6652635931968689, Val Acc: 0.7865\n","Epoch [160/500], Loss: 0.7642847299575806, Train Acc: 0.7232,train F1-score:0.6872 Val Loss: 0.6700594425201416, Val Acc: 0.7912\n","Epoch [161/500], Loss: 0.7714687585830688, Train Acc: 0.7258,train F1-score:0.6906 Val Loss: 0.6736764907836914, Val Acc: 0.7918\n","Epoch [162/500], Loss: 0.7732037901878357, Train Acc: 0.7255,train F1-score:0.6921 Val Loss: 0.6783266067504883, Val Acc: 0.7905\n","Epoch [163/500], Loss: 0.7605850100517273, Train Acc: 0.7270,train F1-score:0.6931 Val Loss: 0.6811763048171997, Val Acc: 0.7865\n","Epoch [164/500], Loss: 0.7609843015670776, Train Acc: 0.7288,train F1-score:0.6959 Val Loss: 0.6735154986381531, Val Acc: 0.7892\n","Epoch [165/500], Loss: 0.7690370082855225, Train Acc: 0.7268,train F1-score:0.6934 Val Loss: 0.6713848114013672, Val Acc: 0.7892\n","Epoch [166/500], Loss: 0.7668508291244507, Train Acc: 0.7263,train F1-score:0.6906 Val Loss: 0.673619270324707, Val Acc: 0.7811\n","Epoch [167/500], Loss: 0.7538971900939941, Train Acc: 0.7275,train F1-score:0.6897 Val Loss: 0.6752415895462036, Val Acc: 0.7811\n","Epoch [168/500], Loss: 0.7615461349487305, Train Acc: 0.7317,train F1-score:0.6938 Val Loss: 0.678546130657196, Val Acc: 0.7918\n","Epoch [169/500], Loss: 0.7609270215034485, Train Acc: 0.7340,train F1-score:0.6996 Val Loss: 0.6793160438537598, Val Acc: 0.7979\n","Epoch [170/500], Loss: 0.7491713762283325, Train Acc: 0.7353,train F1-score:0.7021 Val Loss: 0.6769610047340393, Val Acc: 0.7992\n","Epoch [171/500], Loss: 0.75992751121521, Train Acc: 0.7313,train F1-score:0.7012 Val Loss: 0.6704161763191223, Val Acc: 0.7992\n","Epoch [172/500], Loss: 0.7481301426887512, Train Acc: 0.7320,train F1-score:0.7009 Val Loss: 0.6639083027839661, Val Acc: 0.7992\n","Epoch [173/500], Loss: 0.7466089129447937, Train Acc: 0.7375,train F1-score:0.7069 Val Loss: 0.6595918536186218, Val Acc: 0.7979\n","Epoch [174/500], Loss: 0.7521439790725708, Train Acc: 0.7312,train F1-score:0.7007 Val Loss: 0.6611470580101013, Val Acc: 0.7985\n","Epoch [175/500], Loss: 0.7530699372291565, Train Acc: 0.7367,train F1-score:0.7080 Val Loss: 0.6633657813072205, Val Acc: 0.7965\n","Epoch [176/500], Loss: 0.7410634756088257, Train Acc: 0.7410,train F1-score:0.7118 Val Loss: 0.6552131772041321, Val Acc: 0.7959\n","Epoch [177/500], Loss: 0.7512293457984924, Train Acc: 0.7352,train F1-score:0.7049 Val Loss: 0.6551202535629272, Val Acc: 0.7938\n","Epoch [178/500], Loss: 0.7591114044189453, Train Acc: 0.7360,train F1-score:0.7046 Val Loss: 0.6586131453514099, Val Acc: 0.7945\n","Epoch [179/500], Loss: 0.7398831248283386, Train Acc: 0.7365,train F1-score:0.7070 Val Loss: 0.6593278646469116, Val Acc: 0.7992\n","Epoch [180/500], Loss: 0.7470986247062683, Train Acc: 0.7404,train F1-score:0.7134 Val Loss: 0.6525237560272217, Val Acc: 0.7972\n","Epoch [181/500], Loss: 0.7459598183631897, Train Acc: 0.7397,train F1-score:0.7142 Val Loss: 0.6393185257911682, Val Acc: 0.7992\n","Epoch [182/500], Loss: 0.7504581212997437, Train Acc: 0.7348,train F1-score:0.7066 Val Loss: 0.6421811580657959, Val Acc: 0.7979\n","Epoch [183/500], Loss: 0.808992326259613, Train Acc: 0.7416,train F1-score:0.7130 Val Loss: 0.65406733751297, Val Acc: 0.7972\n","Epoch [184/500], Loss: 0.7470378279685974, Train Acc: 0.7376,train F1-score:0.7103 Val Loss: 0.6645057201385498, Val Acc: 0.8025\n","Epoch [185/500], Loss: 0.7496759295463562, Train Acc: 0.7376,train F1-score:0.7104 Val Loss: 0.6642708778381348, Val Acc: 0.8012\n","Epoch [186/500], Loss: 0.7650794982910156, Train Acc: 0.7300,train F1-score:0.7017 Val Loss: 0.6488997936248779, Val Acc: 0.7999\n","Epoch [187/500], Loss: 0.7595470547676086, Train Acc: 0.7360,train F1-score:0.7054 Val Loss: 0.6321080327033997, Val Acc: 0.7959\n","Epoch [188/500], Loss: 0.7676619291305542, Train Acc: 0.7312,train F1-score:0.6965 Val Loss: 0.62901771068573, Val Acc: 0.7979\n","Epoch [189/500], Loss: 0.7580141425132751, Train Acc: 0.7304,train F1-score:0.6930 Val Loss: 0.6379356384277344, Val Acc: 0.7952\n","Epoch [190/500], Loss: 0.7649245858192444, Train Acc: 0.7358,train F1-score:0.7001 Val Loss: 0.6487882137298584, Val Acc: 0.7959\n","Epoch [191/500], Loss: 0.7621361613273621, Train Acc: 0.7309,train F1-score:0.6980 Val Loss: 0.6471375226974487, Val Acc: 0.7925\n","Epoch [192/500], Loss: 0.7624797821044922, Train Acc: 0.7319,train F1-score:0.6974 Val Loss: 0.6499984264373779, Val Acc: 0.7952\n","Epoch [193/500], Loss: 0.7680925130844116, Train Acc: 0.7278,train F1-score:0.6898 Val Loss: 0.662813663482666, Val Acc: 0.7912\n","Epoch [194/500], Loss: 0.7616848945617676, Train Acc: 0.7300,train F1-score:0.6943 Val Loss: 0.670680046081543, Val Acc: 0.7952\n","Epoch [195/500], Loss: 0.7569618821144104, Train Acc: 0.7358,train F1-score:0.6999 Val Loss: 0.6750007271766663, Val Acc: 0.7912\n","Epoch [196/500], Loss: 0.7611359357833862, Train Acc: 0.7308,train F1-score:0.7003 Val Loss: 0.6609659790992737, Val Acc: 0.7985\n","Epoch [197/500], Loss: 0.7608237862586975, Train Acc: 0.7342,train F1-score:0.7073 Val Loss: 0.642444372177124, Val Acc: 0.7959\n","Epoch [198/500], Loss: 0.750058650970459, Train Acc: 0.7371,train F1-score:0.7094 Val Loss: 0.6375024914741516, Val Acc: 0.7999\n","Epoch [199/500], Loss: 0.750939130783081, Train Acc: 0.7393,train F1-score:0.7117 Val Loss: 0.6469590663909912, Val Acc: 0.8019\n","Epoch [200/500], Loss: 0.7537090182304382, Train Acc: 0.7398,train F1-score:0.7137 Val Loss: 0.6582304239273071, Val Acc: 0.8039\n","Epoch [201/500], Loss: 0.7425796389579773, Train Acc: 0.7385,train F1-score:0.7136 Val Loss: 0.657075047492981, Val Acc: 0.8039\n","Epoch [202/500], Loss: 0.744564950466156, Train Acc: 0.7365,train F1-score:0.7140 Val Loss: 0.647376537322998, Val Acc: 0.8012\n","Epoch [203/500], Loss: 0.7436517477035522, Train Acc: 0.7417,train F1-score:0.7153 Val Loss: 0.6394782662391663, Val Acc: 0.8046\n","Epoch [204/500], Loss: 0.7526176571846008, Train Acc: 0.7377,train F1-score:0.7137 Val Loss: 0.6411405205726624, Val Acc: 0.8039\n","Epoch [205/500], Loss: 0.7406008243560791, Train Acc: 0.7399,train F1-score:0.7159 Val Loss: 0.6462493538856506, Val Acc: 0.8059\n","Epoch [206/500], Loss: 0.7459246516227722, Train Acc: 0.7382,train F1-score:0.7141 Val Loss: 0.6481543779373169, Val Acc: 0.8092\n","Epoch [207/500], Loss: 0.7376025319099426, Train Acc: 0.7407,train F1-score:0.7177 Val Loss: 0.6397187113761902, Val Acc: 0.8079\n","Epoch [208/500], Loss: 0.7375708818435669, Train Acc: 0.7402,train F1-score:0.7111 Val Loss: 0.6348594427108765, Val Acc: 0.8072\n","Epoch [209/500], Loss: 0.742533802986145, Train Acc: 0.7401,train F1-score:0.7118 Val Loss: 0.6361613273620605, Val Acc: 0.8066\n","Epoch [210/500], Loss: 0.7506762742996216, Train Acc: 0.7424,train F1-score:0.7176 Val Loss: 0.6418343782424927, Val Acc: 0.8012\n","Epoch [211/500], Loss: 0.7316097021102905, Train Acc: 0.7433,train F1-score:0.7219 Val Loss: 0.6348908543586731, Val Acc: 0.8059\n","Epoch [212/500], Loss: 0.7303193211555481, Train Acc: 0.7454,train F1-score:0.7222 Val Loss: 0.6309801936149597, Val Acc: 0.8052\n","Epoch [213/500], Loss: 0.7314437031745911, Train Acc: 0.7469,train F1-score:0.7214 Val Loss: 0.6343854069709778, Val Acc: 0.8052\n","Epoch [214/500], Loss: 0.7241771221160889, Train Acc: 0.7443,train F1-score:0.7215 Val Loss: 0.6434611678123474, Val Acc: 0.8072\n","Epoch [215/500], Loss: 0.731532096862793, Train Acc: 0.7429,train F1-score:0.7231 Val Loss: 0.642680287361145, Val Acc: 0.8019\n","Epoch [216/500], Loss: 0.7355918884277344, Train Acc: 0.7456,train F1-score:0.7228 Val Loss: 0.6405503749847412, Val Acc: 0.7965\n","Epoch [217/500], Loss: 0.7289755344390869, Train Acc: 0.7455,train F1-score:0.7208 Val Loss: 0.6390724778175354, Val Acc: 0.7972\n","Epoch [218/500], Loss: 0.7193499207496643, Train Acc: 0.7495,train F1-score:0.7249 Val Loss: 0.6339472532272339, Val Acc: 0.7992\n","Epoch [219/500], Loss: 0.7246082425117493, Train Acc: 0.7437,train F1-score:0.7198 Val Loss: 0.6308237910270691, Val Acc: 0.7959\n","Epoch [220/500], Loss: 0.7251695990562439, Train Acc: 0.7485,train F1-score:0.7264 Val Loss: 0.6291659474372864, Val Acc: 0.7965\n","Epoch [221/500], Loss: 0.730401873588562, Train Acc: 0.7396,train F1-score:0.7189 Val Loss: 0.6272532939910889, Val Acc: 0.7965\n","Epoch [222/500], Loss: 0.7194504141807556, Train Acc: 0.7469,train F1-score:0.7232 Val Loss: 0.6257091164588928, Val Acc: 0.7959\n","Epoch [223/500], Loss: 0.7271772623062134, Train Acc: 0.7443,train F1-score:0.7195 Val Loss: 0.6298627853393555, Val Acc: 0.8005\n","Epoch [224/500], Loss: 0.7269873023033142, Train Acc: 0.7466,train F1-score:0.7221 Val Loss: 0.6433676481246948, Val Acc: 0.7985\n","Epoch [225/500], Loss: 0.7203771471977234, Train Acc: 0.7501,train F1-score:0.7265 Val Loss: 0.652028501033783, Val Acc: 0.7985\n","Epoch [226/500], Loss: 0.7237321138381958, Train Acc: 0.7516,train F1-score:0.7298 Val Loss: 0.6443676948547363, Val Acc: 0.7992\n","Epoch [227/500], Loss: 0.7172470688819885, Train Acc: 0.7504,train F1-score:0.7280 Val Loss: 0.6296777129173279, Val Acc: 0.7999\n","Epoch [228/500], Loss: 0.721341609954834, Train Acc: 0.7460,train F1-score:0.7220 Val Loss: 0.6266801953315735, Val Acc: 0.8046\n","Epoch [229/500], Loss: 0.7174551486968994, Train Acc: 0.7476,train F1-score:0.7234 Val Loss: 0.6306388974189758, Val Acc: 0.8126\n","Epoch [230/500], Loss: 0.7198706269264221, Train Acc: 0.7485,train F1-score:0.7268 Val Loss: 0.6353781223297119, Val Acc: 0.8112\n","Epoch [231/500], Loss: 0.7270914912223816, Train Acc: 0.7490,train F1-score:0.7302 Val Loss: 0.6296088695526123, Val Acc: 0.8139\n","Epoch [232/500], Loss: 0.702967643737793, Train Acc: 0.7564,train F1-score:0.7372 Val Loss: 0.6221075654029846, Val Acc: 0.8206\n","Epoch [233/500], Loss: 0.7203522324562073, Train Acc: 0.7463,train F1-score:0.7235 Val Loss: 0.6269274353981018, Val Acc: 0.8206\n","Epoch [234/500], Loss: 0.7133380174636841, Train Acc: 0.7471,train F1-score:0.7261 Val Loss: 0.6346801519393921, Val Acc: 0.8199\n","Epoch [235/500], Loss: 0.715024471282959, Train Acc: 0.7460,train F1-score:0.7271 Val Loss: 0.626939058303833, Val Acc: 0.8166\n","Epoch [236/500], Loss: 0.713008463382721, Train Acc: 0.7495,train F1-score:0.7292 Val Loss: 0.6205291748046875, Val Acc: 0.8019\n","Epoch [237/500], Loss: 0.7127189040184021, Train Acc: 0.7467,train F1-score:0.7233 Val Loss: 0.6256329417228699, Val Acc: 0.8173\n","Epoch [238/500], Loss: 0.7170283794403076, Train Acc: 0.7481,train F1-score:0.7260 Val Loss: 0.6295192837715149, Val Acc: 0.8226\n","Epoch [239/500], Loss: 0.7177379727363586, Train Acc: 0.7499,train F1-score:0.7303 Val Loss: 0.6254907846450806, Val Acc: 0.8079\n","Epoch [240/500], Loss: 0.7120974063873291, Train Acc: 0.7459,train F1-score:0.7281 Val Loss: 0.612208902835846, Val Acc: 0.8072\n","Epoch [241/500], Loss: 0.7124403715133667, Train Acc: 0.7481,train F1-score:0.7262 Val Loss: 0.6100519895553589, Val Acc: 0.8092\n","Epoch [242/500], Loss: 0.7175408005714417, Train Acc: 0.7490,train F1-score:0.7224 Val Loss: 0.6191589832305908, Val Acc: 0.8039\n","Epoch [243/500], Loss: 0.7088088989257812, Train Acc: 0.7514,train F1-score:0.7250 Val Loss: 0.6337041258811951, Val Acc: 0.8012\n","Epoch [244/500], Loss: 0.710468053817749, Train Acc: 0.7539,train F1-score:0.7313 Val Loss: 0.627273678779602, Val Acc: 0.8025\n","Epoch [245/500], Loss: 0.7115460634231567, Train Acc: 0.7505,train F1-score:0.7263 Val Loss: 0.612558126449585, Val Acc: 0.8005\n","Epoch [246/500], Loss: 0.7030411958694458, Train Acc: 0.7532,train F1-score:0.7279 Val Loss: 0.6094228029251099, Val Acc: 0.7992\n","Epoch [247/500], Loss: 0.7084737420082092, Train Acc: 0.7508,train F1-score:0.7255 Val Loss: 0.6168010830879211, Val Acc: 0.8032\n","Epoch [248/500], Loss: 0.6981590390205383, Train Acc: 0.7541,train F1-score:0.7334 Val Loss: 0.6187219023704529, Val Acc: 0.8186\n","Epoch [249/500], Loss: 0.698777973651886, Train Acc: 0.7545,train F1-score:0.7366 Val Loss: 0.6106922626495361, Val Acc: 0.8146\n","Epoch [250/500], Loss: 0.6963869333267212, Train Acc: 0.7564,train F1-score:0.7372 Val Loss: 0.605408251285553, Val Acc: 0.8153\n","Epoch [251/500], Loss: 0.7302599549293518, Train Acc: 0.7466,train F1-score:0.7272 Val Loss: 0.6011500358581543, Val Acc: 0.8025\n","Epoch [252/500], Loss: 0.6995078921318054, Train Acc: 0.7533,train F1-score:0.7326 Val Loss: 0.6093013882637024, Val Acc: 0.8032\n","Epoch [253/500], Loss: 0.7073308825492859, Train Acc: 0.7503,train F1-score:0.7317 Val Loss: 0.6184083223342896, Val Acc: 0.7999\n","Epoch [254/500], Loss: 0.7067838907241821, Train Acc: 0.7537,train F1-score:0.7348 Val Loss: 0.6222131252288818, Val Acc: 0.8012\n","Epoch [255/500], Loss: 0.7016218304634094, Train Acc: 0.7553,train F1-score:0.7336 Val Loss: 0.6179250478744507, Val Acc: 0.8059\n","Epoch [256/500], Loss: 0.704937219619751, Train Acc: 0.7533,train F1-score:0.7314 Val Loss: 0.6151725649833679, Val Acc: 0.8092\n","Epoch [257/500], Loss: 0.7070671319961548, Train Acc: 0.7524,train F1-score:0.7287 Val Loss: 0.6155920624732971, Val Acc: 0.8106\n","Epoch [258/500], Loss: 0.7083694934844971, Train Acc: 0.7536,train F1-score:0.7324 Val Loss: 0.6153765320777893, Val Acc: 0.8086\n","Epoch [259/500], Loss: 0.6955643892288208, Train Acc: 0.7536,train F1-score:0.7313 Val Loss: 0.6105009913444519, Val Acc: 0.8079\n","Epoch [260/500], Loss: 0.695434033870697, Train Acc: 0.7529,train F1-score:0.7306 Val Loss: 0.6088888645172119, Val Acc: 0.8260\n","Epoch [261/500], Loss: 0.6961567401885986, Train Acc: 0.7593,train F1-score:0.7389 Val Loss: 0.610476016998291, Val Acc: 0.8280\n","Epoch [262/500], Loss: 0.6951337456703186, Train Acc: 0.7598,train F1-score:0.7414 Val Loss: 0.6084973812103271, Val Acc: 0.8273\n","Epoch [263/500], Loss: 0.6850656867027283, Train Acc: 0.7613,train F1-score:0.7448 Val Loss: 0.5953803062438965, Val Acc: 0.8280\n","Epoch [264/500], Loss: 0.6987248063087463, Train Acc: 0.7559,train F1-score:0.7373 Val Loss: 0.5882707834243774, Val Acc: 0.8273\n","Epoch [265/500], Loss: 0.6977849006652832, Train Acc: 0.7591,train F1-score:0.7419 Val Loss: 0.5937900543212891, Val Acc: 0.8186\n","Epoch [266/500], Loss: 0.6978369355201721, Train Acc: 0.7567,train F1-score:0.7417 Val Loss: 0.6013489961624146, Val Acc: 0.8173\n","Epoch [267/500], Loss: 0.6960566639900208, Train Acc: 0.7565,train F1-score:0.7424 Val Loss: 0.6043208241462708, Val Acc: 0.8046\n","Epoch [268/500], Loss: 0.6927591562271118, Train Acc: 0.7551,train F1-score:0.7373 Val Loss: 0.6053965091705322, Val Acc: 0.8106\n","Epoch [269/500], Loss: 0.6834647059440613, Train Acc: 0.7612,train F1-score:0.7384 Val Loss: 0.6047399044036865, Val Acc: 0.8032\n","Epoch [270/500], Loss: 0.6935503482818604, Train Acc: 0.7578,train F1-score:0.7329 Val Loss: 0.6089301109313965, Val Acc: 0.8039\n","Epoch [271/500], Loss: 0.6944956183433533, Train Acc: 0.7593,train F1-score:0.7337 Val Loss: 0.6111552715301514, Val Acc: 0.8052\n","Epoch [272/500], Loss: 0.6841849088668823, Train Acc: 0.7599,train F1-score:0.7362 Val Loss: 0.6112884283065796, Val Acc: 0.8046\n","Epoch [273/500], Loss: 0.6921148896217346, Train Acc: 0.7620,train F1-score:0.7399 Val Loss: 0.6123636960983276, Val Acc: 0.8079\n","Epoch [274/500], Loss: 0.6844385862350464, Train Acc: 0.7593,train F1-score:0.7399 Val Loss: 0.6083879470825195, Val Acc: 0.8213\n","Epoch [275/500], Loss: 0.6908695697784424, Train Acc: 0.7602,train F1-score:0.7427 Val Loss: 0.6087843179702759, Val Acc: 0.8146\n","Epoch [276/500], Loss: 0.7025932669639587, Train Acc: 0.7587,train F1-score:0.7426 Val Loss: 0.6142303347587585, Val Acc: 0.8159\n","Epoch [277/500], Loss: 0.6893203854560852, Train Acc: 0.7630,train F1-score:0.7448 Val Loss: 0.6163630485534668, Val Acc: 0.8025\n","Epoch [278/500], Loss: 0.6947177648544312, Train Acc: 0.7584,train F1-score:0.7381 Val Loss: 0.6194294691085815, Val Acc: 0.8032\n","Epoch [279/500], Loss: 0.6998536586761475, Train Acc: 0.7611,train F1-score:0.7424 Val Loss: 0.6188766956329346, Val Acc: 0.8039\n","Epoch [280/500], Loss: 0.6854010224342346, Train Acc: 0.7614,train F1-score:0.7406 Val Loss: 0.6106787323951721, Val Acc: 0.8099\n","Epoch [281/500], Loss: 0.6881012320518494, Train Acc: 0.7554,train F1-score:0.7307 Val Loss: 0.6043033003807068, Val Acc: 0.8092\n","Epoch [282/500], Loss: 0.6888671517372131, Train Acc: 0.7624,train F1-score:0.7394 Val Loss: 0.6053237318992615, Val Acc: 0.8266\n","Epoch [283/500], Loss: 0.6961811780929565, Train Acc: 0.7603,train F1-score:0.7418 Val Loss: 0.6153897643089294, Val Acc: 0.8246\n","Epoch [284/500], Loss: 0.6884157061576843, Train Acc: 0.7586,train F1-score:0.7453 Val Loss: 0.5957292318344116, Val Acc: 0.8246\n","Epoch [285/500], Loss: 0.682607114315033, Train Acc: 0.7617,train F1-score:0.7458 Val Loss: 0.5819348096847534, Val Acc: 0.8233\n","Epoch [286/500], Loss: 0.6871129274368286, Train Acc: 0.7658,train F1-score:0.7466 Val Loss: 0.5804048180580139, Val Acc: 0.8273\n","Epoch [287/500], Loss: 0.6937421560287476, Train Acc: 0.7612,train F1-score:0.7444 Val Loss: 0.5894111394882202, Val Acc: 0.8293\n","Epoch [288/500], Loss: 0.6902421116828918, Train Acc: 0.7607,train F1-score:0.7458 Val Loss: 0.5969324707984924, Val Acc: 0.8266\n","Epoch [289/500], Loss: 0.6876874566078186, Train Acc: 0.7576,train F1-score:0.7441 Val Loss: 0.5925624966621399, Val Acc: 0.8233\n","Epoch [290/500], Loss: 0.6741858720779419, Train Acc: 0.7652,train F1-score:0.7475 Val Loss: 0.5861669778823853, Val Acc: 0.8139\n","Epoch [291/500], Loss: 0.6793150305747986, Train Acc: 0.7622,train F1-score:0.7399 Val Loss: 0.592367947101593, Val Acc: 0.8126\n","Epoch [292/500], Loss: 0.6737645268440247, Train Acc: 0.7665,train F1-score:0.7454 Val Loss: 0.5987221002578735, Val Acc: 0.8166\n","Epoch [293/500], Loss: 0.6889557838439941, Train Acc: 0.7624,train F1-score:0.7444 Val Loss: 0.5995969176292419, Val Acc: 0.8146\n","Epoch [294/500], Loss: 0.6961145401000977, Train Acc: 0.7622,train F1-score:0.7461 Val Loss: 0.588576078414917, Val Acc: 0.8173\n","Epoch [295/500], Loss: 0.681541919708252, Train Acc: 0.7657,train F1-score:0.7470 Val Loss: 0.5904644727706909, Val Acc: 0.8246\n","Epoch [296/500], Loss: 0.6809525489807129, Train Acc: 0.7599,train F1-score:0.7417 Val Loss: 0.5961540937423706, Val Acc: 0.8193\n","Epoch [297/500], Loss: 0.668962836265564, Train Acc: 0.7677,train F1-score:0.7529 Val Loss: 0.6037084460258484, Val Acc: 0.8206\n","Epoch [298/500], Loss: 0.677348792552948, Train Acc: 0.7618,train F1-score:0.7462 Val Loss: 0.6114857196807861, Val Acc: 0.8072\n","Epoch [299/500], Loss: 0.6785091757774353, Train Acc: 0.7601,train F1-score:0.7436 Val Loss: 0.6133027672767639, Val Acc: 0.8133\n","Epoch [300/500], Loss: 0.6782508492469788, Train Acc: 0.7675,train F1-score:0.7498 Val Loss: 0.6026085019111633, Val Acc: 0.8146\n","Epoch [301/500], Loss: 0.6872329115867615, Train Acc: 0.7562,train F1-score:0.7372 Val Loss: 0.6050593256950378, Val Acc: 0.8133\n","Epoch [302/500], Loss: 0.6781889796257019, Train Acc: 0.7599,train F1-score:0.7416 Val Loss: 0.6062819957733154, Val Acc: 0.8119\n","Epoch [303/500], Loss: 0.6758424043655396, Train Acc: 0.7635,train F1-score:0.7451 Val Loss: 0.6076145172119141, Val Acc: 0.8119\n","Epoch [304/500], Loss: 0.6804526448249817, Train Acc: 0.7625,train F1-score:0.7471 Val Loss: 0.5952727794647217, Val Acc: 0.8052\n","Epoch [305/500], Loss: 0.6786916851997375, Train Acc: 0.7654,train F1-score:0.7479 Val Loss: 0.5867778658866882, Val Acc: 0.8025\n","Epoch [306/500], Loss: 0.6711167097091675, Train Acc: 0.7661,train F1-score:0.7487 Val Loss: 0.5806294083595276, Val Acc: 0.8039\n","Epoch [307/500], Loss: 0.687977135181427, Train Acc: 0.7646,train F1-score:0.7467 Val Loss: 0.5839349627494812, Val Acc: 0.8019\n","Epoch [308/500], Loss: 0.6809364557266235, Train Acc: 0.7596,train F1-score:0.7436 Val Loss: 0.5940923690795898, Val Acc: 0.8039\n","Epoch [309/500], Loss: 0.6890379786491394, Train Acc: 0.7614,train F1-score:0.7443 Val Loss: 0.6039552092552185, Val Acc: 0.8059\n","Epoch [310/500], Loss: 0.6655732989311218, Train Acc: 0.7627,train F1-score:0.7447 Val Loss: 0.6004661321640015, Val Acc: 0.8099\n","Epoch [311/500], Loss: 0.6768816709518433, Train Acc: 0.7644,train F1-score:0.7462 Val Loss: 0.5971699953079224, Val Acc: 0.8133\n","Epoch [312/500], Loss: 0.6715412139892578, Train Acc: 0.7615,train F1-score:0.7433 Val Loss: 0.5920896530151367, Val Acc: 0.8240\n","Epoch [313/500], Loss: 0.6750909090042114, Train Acc: 0.7631,train F1-score:0.7471 Val Loss: 0.5857065320014954, Val Acc: 0.8233\n","Epoch [314/500], Loss: 0.6663908362388611, Train Acc: 0.7649,train F1-score:0.7466 Val Loss: 0.5835622549057007, Val Acc: 0.8260\n","Epoch [315/500], Loss: 0.6762646436691284, Train Acc: 0.7676,train F1-score:0.7510 Val Loss: 0.5885654091835022, Val Acc: 0.8233\n","Epoch [316/500], Loss: 0.6685001254081726, Train Acc: 0.7698,train F1-score:0.7544 Val Loss: 0.593335747718811, Val Acc: 0.8213\n","Epoch [317/500], Loss: 0.673823356628418, Train Acc: 0.7625,train F1-score:0.7484 Val Loss: 0.5894405841827393, Val Acc: 0.8106\n","Epoch [318/500], Loss: 0.684628963470459, Train Acc: 0.7618,train F1-score:0.7453 Val Loss: 0.5908934473991394, Val Acc: 0.8119\n","Epoch [319/500], Loss: 0.6631513833999634, Train Acc: 0.7675,train F1-score:0.7477 Val Loss: 0.5919817090034485, Val Acc: 0.8133\n","Epoch [320/500], Loss: 0.8580780625343323, Train Acc: 0.7582,train F1-score:0.7410 Val Loss: 0.5857649445533752, Val Acc: 0.8066\n","Epoch [321/500], Loss: 0.6774017214775085, Train Acc: 0.7580,train F1-score:0.7344 Val Loss: 0.5859673023223877, Val Acc: 0.8052\n","Epoch [322/500], Loss: 0.6883774399757385, Train Acc: 0.7630,train F1-score:0.7382 Val Loss: 0.5974974632263184, Val Acc: 0.8079\n","Epoch [323/500], Loss: 0.6818354725837708, Train Acc: 0.7640,train F1-score:0.7456 Val Loss: 0.6121332049369812, Val Acc: 0.8032\n","Epoch [324/500], Loss: 0.6829476356506348, Train Acc: 0.7623,train F1-score:0.7471 Val Loss: 0.5979644060134888, Val Acc: 0.8099\n","Epoch [325/500], Loss: 0.6886962652206421, Train Acc: 0.7608,train F1-score:0.7418 Val Loss: 0.5835708379745483, Val Acc: 0.8066\n","Epoch [326/500], Loss: 0.6839640140533447, Train Acc: 0.7624,train F1-score:0.7391 Val Loss: 0.5796473622322083, Val Acc: 0.8079\n","Epoch [327/500], Loss: 0.6901479959487915, Train Acc: 0.7584,train F1-score:0.7362 Val Loss: 0.5914517045021057, Val Acc: 0.8066\n","Epoch [328/500], Loss: 0.6819312572479248, Train Acc: 0.7628,train F1-score:0.7439 Val Loss: 0.5931051969528198, Val Acc: 0.8086\n","Epoch [329/500], Loss: 0.6865548491477966, Train Acc: 0.7569,train F1-score:0.7391 Val Loss: 0.5918207168579102, Val Acc: 0.8086\n","Epoch [330/500], Loss: 0.6812855005264282, Train Acc: 0.7644,train F1-score:0.7437 Val Loss: 0.5950315594673157, Val Acc: 0.8092\n","Epoch [331/500], Loss: 0.6912919282913208, Train Acc: 0.7571,train F1-score:0.7343 Val Loss: 0.6067948937416077, Val Acc: 0.8112\n","Epoch [332/500], Loss: 0.6879739165306091, Train Acc: 0.7626,train F1-score:0.7432 Val Loss: 0.615372896194458, Val Acc: 0.8139\n","Epoch [333/500], Loss: 0.6875004172325134, Train Acc: 0.7612,train F1-score:0.7431 Val Loss: 0.6071522235870361, Val Acc: 0.8179\n","Epoch [334/500], Loss: 0.673681378364563, Train Acc: 0.7648,train F1-score:0.7451 Val Loss: 0.5953576564788818, Val Acc: 0.8092\n","Epoch [335/500], Loss: 0.674656331539154, Train Acc: 0.7675,train F1-score:0.7456 Val Loss: 0.5913106203079224, Val Acc: 0.8106\n","Epoch [336/500], Loss: 0.6775196194648743, Train Acc: 0.7629,train F1-score:0.7425 Val Loss: 0.5977591872215271, Val Acc: 0.8139\n","Epoch [337/500], Loss: 0.6737677454948425, Train Acc: 0.7656,train F1-score:0.7487 Val Loss: 0.6051644086837769, Val Acc: 0.8133\n","Epoch [338/500], Loss: 0.6759121417999268, Train Acc: 0.7660,train F1-score:0.7515 Val Loss: 0.6021413803100586, Val Acc: 0.8133\n","Epoch [339/500], Loss: 0.6710836291313171, Train Acc: 0.7677,train F1-score:0.7502 Val Loss: 0.5964873433113098, Val Acc: 0.8133\n","Epoch [340/500], Loss: 0.6815556287765503, Train Acc: 0.7660,train F1-score:0.7454 Val Loss: 0.5931263566017151, Val Acc: 0.8072\n","Epoch [341/500], Loss: 0.6855533123016357, Train Acc: 0.7654,train F1-score:0.7460 Val Loss: 0.596528172492981, Val Acc: 0.8066\n","Epoch [342/500], Loss: 0.6644479632377625, Train Acc: 0.7653,train F1-score:0.7465 Val Loss: 0.5958923697471619, Val Acc: 0.8106\n","Epoch [343/500], Loss: 0.6763473749160767, Train Acc: 0.7648,train F1-score:0.7457 Val Loss: 0.5912595987319946, Val Acc: 0.8099\n","Epoch [344/500], Loss: 0.6739470958709717, Train Acc: 0.7643,train F1-score:0.7430 Val Loss: 0.5930648446083069, Val Acc: 0.8079\n","Epoch [345/500], Loss: 0.6767558455467224, Train Acc: 0.7622,train F1-score:0.7420 Val Loss: 0.5978766679763794, Val Acc: 0.8112\n","Epoch [346/500], Loss: 0.6718049049377441, Train Acc: 0.7646,train F1-score:0.7466 Val Loss: 0.5974195003509521, Val Acc: 0.8106\n","Epoch [347/500], Loss: 0.6640200614929199, Train Acc: 0.7670,train F1-score:0.7492 Val Loss: 0.5927838087081909, Val Acc: 0.8159\n","Epoch [348/500], Loss: 0.6719248294830322, Train Acc: 0.7626,train F1-score:0.7463 Val Loss: 0.5886566042900085, Val Acc: 0.8179\n","Epoch [349/500], Loss: 0.6788508892059326, Train Acc: 0.7586,train F1-score:0.7397 Val Loss: 0.5952674746513367, Val Acc: 0.8146\n","Epoch [350/500], Loss: 0.6663199663162231, Train Acc: 0.7672,train F1-score:0.7465 Val Loss: 0.6081562638282776, Val Acc: 0.8153\n","Epoch [351/500], Loss: 0.6691911816596985, Train Acc: 0.7669,train F1-score:0.7492 Val Loss: 0.6140925884246826, Val Acc: 0.8153\n","Epoch [352/500], Loss: 0.6619868278503418, Train Acc: 0.7646,train F1-score:0.7486 Val Loss: 0.6002984642982483, Val Acc: 0.8133\n","Epoch [353/500], Loss: 0.6614465713500977, Train Acc: 0.7692,train F1-score:0.7521 Val Loss: 0.5898116827011108, Val Acc: 0.8112\n","Epoch [354/500], Loss: 0.6897152066230774, Train Acc: 0.7638,train F1-score:0.7459 Val Loss: 0.5981814861297607, Val Acc: 0.8159\n","Epoch [355/500], Loss: 0.6684284806251526, Train Acc: 0.7691,train F1-score:0.7530 Val Loss: 0.6091566681861877, Val Acc: 0.8206\n","Epoch [356/500], Loss: 0.6712501645088196, Train Acc: 0.7665,train F1-score:0.7531 Val Loss: 0.6036582589149475, Val Acc: 0.8166\n","Epoch [357/500], Loss: 0.6695465445518494, Train Acc: 0.7706,train F1-score:0.7548 Val Loss: 0.5934203863143921, Val Acc: 0.8139\n","Epoch [358/500], Loss: 0.6762725710868835, Train Acc: 0.7653,train F1-score:0.7475 Val Loss: 0.5846564769744873, Val Acc: 0.8179\n","Epoch [359/500], Loss: 0.6749705672264099, Train Acc: 0.7695,train F1-score:0.7505 Val Loss: 0.587899386882782, Val Acc: 0.8226\n","Epoch [360/500], Loss: 0.6791412234306335, Train Acc: 0.7698,train F1-score:0.7524 Val Loss: 0.5915491580963135, Val Acc: 0.8220\n","Epoch [361/500], Loss: 0.6677393913269043, Train Acc: 0.7692,train F1-score:0.7529 Val Loss: 0.5882235765457153, Val Acc: 0.8240\n","Epoch [362/500], Loss: 0.6709769368171692, Train Acc: 0.7711,train F1-score:0.7558 Val Loss: 0.5821663737297058, Val Acc: 0.8226\n","Epoch [363/500], Loss: 0.6724761724472046, Train Acc: 0.7698,train F1-score:0.7532 Val Loss: 0.5817537903785706, Val Acc: 0.8226\n","Epoch [364/500], Loss: 0.6848915815353394, Train Acc: 0.7690,train F1-score:0.7526 Val Loss: 0.589859127998352, Val Acc: 0.8226\n","Epoch [365/500], Loss: 0.731656551361084, Train Acc: 0.7634,train F1-score:0.7485 Val Loss: 0.6016579866409302, Val Acc: 0.8220\n","Epoch [366/500], Loss: 0.6774986386299133, Train Acc: 0.7684,train F1-score:0.7545 Val Loss: 0.6017326712608337, Val Acc: 0.8179\n","Epoch [367/500], Loss: 0.6679903268814087, Train Acc: 0.7674,train F1-score:0.7508 Val Loss: 0.5980514883995056, Val Acc: 0.8112\n","Epoch [368/500], Loss: 0.6691886782646179, Train Acc: 0.7673,train F1-score:0.7502 Val Loss: 0.5841025114059448, Val Acc: 0.8119\n","Epoch [369/500], Loss: 0.6707603931427002, Train Acc: 0.7662,train F1-score:0.7499 Val Loss: 0.5781571865081787, Val Acc: 0.8173\n","Epoch [370/500], Loss: 0.6852661371231079, Train Acc: 0.7624,train F1-score:0.7451 Val Loss: 0.5848369002342224, Val Acc: 0.8159\n","Epoch [371/500], Loss: 0.6677893996238708, Train Acc: 0.7688,train F1-score:0.7528 Val Loss: 0.5943775773048401, Val Acc: 0.8220\n","Epoch [372/500], Loss: 0.6884228587150574, Train Acc: 0.7696,train F1-score:0.7520 Val Loss: 0.6046079397201538, Val Acc: 0.8193\n","Epoch [373/500], Loss: 0.6738126873970032, Train Acc: 0.7688,train F1-score:0.7515 Val Loss: 0.6098717451095581, Val Acc: 0.8146\n","Epoch [374/500], Loss: 0.6736031174659729, Train Acc: 0.7688,train F1-score:0.7530 Val Loss: 0.5984153151512146, Val Acc: 0.8139\n","Epoch [375/500], Loss: 0.6682211756706238, Train Acc: 0.7698,train F1-score:0.7521 Val Loss: 0.5852732062339783, Val Acc: 0.8146\n","Epoch [376/500], Loss: 0.6730908751487732, Train Acc: 0.7641,train F1-score:0.7460 Val Loss: 0.5853284001350403, Val Acc: 0.8092\n","Epoch [377/500], Loss: 0.6703506112098694, Train Acc: 0.7706,train F1-score:0.7511 Val Loss: 0.592850923538208, Val Acc: 0.8119\n","Epoch [378/500], Loss: 0.6643697023391724, Train Acc: 0.7670,train F1-score:0.7508 Val Loss: 0.5907116532325745, Val Acc: 0.8059\n","Epoch [379/500], Loss: 0.6716200709342957, Train Acc: 0.7634,train F1-score:0.7478 Val Loss: 0.5842792987823486, Val Acc: 0.8039\n","Epoch [380/500], Loss: 0.6650603413581848, Train Acc: 0.7663,train F1-score:0.7496 Val Loss: 0.5793136954307556, Val Acc: 0.8126\n","Epoch [381/500], Loss: 0.6775986552238464, Train Acc: 0.7655,train F1-score:0.7462 Val Loss: 0.5833582282066345, Val Acc: 0.8173\n","Epoch [382/500], Loss: 0.6633890271186829, Train Acc: 0.7640,train F1-score:0.7440 Val Loss: 0.5928266048431396, Val Acc: 0.8133\n","Epoch [383/500], Loss: 0.6604062914848328, Train Acc: 0.7664,train F1-score:0.7497 Val Loss: 0.5936596989631653, Val Acc: 0.8146\n","Epoch [384/500], Loss: 0.6790841817855835, Train Acc: 0.7698,train F1-score:0.7551 Val Loss: 0.5784229636192322, Val Acc: 0.8146\n","Epoch [385/500], Loss: 0.6557527184486389, Train Acc: 0.7753,train F1-score:0.7607 Val Loss: 0.5599587559700012, Val Acc: 0.8139\n","Epoch [386/500], Loss: 0.6614255309104919, Train Acc: 0.7716,train F1-score:0.7539 Val Loss: 0.557011604309082, Val Acc: 0.8153\n","Epoch [387/500], Loss: 0.6527843475341797, Train Acc: 0.7760,train F1-score:0.7592 Val Loss: 0.5657474398612976, Val Acc: 0.8179\n","Epoch [388/500], Loss: 0.6771626472473145, Train Acc: 0.7716,train F1-score:0.7578 Val Loss: 0.5791276097297668, Val Acc: 0.8179\n","Epoch [389/500], Loss: 0.6607663035392761, Train Acc: 0.7702,train F1-score:0.7575 Val Loss: 0.5774965882301331, Val Acc: 0.8179\n","Epoch [390/500], Loss: 0.655426025390625, Train Acc: 0.7732,train F1-score:0.7572 Val Loss: 0.5742585062980652, Val Acc: 0.8179\n","Epoch [391/500], Loss: 0.6510180234909058, Train Acc: 0.7756,train F1-score:0.7575 Val Loss: 0.5768049955368042, Val Acc: 0.8179\n","Epoch [392/500], Loss: 0.6560177206993103, Train Acc: 0.7692,train F1-score:0.7518 Val Loss: 0.5843921899795532, Val Acc: 0.8193\n","Epoch [393/500], Loss: 0.6573290228843689, Train Acc: 0.7693,train F1-score:0.7547 Val Loss: 0.5805818438529968, Val Acc: 0.8206\n","Epoch [394/500], Loss: 0.6631680130958557, Train Acc: 0.7781,train F1-score:0.7639 Val Loss: 0.572382926940918, Val Acc: 0.8206\n","Epoch [395/500], Loss: 0.6492693424224854, Train Acc: 0.7728,train F1-score:0.7578 Val Loss: 0.5666906237602234, Val Acc: 0.8193\n","Epoch [396/500], Loss: 0.6563087701797485, Train Acc: 0.7768,train F1-score:0.7580 Val Loss: 0.5659534931182861, Val Acc: 0.8220\n","Epoch [397/500], Loss: 0.65024334192276, Train Acc: 0.7759,train F1-score:0.7600 Val Loss: 0.5766069889068604, Val Acc: 0.8213\n","Epoch [398/500], Loss: 0.6581427454948425, Train Acc: 0.7752,train F1-score:0.7619 Val Loss: 0.5797235369682312, Val Acc: 0.8179\n","Epoch [399/500], Loss: 0.6483513712882996, Train Acc: 0.7784,train F1-score:0.7647 Val Loss: 0.5821028351783752, Val Acc: 0.8173\n","Epoch [400/500], Loss: 0.6550162434577942, Train Acc: 0.7716,train F1-score:0.7548 Val Loss: 0.5804014205932617, Val Acc: 0.8186\n","Epoch [401/500], Loss: 0.6546993255615234, Train Acc: 0.7760,train F1-score:0.7588 Val Loss: 0.5752503275871277, Val Acc: 0.8193\n","Epoch [402/500], Loss: 0.6518689393997192, Train Acc: 0.7779,train F1-score:0.7628 Val Loss: 0.5745382308959961, Val Acc: 0.8220\n","Epoch [403/500], Loss: 0.6576220393180847, Train Acc: 0.7796,train F1-score:0.7658 Val Loss: 0.5728481411933899, Val Acc: 0.8199\n","Epoch [404/500], Loss: 0.6414936184883118, Train Acc: 0.7758,train F1-score:0.7613 Val Loss: 0.5692453980445862, Val Acc: 0.8193\n","Epoch [405/500], Loss: 0.6827329993247986, Train Acc: 0.7764,train F1-score:0.7591 Val Loss: 0.5722434520721436, Val Acc: 0.8199\n","Epoch [406/500], Loss: 0.6402800679206848, Train Acc: 0.7778,train F1-score:0.7588 Val Loss: 0.5749837756156921, Val Acc: 0.8220\n","Epoch [407/500], Loss: 0.6408178210258484, Train Acc: 0.7788,train F1-score:0.7635 Val Loss: 0.5745667815208435, Val Acc: 0.8226\n","Epoch [408/500], Loss: 0.6534343361854553, Train Acc: 0.7785,train F1-score:0.7652 Val Loss: 0.5664241909980774, Val Acc: 0.8186\n","Epoch [409/500], Loss: 0.6502848863601685, Train Acc: 0.7785,train F1-score:0.7647 Val Loss: 0.5602650046348572, Val Acc: 0.8179\n","Epoch [410/500], Loss: 0.6501818299293518, Train Acc: 0.7745,train F1-score:0.7598 Val Loss: 0.5567005276679993, Val Acc: 0.8193\n","Epoch [411/500], Loss: 0.644295334815979, Train Acc: 0.7785,train F1-score:0.7625 Val Loss: 0.5586802959442139, Val Acc: 0.8186\n","Epoch [412/500], Loss: 0.657317578792572, Train Acc: 0.7728,train F1-score:0.7571 Val Loss: 0.5627807378768921, Val Acc: 0.8186\n","Epoch [413/500], Loss: 0.6380359530448914, Train Acc: 0.7827,train F1-score:0.7690 Val Loss: 0.5567719340324402, Val Acc: 0.8206\n","Epoch [414/500], Loss: 0.6408665776252747, Train Acc: 0.7804,train F1-score:0.7666 Val Loss: 0.5513820648193359, Val Acc: 0.8193\n","Epoch [415/500], Loss: 0.6479759216308594, Train Acc: 0.7747,train F1-score:0.7606 Val Loss: 0.5553043484687805, Val Acc: 0.8186\n","Epoch [416/500], Loss: 0.6360035538673401, Train Acc: 0.7807,train F1-score:0.7656 Val Loss: 0.5569275617599487, Val Acc: 0.8220\n","Epoch [417/500], Loss: 0.6392399072647095, Train Acc: 0.7788,train F1-score:0.7644 Val Loss: 0.5580741167068481, Val Acc: 0.8220\n","Epoch [418/500], Loss: 0.6399903297424316, Train Acc: 0.7808,train F1-score:0.7671 Val Loss: 0.558967649936676, Val Acc: 0.8226\n","Epoch [419/500], Loss: 0.6405026912689209, Train Acc: 0.7783,train F1-score:0.7634 Val Loss: 0.5553271174430847, Val Acc: 0.8213\n","Epoch [420/500], Loss: 0.6423048973083496, Train Acc: 0.7727,train F1-score:0.7557 Val Loss: 0.5575459003448486, Val Acc: 0.8206\n","Epoch [421/500], Loss: 0.6319966316223145, Train Acc: 0.7791,train F1-score:0.7625 Val Loss: 0.5580947995185852, Val Acc: 0.8220\n","Epoch [422/500], Loss: 0.6316509246826172, Train Acc: 0.7822,train F1-score:0.7665 Val Loss: 0.5595546364784241, Val Acc: 0.8226\n","Epoch [423/500], Loss: 0.6394965648651123, Train Acc: 0.7824,train F1-score:0.7684 Val Loss: 0.56421959400177, Val Acc: 0.8226\n","Epoch [424/500], Loss: 0.6405516266822815, Train Acc: 0.7794,train F1-score:0.7665 Val Loss: 0.5636971592903137, Val Acc: 0.8407\n","Epoch [425/500], Loss: 0.639980137348175, Train Acc: 0.7764,train F1-score:0.7638 Val Loss: 0.5579010248184204, Val Acc: 0.8186\n","Epoch [426/500], Loss: 0.6388295292854309, Train Acc: 0.7814,train F1-score:0.7657 Val Loss: 0.5512627363204956, Val Acc: 0.8193\n","Epoch [427/500], Loss: 0.6414634585380554, Train Acc: 0.7796,train F1-score:0.7647 Val Loss: 0.5498380064964294, Val Acc: 0.8220\n","Epoch [428/500], Loss: 0.6387695670127869, Train Acc: 0.7816,train F1-score:0.7674 Val Loss: 0.5531996488571167, Val Acc: 0.8240\n","Epoch [429/500], Loss: 0.6494244933128357, Train Acc: 0.7783,train F1-score:0.7648 Val Loss: 0.5584180355072021, Val Acc: 0.8220\n","Epoch [430/500], Loss: 0.6416354775428772, Train Acc: 0.7772,train F1-score:0.7631 Val Loss: 0.5612902641296387, Val Acc: 0.8159\n","Epoch [431/500], Loss: 0.636896550655365, Train Acc: 0.7814,train F1-score:0.7675 Val Loss: 0.5579168796539307, Val Acc: 0.8173\n","Epoch [432/500], Loss: 0.6449029445648193, Train Acc: 0.7740,train F1-score:0.7592 Val Loss: 0.5517345070838928, Val Acc: 0.8173\n","Epoch [433/500], Loss: 0.6346842050552368, Train Acc: 0.7820,train F1-score:0.7683 Val Loss: 0.5478937029838562, Val Acc: 0.8193\n","Epoch [434/500], Loss: 0.6376314163208008, Train Acc: 0.7822,train F1-score:0.7701 Val Loss: 0.5523531436920166, Val Acc: 0.8166\n","Epoch [435/500], Loss: 0.6345245242118835, Train Acc: 0.7833,train F1-score:0.7703 Val Loss: 0.5598055720329285, Val Acc: 0.8186\n","Epoch [436/500], Loss: 0.6377540826797485, Train Acc: 0.7797,train F1-score:0.7653 Val Loss: 0.5634660720825195, Val Acc: 0.8186\n","Epoch [437/500], Loss: 0.6357632875442505, Train Acc: 0.7814,train F1-score:0.7661 Val Loss: 0.5613719820976257, Val Acc: 0.8226\n","Epoch [438/500], Loss: 0.6391410231590271, Train Acc: 0.7822,train F1-score:0.7662 Val Loss: 0.5568165183067322, Val Acc: 0.8246\n","Epoch [439/500], Loss: 0.6347289085388184, Train Acc: 0.7820,train F1-score:0.7659 Val Loss: 0.5543770790100098, Val Acc: 0.8233\n","Epoch [440/500], Loss: 0.6367148160934448, Train Acc: 0.7808,train F1-score:0.7668 Val Loss: 0.5529553294181824, Val Acc: 0.8220\n","Epoch [441/500], Loss: 0.6420160531997681, Train Acc: 0.7797,train F1-score:0.7673 Val Loss: 0.5521775484085083, Val Acc: 0.8233\n","Epoch [442/500], Loss: 0.6395280361175537, Train Acc: 0.7765,train F1-score:0.7636 Val Loss: 0.5525492429733276, Val Acc: 0.8220\n","Epoch [443/500], Loss: 0.646126389503479, Train Acc: 0.7768,train F1-score:0.7635 Val Loss: 0.5520976185798645, Val Acc: 0.8213\n","Epoch [444/500], Loss: 0.6417326331138611, Train Acc: 0.7783,train F1-score:0.7647 Val Loss: 0.5539253354072571, Val Acc: 0.8233\n","Epoch [445/500], Loss: 0.6542952656745911, Train Acc: 0.7743,train F1-score:0.7618 Val Loss: 0.5591719746589661, Val Acc: 0.8220\n","Epoch [446/500], Loss: 0.6415124535560608, Train Acc: 0.7729,train F1-score:0.7586 Val Loss: 0.5622254014015198, Val Acc: 0.8166\n","Epoch [447/500], Loss: 0.6399766802787781, Train Acc: 0.7735,train F1-score:0.7558 Val Loss: 0.5692492127418518, Val Acc: 0.8166\n","Epoch [448/500], Loss: 0.6289504766464233, Train Acc: 0.7771,train F1-score:0.7605 Val Loss: 0.5689536929130554, Val Acc: 0.8173\n","Epoch [449/500], Loss: 0.6451728940010071, Train Acc: 0.7788,train F1-score:0.7638 Val Loss: 0.5682381391525269, Val Acc: 0.8146\n","Epoch [450/500], Loss: 0.6306220889091492, Train Acc: 0.7814,train F1-score:0.7684 Val Loss: 0.5614873170852661, Val Acc: 0.8179\n","Epoch [451/500], Loss: 0.6348563432693481, Train Acc: 0.7763,train F1-score:0.7619 Val Loss: 0.5578415393829346, Val Acc: 0.8159\n","Epoch [452/500], Loss: 0.63599693775177, Train Acc: 0.7763,train F1-score:0.7619 Val Loss: 0.5595285296440125, Val Acc: 0.8166\n","Epoch [453/500], Loss: 0.6309564709663391, Train Acc: 0.7809,train F1-score:0.7664 Val Loss: 0.5627800226211548, Val Acc: 0.8166\n","Epoch [454/500], Loss: 0.637446939945221, Train Acc: 0.7786,train F1-score:0.7647 Val Loss: 0.5684185028076172, Val Acc: 0.8166\n","Epoch [455/500], Loss: 0.6421944499015808, Train Acc: 0.7766,train F1-score:0.7647 Val Loss: 0.5641247630119324, Val Acc: 0.8126\n","Epoch [456/500], Loss: 0.6375392079353333, Train Acc: 0.7809,train F1-score:0.7690 Val Loss: 0.5565797090530396, Val Acc: 0.8146\n","Epoch [457/500], Loss: 0.6380048990249634, Train Acc: 0.7803,train F1-score:0.7652 Val Loss: 0.5556468367576599, Val Acc: 0.8153\n","Epoch [458/500], Loss: 0.6373072266578674, Train Acc: 0.7785,train F1-score:0.7640 Val Loss: 0.55723637342453, Val Acc: 0.8206\n","Epoch [459/500], Loss: 0.6243630647659302, Train Acc: 0.7850,train F1-score:0.7722 Val Loss: 0.5556755661964417, Val Acc: 0.8166\n","Epoch [460/500], Loss: 0.6308674216270447, Train Acc: 0.7855,train F1-score:0.7738 Val Loss: 0.5494242906570435, Val Acc: 0.8226\n","Epoch [461/500], Loss: 0.6294606328010559, Train Acc: 0.7824,train F1-score:0.7689 Val Loss: 0.5444987416267395, Val Acc: 0.8226\n","Epoch [462/500], Loss: 0.6313072443008423, Train Acc: 0.7814,train F1-score:0.7666 Val Loss: 0.5444723963737488, Val Acc: 0.8233\n","Epoch [463/500], Loss: 0.630978524684906, Train Acc: 0.7779,train F1-score:0.7649 Val Loss: 0.5505744814872742, Val Acc: 0.8233\n","Epoch [464/500], Loss: 0.6419099569320679, Train Acc: 0.7804,train F1-score:0.7667 Val Loss: 0.55963534116745, Val Acc: 0.8253\n","Epoch [465/500], Loss: 0.6363016963005066, Train Acc: 0.7786,train F1-score:0.7650 Val Loss: 0.5676593780517578, Val Acc: 0.8206\n","Epoch [466/500], Loss: 0.6395938396453857, Train Acc: 0.7787,train F1-score:0.7637 Val Loss: 0.5680614113807678, Val Acc: 0.8233\n","Epoch [467/500], Loss: 0.6350930333137512, Train Acc: 0.7819,train F1-score:0.7663 Val Loss: 0.5590975284576416, Val Acc: 0.8253\n","Epoch [468/500], Loss: 0.6251986026763916, Train Acc: 0.7854,train F1-score:0.7718 Val Loss: 0.5484300255775452, Val Acc: 0.8253\n","Epoch [469/500], Loss: 0.631643533706665, Train Acc: 0.7836,train F1-score:0.7695 Val Loss: 0.5435941219329834, Val Acc: 0.8266\n","Epoch [470/500], Loss: 0.6312651634216309, Train Acc: 0.7854,train F1-score:0.7725 Val Loss: 0.5455067157745361, Val Acc: 0.8273\n","Epoch [471/500], Loss: 0.6215130686759949, Train Acc: 0.7860,train F1-score:0.7726 Val Loss: 0.5441767573356628, Val Acc: 0.8246\n","Epoch [472/500], Loss: 0.6170343160629272, Train Acc: 0.7875,train F1-score:0.7744 Val Loss: 0.5366414189338684, Val Acc: 0.8253\n","Epoch [473/500], Loss: 0.6197254657745361, Train Acc: 0.7899,train F1-score:0.7772 Val Loss: 0.5326988697052002, Val Acc: 0.8240\n","Epoch [474/500], Loss: 0.6272782683372498, Train Acc: 0.7867,train F1-score:0.7738 Val Loss: 0.5337796211242676, Val Acc: 0.8246\n","Epoch [475/500], Loss: 0.6211044192314148, Train Acc: 0.7885,train F1-score:0.7751 Val Loss: 0.5389797687530518, Val Acc: 0.8246\n","Epoch [476/500], Loss: 0.6218870282173157, Train Acc: 0.7889,train F1-score:0.7776 Val Loss: 0.5373740196228027, Val Acc: 0.8266\n","Epoch [477/500], Loss: 0.6275806427001953, Train Acc: 0.7889,train F1-score:0.7771 Val Loss: 0.5375974774360657, Val Acc: 0.8300\n","Epoch [478/500], Loss: 0.619732677936554, Train Acc: 0.7873,train F1-score:0.7747 Val Loss: 0.5374434590339661, Val Acc: 0.8307\n","Epoch [479/500], Loss: 0.6748864054679871, Train Acc: 0.7831,train F1-score:0.7710 Val Loss: 0.5402281880378723, Val Acc: 0.8280\n","Epoch [480/500], Loss: 0.6262285709381104, Train Acc: 0.7862,train F1-score:0.7724 Val Loss: 0.5430018901824951, Val Acc: 0.8246\n","Epoch [481/500], Loss: 0.6307905912399292, Train Acc: 0.7833,train F1-score:0.7708 Val Loss: 0.5445722937583923, Val Acc: 0.8220\n","Epoch [482/500], Loss: 0.6322023868560791, Train Acc: 0.7827,train F1-score:0.7692 Val Loss: 0.5413045883178711, Val Acc: 0.8233\n","Epoch [483/500], Loss: 0.6256845593452454, Train Acc: 0.7860,train F1-score:0.7741 Val Loss: 0.5380949378013611, Val Acc: 0.8240\n","Epoch [484/500], Loss: 0.6301108598709106, Train Acc: 0.7874,train F1-score:0.7756 Val Loss: 0.5374187231063843, Val Acc: 0.8206\n","Epoch [485/500], Loss: 0.6232143640518188, Train Acc: 0.7875,train F1-score:0.7738 Val Loss: 0.5395967960357666, Val Acc: 0.8220\n","Epoch [486/500], Loss: 0.6291255354881287, Train Acc: 0.7857,train F1-score:0.7729 Val Loss: 0.5415103435516357, Val Acc: 0.8220\n","Epoch [487/500], Loss: 0.6215851306915283, Train Acc: 0.7850,train F1-score:0.7722 Val Loss: 0.5426604747772217, Val Acc: 0.8213\n","Epoch [488/500], Loss: 0.6253035664558411, Train Acc: 0.7864,train F1-score:0.7723 Val Loss: 0.5463351011276245, Val Acc: 0.8213\n","Epoch [489/500], Loss: 0.6316758394241333, Train Acc: 0.7855,train F1-score:0.7712 Val Loss: 0.55169278383255, Val Acc: 0.8199\n","Epoch [490/500], Loss: 0.6235783100128174, Train Acc: 0.7826,train F1-score:0.7676 Val Loss: 0.5516621470451355, Val Acc: 0.8179\n","Epoch [491/500], Loss: 0.6234830617904663, Train Acc: 0.7826,train F1-score:0.7691 Val Loss: 0.5485398769378662, Val Acc: 0.8206\n","Epoch [492/500], Loss: 0.617049515247345, Train Acc: 0.7912,train F1-score:0.7776 Val Loss: 0.544894278049469, Val Acc: 0.8220\n","Epoch [493/500], Loss: 0.6193265318870544, Train Acc: 0.7907,train F1-score:0.7787 Val Loss: 0.5402734279632568, Val Acc: 0.8260\n","Epoch [494/500], Loss: 0.6201533675193787, Train Acc: 0.7863,train F1-score:0.7735 Val Loss: 0.537596583366394, Val Acc: 0.8240\n","Epoch [495/500], Loss: 0.6288838386535645, Train Acc: 0.7857,train F1-score:0.7716 Val Loss: 0.5405251979827881, Val Acc: 0.8226\n","Epoch [496/500], Loss: 0.6241405606269836, Train Acc: 0.7919,train F1-score:0.7788 Val Loss: 0.5491559505462646, Val Acc: 0.8193\n","Epoch [497/500], Loss: 0.6098843216896057, Train Acc: 0.7917,train F1-score:0.7788 Val Loss: 0.5480454564094543, Val Acc: 0.8186\n","Epoch [498/500], Loss: 0.6100198030471802, Train Acc: 0.7925,train F1-score:0.7806 Val Loss: 0.5392729640007019, Val Acc: 0.8226\n","Epoch [499/500], Loss: 0.6305071711540222, Train Acc: 0.7886,train F1-score:0.7754 Val Loss: 0.5350678563117981, Val Acc: 0.8240\n","Epoch [500/500], Loss: 0.6254281401634216, Train Acc: 0.7880,train F1-score:0.7744 Val Loss: 0.5392343997955322, Val Acc: 0.8199\n","Test Loss: 0.5448126792907715, Test Accuracy: 0.8063002680965148\n","Precision: 0.7548, Recall: 0.8063, F1-score: 0.7688\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["F1-score saved to file.\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GATNet(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_dim, out_channels, num_layers):\n","        super(GATNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GATConv(in_channels, hidden_dim, heads=8))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GATConv(hidden_dim * 8, hidden_dim, heads=8))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim * 8, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.fc(x)\n","        return x\n","\n","# Define model\n","model = GATNet(in_channels=X_train.shape[1], hidden_dim=128, out_channels=13, num_layers=4).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model4.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer4/train_f1_scores_GAT.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer4/train_loss_GAT.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer4/epochs_GAT.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model4.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/layer4/f1_score_GAT.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1Dwd8x0J5hb_O71aN8u1E8fYv3ejvHLLj","timestamp":1714732767381},{"file_id":"1G7cH2KvNhFQ-zXs0dyaL7TR2m9zbI8hr","timestamp":1712733709431}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}