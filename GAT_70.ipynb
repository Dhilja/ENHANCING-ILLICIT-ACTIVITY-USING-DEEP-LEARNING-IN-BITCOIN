{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G7cH2KvNhFQ-zXs0dyaL7TR2m9zbI8hr","timestamp":1712729723829}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import sklearn\n","\n","print(f\"scikit-learn version installed in Colab: {sklearn.__version__}\")\n"],"metadata":{"id":"7dwH7gXuGNo0","executionInfo":{"status":"ok","timestamp":1714549239851,"user_tz":-330,"elapsed":1529,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"ef8ea77b-ec79-4639-be9d-cf15009d87c9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["scikit-learn version installed in Colab: 1.2.2\n"]}]},{"cell_type":"code","source":["import networkx as nx\n","\n","print(f\"NetworkX version installed in Colab: {nx.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kw2AysilGwoU","executionInfo":{"status":"ok","timestamp":1714549367063,"user_tz":-330,"elapsed":1553,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"7dbfda25-a5c1-4e5e-8603-e4c1705fd9cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NetworkX version installed in Colab: 3.3\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlrWhbWEYmIN","executionInfo":{"status":"ok","timestamp":1714802320700,"user_tz":-330,"elapsed":3604,"user":{"displayName":"Goutham Krishna","userId":"05855847327907102213"}},"outputId":"0ce284ad-a6ca-4adc-fedc-8b7f711bda49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":[],"metadata":{"id":"pKaLgK5oGNA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch-geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEuPfhoQZgJ2","executionInfo":{"status":"ok","timestamp":1714802329283,"user_tz":-330,"elapsed":8585,"user":{"displayName":"Goutham Krishna","userId":"05855847327907102213"}},"outputId":"5a8974a7-f1f2-40ec-f87c-7f2d03b7db69"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT_70/edge_index_train.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT_70/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT_70/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT_70/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT_70/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT_70/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT_70/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT_70/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT_70/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GATNet(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_dim, out_channels, num_layers):\n","        super(GATNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GATConv(in_channels, hidden_dim, heads=8))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GATConv(hidden_dim * 8, hidden_dim, heads=8))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim * 8, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.fc(x)\n","        return x\n","\n","# Define model\n","model = GATNet(in_channels=X_train.shape[1], hidden_dim=64, out_channels=13, num_layers=2).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model1.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBLlJQNsZuY7","executionInfo":{"status":"ok","timestamp":1714805386838,"user_tz":-330,"elapsed":8688,"user":{"displayName":"Goutham Krishna","userId":"05855847327907102213"}},"outputId":"baf5db27-ddbb-474d-a3ae-cf4827898444"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Epoch [1/500], Loss: 3.2862422466278076, Train Acc: 0.0508,train F1-score:0.0407 Val Loss: 2.0105843544006348, Val Acc: 0.6531\n","Epoch [2/500], Loss: 2.0662670135498047, Train Acc: 0.6476,train F1-score:0.5104 Val Loss: 1.816688895225525, Val Acc: 0.3951\n","Epoch [3/500], Loss: 1.9773510694503784, Train Acc: 0.4124,train F1-score:0.4183 Val Loss: 1.1941050291061401, Val Acc: 0.6732\n","Epoch [4/500], Loss: 1.3329651355743408, Train Acc: 0.6424,train F1-score:0.5660 Val Loss: 1.2084616422653198, Val Acc: 0.6531\n","Epoch [5/500], Loss: 1.2600994110107422, Train Acc: 0.6521,train F1-score:0.5232 Val Loss: 1.1592413187026978, Val Acc: 0.6772\n","Epoch [6/500], Loss: 1.265726923942566, Train Acc: 0.6670,train F1-score:0.5555 Val Loss: 1.0671194791793823, Val Acc: 0.6978\n","Epoch [7/500], Loss: 1.1017876863479614, Train Acc: 0.6770,train F1-score:0.5823 Val Loss: 1.0313329696655273, Val Acc: 0.7036\n","Epoch [8/500], Loss: 1.0440677404403687, Train Acc: 0.6788,train F1-score:0.5991 Val Loss: 0.9595162272453308, Val Acc: 0.6808\n","Epoch [9/500], Loss: 0.9645971059799194, Train Acc: 0.6773,train F1-score:0.5919 Val Loss: 0.917173445224762, Val Acc: 0.6857\n","Epoch [10/500], Loss: 0.9203161597251892, Train Acc: 0.6813,train F1-score:0.5923 Val Loss: 0.898838460445404, Val Acc: 0.7134\n","Epoch [11/500], Loss: 0.8995507955551147, Train Acc: 0.6988,train F1-score:0.6309 Val Loss: 0.8557477593421936, Val Acc: 0.7219\n","Epoch [12/500], Loss: 0.857175886631012, Train Acc: 0.7199,train F1-score:0.6741 Val Loss: 0.8367065787315369, Val Acc: 0.7004\n","Epoch [13/500], Loss: 0.8335647583007812, Train Acc: 0.7071,train F1-score:0.6697 Val Loss: 0.8083112239837646, Val Acc: 0.7013\n","Epoch [14/500], Loss: 0.8201077580451965, Train Acc: 0.7100,train F1-score:0.6677 Val Loss: 0.7772130370140076, Val Acc: 0.7304\n","Epoch [15/500], Loss: 0.7882190942764282, Train Acc: 0.7283,train F1-score:0.6788 Val Loss: 0.7795159816741943, Val Acc: 0.7415\n","Epoch [16/500], Loss: 0.8265790939331055, Train Acc: 0.7298,train F1-score:0.6799 Val Loss: 0.7768022418022156, Val Acc: 0.7522\n","Epoch [17/500], Loss: 0.7745392918586731, Train Acc: 0.7360,train F1-score:0.6901 Val Loss: 0.7642336487770081, Val Acc: 0.7598\n","Epoch [18/500], Loss: 0.7409276366233826, Train Acc: 0.7460,train F1-score:0.7104 Val Loss: 0.7624428868293762, Val Acc: 0.7571\n","Epoch [19/500], Loss: 0.7208863496780396, Train Acc: 0.7538,train F1-score:0.7256 Val Loss: 0.7700489163398743, Val Acc: 0.7598\n","Epoch [20/500], Loss: 0.7082737684249878, Train Acc: 0.7551,train F1-score:0.7269 Val Loss: 0.7620859742164612, Val Acc: 0.7621\n","Epoch [21/500], Loss: 0.7023399472236633, Train Acc: 0.7561,train F1-score:0.7237 Val Loss: 0.7546139359474182, Val Acc: 0.7647\n","Epoch [22/500], Loss: 1.1521844863891602, Train Acc: 0.7648,train F1-score:0.7325 Val Loss: 0.7467209696769714, Val Acc: 0.7705\n","Epoch [23/500], Loss: 0.9371009469032288, Train Acc: 0.7695,train F1-score:0.7373 Val Loss: 0.7488095164299011, Val Acc: 0.7576\n","Epoch [24/500], Loss: 0.8476921319961548, Train Acc: 0.7678,train F1-score:0.7403 Val Loss: 0.756888210773468, Val Acc: 0.7554\n","Epoch [25/500], Loss: 0.799159824848175, Train Acc: 0.7713,train F1-score:0.7494 Val Loss: 0.7499456405639648, Val Acc: 0.7558\n","Epoch [26/500], Loss: 0.7486699223518372, Train Acc: 0.7826,train F1-score:0.7661 Val Loss: 0.7440378069877625, Val Acc: 0.7638\n","Epoch [27/500], Loss: 0.7036293148994446, Train Acc: 0.7868,train F1-score:0.7721 Val Loss: 0.7193819284439087, Val Acc: 0.7661\n","Epoch [28/500], Loss: 0.6790308952331543, Train Acc: 0.7843,train F1-score:0.7670 Val Loss: 0.7136101126670837, Val Acc: 0.7705\n","Epoch [29/500], Loss: 0.6519705653190613, Train Acc: 0.7866,train F1-score:0.7690 Val Loss: 0.7188712954521179, Val Acc: 0.7714\n","Epoch [30/500], Loss: 0.644204318523407, Train Acc: 0.7835,train F1-score:0.7644 Val Loss: 0.7118996381759644, Val Acc: 0.7763\n","Epoch [31/500], Loss: 0.6267755627632141, Train Acc: 0.7898,train F1-score:0.7713 Val Loss: 0.7053869366645813, Val Acc: 0.7714\n","Epoch [32/500], Loss: 0.6027466654777527, Train Acc: 0.7926,train F1-score:0.7754 Val Loss: 0.7110975384712219, Val Acc: 0.7759\n","Epoch [33/500], Loss: 0.5937607288360596, Train Acc: 0.7990,train F1-score:0.7844 Val Loss: 0.715776801109314, Val Acc: 0.7821\n","Epoch [34/500], Loss: 0.5827344655990601, Train Acc: 0.8049,train F1-score:0.7935 Val Loss: 0.7165666818618774, Val Acc: 0.7866\n","Epoch [35/500], Loss: 0.5693809986114502, Train Acc: 0.8048,train F1-score:0.7945 Val Loss: 0.7182111740112305, Val Acc: 0.7826\n","Epoch [36/500], Loss: 0.5614941716194153, Train Acc: 0.8008,train F1-score:0.7880 Val Loss: 0.7209885120391846, Val Acc: 0.7835\n","Epoch [37/500], Loss: 0.5550413131713867, Train Acc: 0.8068,train F1-score:0.7921 Val Loss: 0.7218694686889648, Val Acc: 0.7839\n","Epoch [38/500], Loss: 0.5481438636779785, Train Acc: 0.8100,train F1-score:0.7944 Val Loss: 0.7216411232948303, Val Acc: 0.7875\n","Epoch [39/500], Loss: 0.547103226184845, Train Acc: 0.8066,train F1-score:0.7937 Val Loss: 0.7174756526947021, Val Acc: 0.7875\n","Epoch [40/500], Loss: 0.5355754494667053, Train Acc: 0.8139,train F1-score:0.8038 Val Loss: 0.7018197178840637, Val Acc: 0.7888\n","Epoch [41/500], Loss: 0.5196269750595093, Train Acc: 0.8216,train F1-score:0.8117 Val Loss: 0.6911102533340454, Val Acc: 0.7902\n","Epoch [42/500], Loss: 0.5225638151168823, Train Acc: 0.8180,train F1-score:0.8058 Val Loss: 0.6891849040985107, Val Acc: 0.7915\n","Epoch [43/500], Loss: 0.5323364734649658, Train Acc: 0.8185,train F1-score:0.8050 Val Loss: 0.6905927658081055, Val Acc: 0.7920\n","Epoch [44/500], Loss: 0.514685332775116, Train Acc: 0.8215,train F1-score:0.8088 Val Loss: 0.6957536935806274, Val Acc: 0.7937\n","Epoch [45/500], Loss: 0.5036318302154541, Train Acc: 0.8274,train F1-score:0.8168 Val Loss: 0.7016079425811768, Val Acc: 0.7960\n","Epoch [46/500], Loss: 0.5025725364685059, Train Acc: 0.8296,train F1-score:0.8220 Val Loss: 0.6984250545501709, Val Acc: 0.8009\n","Epoch [47/500], Loss: 0.4940530061721802, Train Acc: 0.8303,train F1-score:0.8227 Val Loss: 0.6913435459136963, Val Acc: 0.8000\n","Epoch [48/500], Loss: 0.4831286668777466, Train Acc: 0.8301,train F1-score:0.8212 Val Loss: 0.6868123412132263, Val Acc: 0.8000\n","Epoch [49/500], Loss: 0.4932442009449005, Train Acc: 0.8284,train F1-score:0.8185 Val Loss: 0.6871243715286255, Val Acc: 0.8018\n","Epoch [50/500], Loss: 0.4818219542503357, Train Acc: 0.8323,train F1-score:0.8228 Val Loss: 0.6842057108879089, Val Acc: 0.8036\n","Epoch [51/500], Loss: 0.4787309169769287, Train Acc: 0.8337,train F1-score:0.8241 Val Loss: 0.686475932598114, Val Acc: 0.8107\n","Epoch [52/500], Loss: 0.46940791606903076, Train Acc: 0.8375,train F1-score:0.8292 Val Loss: 0.6862354874610901, Val Acc: 0.8103\n","Epoch [53/500], Loss: 0.460121214389801, Train Acc: 0.8384,train F1-score:0.8298 Val Loss: 0.6816994547843933, Val Acc: 0.8107\n","Epoch [54/500], Loss: 0.46313801407814026, Train Acc: 0.8396,train F1-score:0.8306 Val Loss: 0.6724005937576294, Val Acc: 0.8116\n","Epoch [55/500], Loss: 0.48042944073677063, Train Acc: 0.8400,train F1-score:0.8309 Val Loss: 0.6657601594924927, Val Acc: 0.8121\n","Epoch [56/500], Loss: 0.4547942876815796, Train Acc: 0.8418,train F1-score:0.8333 Val Loss: 0.6413154006004333, Val Acc: 0.8147\n","Epoch [57/500], Loss: 0.45812252163887024, Train Acc: 0.8422,train F1-score:0.8346 Val Loss: 0.6379392147064209, Val Acc: 0.8129\n","Epoch [58/500], Loss: 0.4452739953994751, Train Acc: 0.8473,train F1-score:0.8401 Val Loss: 0.632774829864502, Val Acc: 0.8138\n","Epoch [59/500], Loss: 0.44660210609436035, Train Acc: 0.8460,train F1-score:0.8388 Val Loss: 0.6273656487464905, Val Acc: 0.8152\n","Epoch [60/500], Loss: 0.43849843740463257, Train Acc: 0.8481,train F1-score:0.8406 Val Loss: 0.6245466470718384, Val Acc: 0.8187\n","Epoch [61/500], Loss: 0.4436470866203308, Train Acc: 0.8464,train F1-score:0.8383 Val Loss: 0.6235865950584412, Val Acc: 0.8210\n","Epoch [62/500], Loss: 0.4319399893283844, Train Acc: 0.8508,train F1-score:0.8441 Val Loss: 0.6239048838615417, Val Acc: 0.8201\n","Epoch [63/500], Loss: 0.42745959758758545, Train Acc: 0.8486,train F1-score:0.8414 Val Loss: 0.6216170191764832, Val Acc: 0.8228\n","Epoch [64/500], Loss: 0.4312048554420471, Train Acc: 0.8497,train F1-score:0.8420 Val Loss: 0.6135615110397339, Val Acc: 0.8254\n","Epoch [65/500], Loss: 0.4220588207244873, Train Acc: 0.8548,train F1-score:0.8477 Val Loss: 0.6090378761291504, Val Acc: 0.8250\n","Epoch [66/500], Loss: 0.42047828435897827, Train Acc: 0.8525,train F1-score:0.8449 Val Loss: 0.60677570104599, Val Acc: 0.8246\n","Epoch [67/500], Loss: 0.4205591380596161, Train Acc: 0.8520,train F1-score:0.8441 Val Loss: 0.6052687764167786, Val Acc: 0.8259\n","Epoch [68/500], Loss: 0.41605833172798157, Train Acc: 0.8583,train F1-score:0.8512 Val Loss: 0.6048868894577026, Val Acc: 0.8272\n","Epoch [69/500], Loss: 0.4124865233898163, Train Acc: 0.8589,train F1-score:0.8522 Val Loss: 0.5987914204597473, Val Acc: 0.8263\n","Epoch [70/500], Loss: 0.4085032343864441, Train Acc: 0.8588,train F1-score:0.8523 Val Loss: 0.5903283953666687, Val Acc: 0.8268\n","Epoch [71/500], Loss: 0.40669533610343933, Train Acc: 0.8613,train F1-score:0.8553 Val Loss: 0.5815620422363281, Val Acc: 0.8272\n","Epoch [72/500], Loss: 0.41947686672210693, Train Acc: 0.8594,train F1-score:0.8532 Val Loss: 0.5711809992790222, Val Acc: 0.8272\n","Epoch [73/500], Loss: 0.3993532657623291, Train Acc: 0.8632,train F1-score:0.8566 Val Loss: 0.5687999725341797, Val Acc: 0.8313\n","Epoch [74/500], Loss: 0.39275550842285156, Train Acc: 0.8635,train F1-score:0.8575 Val Loss: 0.5730276703834534, Val Acc: 0.8330\n","Epoch [75/500], Loss: 0.45021966099739075, Train Acc: 0.8640,train F1-score:0.8581 Val Loss: 0.5724989175796509, Val Acc: 0.8299\n","Epoch [76/500], Loss: 0.38884371519088745, Train Acc: 0.8669,train F1-score:0.8616 Val Loss: 0.5764907002449036, Val Acc: 0.8286\n","Epoch [77/500], Loss: 0.39143383502960205, Train Acc: 0.8650,train F1-score:0.8592 Val Loss: 0.5783368349075317, Val Acc: 0.8313\n","Epoch [78/500], Loss: 0.3873361647129059, Train Acc: 0.8675,train F1-score:0.8613 Val Loss: 0.5796668529510498, Val Acc: 0.8335\n","Epoch [79/500], Loss: 0.38321778178215027, Train Acc: 0.8677,train F1-score:0.8619 Val Loss: 0.5790582895278931, Val Acc: 0.8330\n","Epoch [80/500], Loss: 0.3794795274734497, Train Acc: 0.8671,train F1-score:0.8615 Val Loss: 0.578274667263031, Val Acc: 0.8344\n","Epoch [81/500], Loss: 0.38664087653160095, Train Acc: 0.8680,train F1-score:0.8619 Val Loss: 0.6303606629371643, Val Acc: 0.8326\n","Epoch [82/500], Loss: 0.3723965585231781, Train Acc: 0.8757,train F1-score:0.8696 Val Loss: 0.6223305463790894, Val Acc: 0.8326\n","Epoch [83/500], Loss: 0.37296587228775024, Train Acc: 0.8711,train F1-score:0.8649 Val Loss: 0.5666927099227905, Val Acc: 0.8366\n","Epoch [84/500], Loss: 0.3679020404815674, Train Acc: 0.8756,train F1-score:0.8702 Val Loss: 0.5626022815704346, Val Acc: 0.8375\n","Epoch [85/500], Loss: 0.37393099069595337, Train Acc: 0.8754,train F1-score:0.8697 Val Loss: 0.5553310513496399, Val Acc: 0.8375\n","Epoch [86/500], Loss: 0.36496683955192566, Train Acc: 0.8727,train F1-score:0.8665 Val Loss: 0.5499562621116638, Val Acc: 0.8384\n","Epoch [87/500], Loss: 0.35993507504463196, Train Acc: 0.8751,train F1-score:0.8689 Val Loss: 0.5461292266845703, Val Acc: 0.8406\n","Epoch [88/500], Loss: 0.3622826337814331, Train Acc: 0.8733,train F1-score:0.8676 Val Loss: 0.5417798161506653, Val Acc: 0.8397\n","Epoch [89/500], Loss: 0.3638818562030792, Train Acc: 0.8756,train F1-score:0.8705 Val Loss: 0.5352696180343628, Val Acc: 0.8375\n","Epoch [90/500], Loss: 0.36271968483924866, Train Acc: 0.8790,train F1-score:0.8738 Val Loss: 0.5289207100868225, Val Acc: 0.8375\n","Epoch [91/500], Loss: 0.3560428321361542, Train Acc: 0.8787,train F1-score:0.8731 Val Loss: 0.5271828174591064, Val Acc: 0.8384\n","Epoch [92/500], Loss: 0.3536391258239746, Train Acc: 0.8780,train F1-score:0.8719 Val Loss: 0.5265290141105652, Val Acc: 0.8375\n","Epoch [93/500], Loss: 0.3524473309516907, Train Acc: 0.8748,train F1-score:0.8690 Val Loss: 0.5486016273498535, Val Acc: 0.8344\n","Epoch [94/500], Loss: 0.3525015413761139, Train Acc: 0.8805,train F1-score:0.8754 Val Loss: 0.5665580630302429, Val Acc: 0.8362\n","Epoch [95/500], Loss: 0.34781888127326965, Train Acc: 0.8809,train F1-score:0.8759 Val Loss: 0.563105583190918, Val Acc: 0.8411\n","Epoch [96/500], Loss: 0.34716057777404785, Train Acc: 0.8803,train F1-score:0.8750 Val Loss: 0.5597008466720581, Val Acc: 0.8406\n","Epoch [97/500], Loss: 0.34615814685821533, Train Acc: 0.8789,train F1-score:0.8729 Val Loss: 0.5251204967498779, Val Acc: 0.8406\n","Epoch [98/500], Loss: 0.3482372760772705, Train Acc: 0.8797,train F1-score:0.8736 Val Loss: 0.5213761329650879, Val Acc: 0.8388\n","Epoch [99/500], Loss: 0.3419104516506195, Train Acc: 0.8835,train F1-score:0.8781 Val Loss: 0.5173055529594421, Val Acc: 0.8366\n","Epoch [100/500], Loss: 0.3477758467197418, Train Acc: 0.8814,train F1-score:0.8763 Val Loss: 0.5128138661384583, Val Acc: 0.8397\n","Epoch [101/500], Loss: 0.34478890895843506, Train Acc: 0.8798,train F1-score:0.8745 Val Loss: 0.5101194977760315, Val Acc: 0.8424\n","Epoch [102/500], Loss: 0.3467254936695099, Train Acc: 0.8796,train F1-score:0.8742 Val Loss: 0.5087998509407043, Val Acc: 0.8424\n","Epoch [103/500], Loss: 0.3400593400001526, Train Acc: 0.8862,train F1-score:0.8815 Val Loss: 0.509752631187439, Val Acc: 0.8442\n","Epoch [104/500], Loss: 0.3415600061416626, Train Acc: 0.8836,train F1-score:0.8792 Val Loss: 0.5094276070594788, Val Acc: 0.8460\n","Epoch [105/500], Loss: 0.3338870704174042, Train Acc: 0.8853,train F1-score:0.8803 Val Loss: 0.5100487470626831, Val Acc: 0.8469\n","Epoch [106/500], Loss: 0.3308609127998352, Train Acc: 0.8875,train F1-score:0.8825 Val Loss: 0.5116515755653381, Val Acc: 0.8500\n","Epoch [107/500], Loss: 0.3292810618877411, Train Acc: 0.8846,train F1-score:0.8791 Val Loss: 0.5152534246444702, Val Acc: 0.8504\n","Epoch [108/500], Loss: 0.32467445731163025, Train Acc: 0.8857,train F1-score:0.8807 Val Loss: 0.5159019231796265, Val Acc: 0.8496\n","Epoch [109/500], Loss: 0.3310457766056061, Train Acc: 0.8864,train F1-score:0.8815 Val Loss: 0.535121500492096, Val Acc: 0.8469\n","Epoch [110/500], Loss: 0.3231765329837799, Train Acc: 0.8875,train F1-score:0.8821 Val Loss: 0.5322038531303406, Val Acc: 0.8482\n","Epoch [111/500], Loss: 0.32985255122184753, Train Acc: 0.8898,train F1-score:0.8847 Val Loss: 0.5292503237724304, Val Acc: 0.8496\n","Epoch [112/500], Loss: 0.3218557834625244, Train Acc: 0.8909,train F1-score:0.8853 Val Loss: 0.5254862904548645, Val Acc: 0.8496\n","Epoch [113/500], Loss: 0.31563955545425415, Train Acc: 0.8912,train F1-score:0.8865 Val Loss: 0.5223142504692078, Val Acc: 0.8496\n","Epoch [114/500], Loss: 0.31588107347488403, Train Acc: 0.8908,train F1-score:0.8861 Val Loss: 0.520914614200592, Val Acc: 0.8487\n","Epoch [115/500], Loss: 0.3180498480796814, Train Acc: 0.8930,train F1-score:0.8882 Val Loss: 0.5195921659469604, Val Acc: 0.8433\n","Epoch [116/500], Loss: 0.31371068954467773, Train Acc: 0.8910,train F1-score:0.8857 Val Loss: 0.5157060027122498, Val Acc: 0.8478\n","Epoch [117/500], Loss: 0.31640616059303284, Train Acc: 0.8918,train F1-score:0.8866 Val Loss: 0.5161606669425964, Val Acc: 0.8491\n","Epoch [118/500], Loss: 0.30818793177604675, Train Acc: 0.8945,train F1-score:0.8906 Val Loss: 0.5146475434303284, Val Acc: 0.8518\n","Epoch [119/500], Loss: 0.3191869258880615, Train Acc: 0.8907,train F1-score:0.8867 Val Loss: 0.5107415914535522, Val Acc: 0.8518\n","Epoch [120/500], Loss: 0.32007598876953125, Train Acc: 0.8918,train F1-score:0.8874 Val Loss: 0.5013989210128784, Val Acc: 0.8527\n","Epoch [121/500], Loss: 0.31448984146118164, Train Acc: 0.8915,train F1-score:0.8866 Val Loss: 0.5011802911758423, Val Acc: 0.8513\n","Epoch [122/500], Loss: 0.30972418189048767, Train Acc: 0.8934,train F1-score:0.8892 Val Loss: 0.5060064196586609, Val Acc: 0.8473\n","Epoch [123/500], Loss: 0.3096698820590973, Train Acc: 0.8920,train F1-score:0.8874 Val Loss: 0.5103251338005066, Val Acc: 0.8509\n","Epoch [124/500], Loss: 0.3064056932926178, Train Acc: 0.8926,train F1-score:0.8883 Val Loss: 0.5180875658988953, Val Acc: 0.8513\n","Epoch [125/500], Loss: 0.3051278591156006, Train Acc: 0.8960,train F1-score:0.8908 Val Loss: 0.5229170918464661, Val Acc: 0.8509\n","Epoch [126/500], Loss: 0.3045211136341095, Train Acc: 0.8960,train F1-score:0.8915 Val Loss: 0.5217201113700867, Val Acc: 0.8540\n","Epoch [127/500], Loss: 0.30374640226364136, Train Acc: 0.8972,train F1-score:0.8926 Val Loss: 0.5232844948768616, Val Acc: 0.8513\n","Epoch [128/500], Loss: 0.3018347918987274, Train Acc: 0.8982,train F1-score:0.8942 Val Loss: 0.5196347832679749, Val Acc: 0.8482\n","Epoch [129/500], Loss: 0.2938774526119232, Train Acc: 0.8987,train F1-score:0.8942 Val Loss: 0.5190946459770203, Val Acc: 0.8504\n","Epoch [130/500], Loss: 0.3027435839176178, Train Acc: 0.8991,train F1-score:0.8950 Val Loss: 0.5221335887908936, Val Acc: 0.8558\n","Epoch [131/500], Loss: 0.294689804315567, Train Acc: 0.8983,train F1-score:0.8943 Val Loss: 0.521737813949585, Val Acc: 0.8558\n","Epoch [132/500], Loss: 0.2984679639339447, Train Acc: 0.8941,train F1-score:0.8900 Val Loss: 0.5235092639923096, Val Acc: 0.8536\n","Epoch [133/500], Loss: 0.2884010076522827, Train Acc: 0.8996,train F1-score:0.8955 Val Loss: 0.5252116322517395, Val Acc: 0.8509\n","Epoch [134/500], Loss: 0.290799081325531, Train Acc: 0.8996,train F1-score:0.8955 Val Loss: 0.5252912640571594, Val Acc: 0.8522\n","Epoch [135/500], Loss: 0.2901601493358612, Train Acc: 0.9014,train F1-score:0.8974 Val Loss: 0.5289872884750366, Val Acc: 0.8536\n","Epoch [136/500], Loss: 0.2851288616657257, Train Acc: 0.9011,train F1-score:0.8971 Val Loss: 0.5304427742958069, Val Acc: 0.8562\n","Epoch [137/500], Loss: 0.2899600565433502, Train Acc: 0.9003,train F1-score:0.8962 Val Loss: 0.5247331261634827, Val Acc: 0.8545\n","Epoch [138/500], Loss: 0.2937902510166168, Train Acc: 0.8994,train F1-score:0.8948 Val Loss: 0.5232205986976624, Val Acc: 0.8549\n","Epoch [139/500], Loss: 0.2915881276130676, Train Acc: 0.9020,train F1-score:0.8979 Val Loss: 0.5209944844245911, Val Acc: 0.8567\n","Epoch [140/500], Loss: 0.28970596194267273, Train Acc: 0.9012,train F1-score:0.8974 Val Loss: 0.5156040191650391, Val Acc: 0.8594\n","Epoch [141/500], Loss: 0.2925761342048645, Train Acc: 0.8989,train F1-score:0.8947 Val Loss: 0.5022192001342773, Val Acc: 0.8580\n","Epoch [142/500], Loss: 0.2934982180595398, Train Acc: 0.8999,train F1-score:0.8956 Val Loss: 0.5020626187324524, Val Acc: 0.8567\n","Epoch [143/500], Loss: 0.2817988991737366, Train Acc: 0.9041,train F1-score:0.9005 Val Loss: 0.5009752511978149, Val Acc: 0.8558\n","Epoch [144/500], Loss: 0.284166544675827, Train Acc: 0.9009,train F1-score:0.8967 Val Loss: 0.5005824565887451, Val Acc: 0.8594\n","Epoch [145/500], Loss: 0.28298279643058777, Train Acc: 0.9043,train F1-score:0.9004 Val Loss: 0.4996800124645233, Val Acc: 0.8603\n","Epoch [146/500], Loss: 0.2838685214519501, Train Acc: 0.9029,train F1-score:0.8994 Val Loss: 0.5019805431365967, Val Acc: 0.8603\n","Epoch [147/500], Loss: 0.28543683886528015, Train Acc: 0.9021,train F1-score:0.8984 Val Loss: 0.5040086507797241, Val Acc: 0.8612\n","Epoch [148/500], Loss: 0.28681233525276184, Train Acc: 0.9040,train F1-score:0.9003 Val Loss: 0.5061320066452026, Val Acc: 0.8598\n","Epoch [149/500], Loss: 0.28036293387413025, Train Acc: 0.9017,train F1-score:0.8985 Val Loss: 0.5068614482879639, Val Acc: 0.8607\n","Epoch [150/500], Loss: 0.2776505947113037, Train Acc: 0.9027,train F1-score:0.8990 Val Loss: 0.5081932544708252, Val Acc: 0.8612\n","Epoch [151/500], Loss: 0.2753254473209381, Train Acc: 0.9054,train F1-score:0.9012 Val Loss: 0.5123776197433472, Val Acc: 0.8549\n","Epoch [152/500], Loss: 0.27734965085983276, Train Acc: 0.9032,train F1-score:0.8998 Val Loss: 0.5130032896995544, Val Acc: 0.8558\n","Epoch [153/500], Loss: 0.279841810464859, Train Acc: 0.9038,train F1-score:0.9005 Val Loss: 0.5091084241867065, Val Acc: 0.8576\n","Epoch [154/500], Loss: 0.2860078811645508, Train Acc: 0.9032,train F1-score:0.8992 Val Loss: 0.5050075650215149, Val Acc: 0.8589\n","Epoch [155/500], Loss: 0.27896445989608765, Train Acc: 0.9035,train F1-score:0.8996 Val Loss: 0.5011887550354004, Val Acc: 0.8585\n","Epoch [156/500], Loss: 0.27801597118377686, Train Acc: 0.9055,train F1-score:0.9013 Val Loss: 0.4990880787372589, Val Acc: 0.8607\n","Epoch [157/500], Loss: 0.2772974967956543, Train Acc: 0.9042,train F1-score:0.9005 Val Loss: 0.5050638914108276, Val Acc: 0.8598\n","Epoch [158/500], Loss: 0.2761482000350952, Train Acc: 0.9055,train F1-score:0.9022 Val Loss: 0.5000535249710083, Val Acc: 0.8621\n","Epoch [159/500], Loss: 0.27480408549308777, Train Acc: 0.9059,train F1-score:0.9025 Val Loss: 0.49818599224090576, Val Acc: 0.8616\n","Epoch [160/500], Loss: 0.27696672081947327, Train Acc: 0.9080,train F1-score:0.9051 Val Loss: 0.5001097321510315, Val Acc: 0.8607\n","Epoch [161/500], Loss: 0.2745978534221649, Train Acc: 0.9043,train F1-score:0.9008 Val Loss: 0.5053238272666931, Val Acc: 0.8603\n","Epoch [162/500], Loss: 0.2735708951950073, Train Acc: 0.9051,train F1-score:0.9016 Val Loss: 0.506500244140625, Val Acc: 0.8598\n","Epoch [163/500], Loss: 0.2606898844242096, Train Acc: 0.9111,train F1-score:0.9082 Val Loss: 0.5076287388801575, Val Acc: 0.8585\n","Epoch [164/500], Loss: 0.27594634890556335, Train Acc: 0.9080,train F1-score:0.9048 Val Loss: 0.5049561262130737, Val Acc: 0.8607\n","Epoch [165/500], Loss: 0.2738631069660187, Train Acc: 0.9073,train F1-score:0.9035 Val Loss: 0.5020850300788879, Val Acc: 0.8661\n","Epoch [166/500], Loss: 0.27089396119117737, Train Acc: 0.9067,train F1-score:0.9027 Val Loss: 0.5002439022064209, Val Acc: 0.8634\n","Epoch [167/500], Loss: 0.2676120400428772, Train Acc: 0.9080,train F1-score:0.9046 Val Loss: 0.49626824259757996, Val Acc: 0.8652\n","Epoch [168/500], Loss: 0.2655683159828186, Train Acc: 0.9083,train F1-score:0.9052 Val Loss: 0.4943483769893646, Val Acc: 0.8621\n","Epoch [169/500], Loss: 0.266978919506073, Train Acc: 0.9107,train F1-score:0.9071 Val Loss: 0.49836382269859314, Val Acc: 0.8652\n","Epoch [170/500], Loss: 0.261788010597229, Train Acc: 0.9092,train F1-score:0.9048 Val Loss: 0.5058379173278809, Val Acc: 0.8621\n","Epoch [171/500], Loss: 0.2666780352592468, Train Acc: 0.9107,train F1-score:0.9073 Val Loss: 0.5134567022323608, Val Acc: 0.8594\n","Epoch [172/500], Loss: 0.25503411889076233, Train Acc: 0.9111,train F1-score:0.9083 Val Loss: 0.5577115416526794, Val Acc: 0.8607\n","Epoch [173/500], Loss: 0.258606493473053, Train Acc: 0.9103,train F1-score:0.9072 Val Loss: 0.5546320676803589, Val Acc: 0.8625\n","Epoch [174/500], Loss: 0.26458939909935, Train Acc: 0.9073,train F1-score:0.9034 Val Loss: 0.5493406057357788, Val Acc: 0.8616\n","Epoch [175/500], Loss: 0.2558610141277313, Train Acc: 0.9131,train F1-score:0.9097 Val Loss: 0.5500757098197937, Val Acc: 0.8643\n","Epoch [176/500], Loss: 0.2689133882522583, Train Acc: 0.9098,train F1-score:0.9066 Val Loss: 0.509380578994751, Val Acc: 0.8652\n","Epoch [177/500], Loss: 0.26411494612693787, Train Acc: 0.9141,train F1-score:0.9113 Val Loss: 0.5094236135482788, Val Acc: 0.8625\n","Epoch [178/500], Loss: 0.2634477913379669, Train Acc: 0.9105,train F1-score:0.9073 Val Loss: 0.5142129063606262, Val Acc: 0.8598\n","Epoch [179/500], Loss: 0.26404058933258057, Train Acc: 0.9095,train F1-score:0.9051 Val Loss: 0.5150548815727234, Val Acc: 0.8571\n","Epoch [180/500], Loss: 0.25629350543022156, Train Acc: 0.9109,train F1-score:0.9071 Val Loss: 0.517047107219696, Val Acc: 0.8629\n","Epoch [181/500], Loss: 0.26388925313949585, Train Acc: 0.9101,train F1-score:0.9070 Val Loss: 0.5210598707199097, Val Acc: 0.8665\n","Epoch [182/500], Loss: 0.2572508454322815, Train Acc: 0.9121,train F1-score:0.9095 Val Loss: 0.5191152691841125, Val Acc: 0.8634\n","Epoch [183/500], Loss: 0.2551480531692505, Train Acc: 0.9119,train F1-score:0.9083 Val Loss: 0.5173963904380798, Val Acc: 0.8616\n","Epoch [184/500], Loss: 0.25330597162246704, Train Acc: 0.9133,train F1-score:0.9094 Val Loss: 0.516204297542572, Val Acc: 0.8643\n","Epoch [185/500], Loss: 0.250285267829895, Train Acc: 0.9122,train F1-score:0.9087 Val Loss: 0.515109658241272, Val Acc: 0.8647\n","Epoch [186/500], Loss: 0.25173065066337585, Train Acc: 0.9159,train F1-score:0.9127 Val Loss: 0.5144319534301758, Val Acc: 0.8643\n","Epoch [187/500], Loss: 0.2532259225845337, Train Acc: 0.9122,train F1-score:0.9095 Val Loss: 0.5090500712394714, Val Acc: 0.8625\n","Epoch [188/500], Loss: 0.24547456204891205, Train Acc: 0.9148,train F1-score:0.9121 Val Loss: 0.5044344067573547, Val Acc: 0.8629\n","Epoch [189/500], Loss: 0.24459567666053772, Train Acc: 0.9144,train F1-score:0.9115 Val Loss: 0.5048415064811707, Val Acc: 0.8629\n","Epoch [190/500], Loss: 0.25238287448883057, Train Acc: 0.9135,train F1-score:0.9103 Val Loss: 0.5073950290679932, Val Acc: 0.8629\n","Epoch [191/500], Loss: 0.2523917555809021, Train Acc: 0.9146,train F1-score:0.9117 Val Loss: 0.5109646916389465, Val Acc: 0.8643\n","Epoch [192/500], Loss: 0.25231990218162537, Train Acc: 0.9124,train F1-score:0.9099 Val Loss: 0.5128304362297058, Val Acc: 0.8643\n","Epoch [193/500], Loss: 0.2510966658592224, Train Acc: 0.9130,train F1-score:0.9105 Val Loss: 0.5124413371086121, Val Acc: 0.8634\n","Epoch [194/500], Loss: 0.24126560986042023, Train Acc: 0.9160,train F1-score:0.9125 Val Loss: 0.509933590888977, Val Acc: 0.8670\n","Epoch [195/500], Loss: 0.2384907454252243, Train Acc: 0.9173,train F1-score:0.9141 Val Loss: 0.5518016815185547, Val Acc: 0.8692\n","Epoch [196/500], Loss: 0.2412075698375702, Train Acc: 0.9152,train F1-score:0.9119 Val Loss: 0.5533481240272522, Val Acc: 0.8688\n","Epoch [197/500], Loss: 0.24088408052921295, Train Acc: 0.9149,train F1-score:0.9116 Val Loss: 0.5548850297927856, Val Acc: 0.8670\n","Epoch [198/500], Loss: 0.2436579018831253, Train Acc: 0.9167,train F1-score:0.9138 Val Loss: 0.555810272693634, Val Acc: 0.8665\n","Epoch [199/500], Loss: 0.2413172721862793, Train Acc: 0.9168,train F1-score:0.9139 Val Loss: 0.5580607056617737, Val Acc: 0.8661\n","Epoch [200/500], Loss: 0.2347991168498993, Train Acc: 0.9181,train F1-score:0.9152 Val Loss: 0.5609990954399109, Val Acc: 0.8629\n","Epoch [201/500], Loss: 0.24376465380191803, Train Acc: 0.9154,train F1-score:0.9122 Val Loss: 0.6490354537963867, Val Acc: 0.8661\n","Epoch [202/500], Loss: 0.24632646143436432, Train Acc: 0.9168,train F1-score:0.9143 Val Loss: 0.6493333578109741, Val Acc: 0.8661\n","Epoch [203/500], Loss: 0.24241012334823608, Train Acc: 0.9148,train F1-score:0.9122 Val Loss: 0.643377423286438, Val Acc: 0.8683\n","Epoch [204/500], Loss: 0.24133612215518951, Train Acc: 0.9158,train F1-score:0.9129 Val Loss: 0.6427270770072937, Val Acc: 0.8656\n","Epoch [205/500], Loss: 0.23714344203472137, Train Acc: 0.9163,train F1-score:0.9133 Val Loss: 0.6422315239906311, Val Acc: 0.8656\n","Epoch [206/500], Loss: 0.242179274559021, Train Acc: 0.9199,train F1-score:0.9174 Val Loss: 0.6416077017784119, Val Acc: 0.8683\n","Epoch [207/500], Loss: 0.24031075835227966, Train Acc: 0.9187,train F1-score:0.9162 Val Loss: 0.6402395367622375, Val Acc: 0.8688\n","Epoch [208/500], Loss: 0.2392052710056305, Train Acc: 0.9175,train F1-score:0.9150 Val Loss: 0.6312803030014038, Val Acc: 0.8643\n","Epoch [209/500], Loss: 0.24143563210964203, Train Acc: 0.9158,train F1-score:0.9125 Val Loss: 0.6255642175674438, Val Acc: 0.8674\n","Epoch [210/500], Loss: 0.23466302454471588, Train Acc: 0.9192,train F1-score:0.9161 Val Loss: 0.564410388469696, Val Acc: 0.8688\n","Epoch [211/500], Loss: 0.23325277864933014, Train Acc: 0.9201,train F1-score:0.9172 Val Loss: 0.5659560561180115, Val Acc: 0.8710\n","Epoch [212/500], Loss: 0.23664557933807373, Train Acc: 0.9187,train F1-score:0.9157 Val Loss: 0.5636053085327148, Val Acc: 0.8705\n","Epoch [213/500], Loss: 0.23694780468940735, Train Acc: 0.9165,train F1-score:0.9132 Val Loss: 0.5596855282783508, Val Acc: 0.8705\n","Epoch [214/500], Loss: 0.24242860078811646, Train Acc: 0.9162,train F1-score:0.9122 Val Loss: 0.5525386929512024, Val Acc: 0.8719\n","Epoch [215/500], Loss: 0.23173609375953674, Train Acc: 0.9202,train F1-score:0.9169 Val Loss: 0.5470017194747925, Val Acc: 0.8688\n","Epoch [216/500], Loss: 0.22818316519260406, Train Acc: 0.9180,train F1-score:0.9152 Val Loss: 0.5504768490791321, Val Acc: 0.8692\n","Epoch [217/500], Loss: 0.2362924963235855, Train Acc: 0.9185,train F1-score:0.9160 Val Loss: 0.554841160774231, Val Acc: 0.8692\n","Epoch [218/500], Loss: 0.2358914315700531, Train Acc: 0.9149,train F1-score:0.9128 Val Loss: 0.5585535168647766, Val Acc: 0.8688\n","Epoch [219/500], Loss: 0.2347472459077835, Train Acc: 0.9149,train F1-score:0.9122 Val Loss: 0.5621832013130188, Val Acc: 0.8683\n","Epoch [220/500], Loss: 0.229747474193573, Train Acc: 0.9207,train F1-score:0.9183 Val Loss: 0.564708411693573, Val Acc: 0.8683\n","Epoch [221/500], Loss: 0.2283611297607422, Train Acc: 0.9172,train F1-score:0.9148 Val Loss: 0.5620893836021423, Val Acc: 0.8705\n","Epoch [222/500], Loss: 0.23442640900611877, Train Acc: 0.9210,train F1-score:0.9185 Val Loss: 0.5606963038444519, Val Acc: 0.8719\n","Epoch [223/500], Loss: 0.23779205977916718, Train Acc: 0.9181,train F1-score:0.9153 Val Loss: 0.5598821043968201, Val Acc: 0.8710\n","Epoch [224/500], Loss: 0.23826684057712555, Train Acc: 0.9209,train F1-score:0.9184 Val Loss: 0.5620995759963989, Val Acc: 0.8670\n","Epoch [225/500], Loss: 0.23381407558918, Train Acc: 0.9190,train F1-score:0.9165 Val Loss: 0.5590683817863464, Val Acc: 0.8683\n","Epoch [226/500], Loss: 0.2304919958114624, Train Acc: 0.9186,train F1-score:0.9161 Val Loss: 0.5573092103004456, Val Acc: 0.8679\n","Epoch [227/500], Loss: 0.2323177307844162, Train Acc: 0.9185,train F1-score:0.9156 Val Loss: 0.557345449924469, Val Acc: 0.8652\n","Epoch [228/500], Loss: 0.23666618764400482, Train Acc: 0.9199,train F1-score:0.9171 Val Loss: 0.5585221648216248, Val Acc: 0.8674\n","Epoch [229/500], Loss: 0.2238820195198059, Train Acc: 0.9209,train F1-score:0.9190 Val Loss: 0.5627177357673645, Val Acc: 0.8692\n","Epoch [230/500], Loss: 0.22859624028205872, Train Acc: 0.9191,train F1-score:0.9167 Val Loss: 0.5622658133506775, Val Acc: 0.8710\n","Epoch [231/500], Loss: 0.22860056161880493, Train Acc: 0.9203,train F1-score:0.9175 Val Loss: 0.5633293986320496, Val Acc: 0.8705\n","Epoch [232/500], Loss: 0.2289838343858719, Train Acc: 0.9197,train F1-score:0.9171 Val Loss: 0.5631670355796814, Val Acc: 0.8732\n","Epoch [233/500], Loss: 0.22809529304504395, Train Acc: 0.9198,train F1-score:0.9170 Val Loss: 0.5631996989250183, Val Acc: 0.8701\n","Epoch [234/500], Loss: 0.22718055546283722, Train Acc: 0.9230,train F1-score:0.9203 Val Loss: 0.5649204254150391, Val Acc: 0.8696\n","Epoch [235/500], Loss: 0.22682689130306244, Train Acc: 0.9207,train F1-score:0.9184 Val Loss: 0.5666593909263611, Val Acc: 0.8688\n","Epoch [236/500], Loss: 0.22953900694847107, Train Acc: 0.9176,train F1-score:0.9151 Val Loss: 0.5723448395729065, Val Acc: 0.8674\n","Epoch [237/500], Loss: 0.2250075340270996, Train Acc: 0.9249,train F1-score:0.9221 Val Loss: 0.5745592713356018, Val Acc: 0.8670\n","Epoch [238/500], Loss: 0.23003049194812775, Train Acc: 0.9194,train F1-score:0.9169 Val Loss: 0.579455554485321, Val Acc: 0.8737\n","Epoch [239/500], Loss: 0.2226586788892746, Train Acc: 0.9239,train F1-score:0.9217 Val Loss: 0.5784539580345154, Val Acc: 0.8728\n","Epoch [240/500], Loss: 0.22719812393188477, Train Acc: 0.9208,train F1-score:0.9179 Val Loss: 0.5736750960350037, Val Acc: 0.8728\n","Epoch [241/500], Loss: 0.2223435789346695, Train Acc: 0.9247,train F1-score:0.9219 Val Loss: 0.5705431699752808, Val Acc: 0.8705\n","Epoch [242/500], Loss: 0.2270708978176117, Train Acc: 0.9190,train F1-score:0.9161 Val Loss: 0.5703931450843811, Val Acc: 0.8674\n","Epoch [243/500], Loss: 0.22003638744354248, Train Acc: 0.9237,train F1-score:0.9215 Val Loss: 0.5738627314567566, Val Acc: 0.8683\n","Epoch [244/500], Loss: 0.2265154868364334, Train Acc: 0.9216,train F1-score:0.9195 Val Loss: 0.5772313475608826, Val Acc: 0.8692\n","Epoch [245/500], Loss: 0.2210254669189453, Train Acc: 0.9237,train F1-score:0.9217 Val Loss: 0.5786406397819519, Val Acc: 0.8701\n","Epoch [246/500], Loss: 0.2199394851922989, Train Acc: 0.9254,train F1-score:0.9228 Val Loss: 0.5827378034591675, Val Acc: 0.8714\n","Epoch [247/500], Loss: 0.23312970995903015, Train Acc: 0.9214,train F1-score:0.9189 Val Loss: 0.5736185312271118, Val Acc: 0.8705\n","Epoch [248/500], Loss: 0.2280474305152893, Train Acc: 0.9223,train F1-score:0.9197 Val Loss: 0.5065032243728638, Val Acc: 0.8705\n","Epoch [249/500], Loss: 0.21637709438800812, Train Acc: 0.9257,train F1-score:0.9235 Val Loss: 0.4997555911540985, Val Acc: 0.8701\n","Epoch [250/500], Loss: 0.21800807118415833, Train Acc: 0.9230,train F1-score:0.9211 Val Loss: 0.4955325424671173, Val Acc: 0.8705\n","Epoch [251/500], Loss: 0.217235267162323, Train Acc: 0.9208,train F1-score:0.9187 Val Loss: 0.4946509897708893, Val Acc: 0.8710\n","Epoch [252/500], Loss: 0.22427810728549957, Train Acc: 0.9212,train F1-score:0.9189 Val Loss: 0.4970146715641022, Val Acc: 0.8696\n","Epoch [253/500], Loss: 0.21140390634536743, Train Acc: 0.9251,train F1-score:0.9229 Val Loss: 0.5036984086036682, Val Acc: 0.8710\n","Epoch [254/500], Loss: 0.21294665336608887, Train Acc: 0.9234,train F1-score:0.9216 Val Loss: 0.5099673271179199, Val Acc: 0.8692\n","Epoch [255/500], Loss: 0.21809335052967072, Train Acc: 0.9254,train F1-score:0.9230 Val Loss: 0.5137307643890381, Val Acc: 0.8719\n","Epoch [256/500], Loss: 0.21808505058288574, Train Acc: 0.9270,train F1-score:0.9247 Val Loss: 0.5184313058853149, Val Acc: 0.8723\n","Epoch [257/500], Loss: 0.20940424501895905, Train Acc: 0.9273,train F1-score:0.9253 Val Loss: 0.5229803323745728, Val Acc: 0.8696\n","Epoch [258/500], Loss: 0.20648805797100067, Train Acc: 0.9276,train F1-score:0.9259 Val Loss: 0.5287256240844727, Val Acc: 0.8705\n","Epoch [259/500], Loss: 0.2177533209323883, Train Acc: 0.9254,train F1-score:0.9233 Val Loss: 0.528937041759491, Val Acc: 0.8723\n","Epoch [260/500], Loss: 0.2108101099729538, Train Acc: 0.9254,train F1-score:0.9230 Val Loss: 0.526928722858429, Val Acc: 0.8719\n","Epoch [261/500], Loss: 0.2209283411502838, Train Acc: 0.9240,train F1-score:0.9221 Val Loss: 0.5236291289329529, Val Acc: 0.8719\n","Epoch [262/500], Loss: 0.22273045778274536, Train Acc: 0.9231,train F1-score:0.9211 Val Loss: 0.5196734070777893, Val Acc: 0.8723\n","Epoch [263/500], Loss: 0.21602454781532288, Train Acc: 0.9208,train F1-score:0.9185 Val Loss: 0.5162578225135803, Val Acc: 0.8710\n","Epoch [264/500], Loss: 0.22287043929100037, Train Acc: 0.9233,train F1-score:0.9211 Val Loss: 0.5134590268135071, Val Acc: 0.8701\n","Epoch [265/500], Loss: 0.21382999420166016, Train Acc: 0.9274,train F1-score:0.9254 Val Loss: 0.5166773796081543, Val Acc: 0.8670\n","Epoch [266/500], Loss: 0.21674054861068726, Train Acc: 0.9240,train F1-score:0.9221 Val Loss: 0.5212734937667847, Val Acc: 0.8647\n","Epoch [267/500], Loss: 0.2219846248626709, Train Acc: 0.9244,train F1-score:0.9219 Val Loss: 0.5089755654335022, Val Acc: 0.8679\n","Early stopping at epoch 267\n","Test Loss: 0.3300418555736542, Test Accuracy: 0.9035283608753908\n","Precision: 0.8999, Recall: 0.9035, F1-score: 0.8977\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MZUV1ypEUMMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=8)\n","decision_tree.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(train_outputs)\n","val_pred = decision_tree.predict(val_outputs)\n","test_pred = decision_tree.predict(test_outputs)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n"],"metadata":{"id":"sZJciy2xhiKJ","executionInfo":{"status":"ok","timestamp":1714805394497,"user_tz":-330,"elapsed":628,"user":{"displayName":"Goutham Krishna","userId":"05855847327907102213"}},"outputId":"a7bb1db6-30a0-43f2-c568-3d547ca11ff2","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.9471719781797301\n","Validation Accuracy (Decision Tree): 0.8486607142857143\n","Test Accuracy (Decision Tree): 0.899955337204109\n"]}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Concatenate the GIN model output with the original feature matrices\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=8)\n","decision_tree.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(X_train_combined)\n","val_pred = decision_tree.predict(X_val_combined)\n","test_pred = decision_tree.predict(X_test_combined)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n"],"metadata":{"id":"bn2qG0LHhwUN","executionInfo":{"status":"ok","timestamp":1714805401007,"user_tz":-330,"elapsed":1395,"user":{"displayName":"Goutham Krishna","userId":"05855847327907102213"}},"outputId":"67dfdd52-ed9e-4795-da76-a9b028c081fa","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.9525313427122213\n","Validation Accuracy (Decision Tree): 0.8558035714285714\n","Test Accuracy (Decision Tree): 0.9053148727110317\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(train_outputs)\n","val_pred = svm_classifier.predict(val_outputs)\n","test_pred = svm_classifier.predict(test_outputs)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n"],"metadata":{"id":"BDVZuO2mimyx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714805408602,"user_tz":-330,"elapsed":3066,"user":{"displayName":"Goutham Krishna","userId":"05855847327907102213"}},"outputId":"e2f20a25-6dd6-4a62-c95c-b39fccaa2dd0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.94229112833764\n","Validation Accuracy (SVM): 0.8611607142857143\n","Test Accuracy (SVM): 0.9084412684234033\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Combine the output of the GIN model with the original features\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(X_train_combined)\n","val_pred = svm_classifier.predict(X_val_combined)\n","test_pred = svm_classifier.predict(X_test_combined)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n"],"metadata":{"id":"GAbAq_fciw4W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714805421798,"user_tz":-330,"elapsed":9679,"user":{"displayName":"Goutham Krishna","userId":"05855847327907102213"}},"outputId":"272cc5ca-f510-43f0-ccd0-d601bcc7cb10"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9537754809072638\n","Validation Accuracy (SVM): 0.8589285714285714\n","Test Accuracy (SVM): 0.9084412684234033\n"]}]}]}