{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1529,"status":"ok","timestamp":1714549239851,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"7dwH7gXuGNo0","outputId":"ef8ea77b-ec79-4639-be9d-cf15009d87c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["scikit-learn version installed in Colab: 1.2.2\n"]}],"source":["import sklearn\n","\n","print(f\"scikit-learn version installed in Colab: {sklearn.__version__}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1553,"status":"ok","timestamp":1714549367063,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"kw2AysilGwoU","outputId":"7dbfda25-a5c1-4e5e-8603-e4c1705fd9cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["NetworkX version installed in Colab: 3.3\n"]}],"source":["import networkx as nx\n","\n","print(f\"NetworkX version installed in Colab: {nx.__version__}\")"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlrWhbWEYmIN","executionInfo":{"status":"ok","timestamp":1714809861596,"user_tz":-330,"elapsed":26393,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"dd58c131-3bd8-43ea-ec73-a52a4c13cddc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18772,"status":"ok","timestamp":1714809880361,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"OEuPfhoQZgJ2","outputId":"87f4f007-fb37-401c-8075-910a2303a245"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.5.3\n"]}],"source":["!pip install torch-geometric"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228148,"status":"ok","timestamp":1714810108501,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"yBLlJQNsZuY7","outputId":"da8629e8-3f08-4944-f340-ddcafd888f67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.9315733909606934, Train Acc: 0.0665,train F1-score:0.1003 Val Loss: 1.700194001197815, Val Acc: 0.6225\n","Epoch [2/500], Loss: 1.8956758975982666, Train Acc: 0.6478,train F1-score:0.5121 Val Loss: 1.3951658010482788, Val Acc: 0.6332\n","Epoch [3/500], Loss: 1.4214621782302856, Train Acc: 0.6363,train F1-score:0.5385 Val Loss: 1.2232803106307983, Val Acc: 0.6506\n","Epoch [4/500], Loss: 1.176771640777588, Train Acc: 0.6421,train F1-score:0.5689 Val Loss: 1.2200710773468018, Val Acc: 0.5930\n","Epoch [5/500], Loss: 1.1610640287399292, Train Acc: 0.6466,train F1-score:0.5794 Val Loss: 1.095110535621643, Val Acc: 0.6278\n","Epoch [6/500], Loss: 1.0101537704467773, Train Acc: 0.6645,train F1-score:0.5941 Val Loss: 0.997617244720459, Val Acc: 0.6586\n","Epoch [7/500], Loss: 0.989360511302948, Train Acc: 0.6642,train F1-score:0.6190 Val Loss: 1.022924542427063, Val Acc: 0.6412\n","Epoch [8/500], Loss: 0.9605795741081238, Train Acc: 0.6794,train F1-score:0.6113 Val Loss: 0.9771831631660461, Val Acc: 0.6653\n","Epoch [9/500], Loss: 0.9111746549606323, Train Acc: 0.6931,train F1-score:0.6280 Val Loss: 0.8932657837867737, Val Acc: 0.6908\n","Epoch [10/500], Loss: 0.8703733682632446, Train Acc: 0.7000,train F1-score:0.6647 Val Loss: 0.90418541431427, Val Acc: 0.6747\n","Epoch [11/500], Loss: 0.8512933254241943, Train Acc: 0.7032,train F1-score:0.6688 Val Loss: 0.9090922474861145, Val Acc: 0.6747\n","Epoch [12/500], Loss: 0.8233174681663513, Train Acc: 0.7221,train F1-score:0.6813 Val Loss: 0.8397600054740906, Val Acc: 0.6948\n","Epoch [13/500], Loss: 0.7883326411247253, Train Acc: 0.7338,train F1-score:0.6920 Val Loss: 0.7869539856910706, Val Acc: 0.7122\n","Epoch [14/500], Loss: 0.766546368598938, Train Acc: 0.7358,train F1-score:0.7044 Val Loss: 0.7718607187271118, Val Acc: 0.7149\n","Epoch [15/500], Loss: 0.9281445741653442, Train Acc: 0.7378,train F1-score:0.7082 Val Loss: 0.7725468873977661, Val Acc: 0.7108\n","Epoch [16/500], Loss: 0.7523561716079712, Train Acc: 0.7459,train F1-score:0.7091 Val Loss: 0.7628500461578369, Val Acc: 0.7082\n","Epoch [17/500], Loss: 0.7266375422477722, Train Acc: 0.7483,train F1-score:0.7094 Val Loss: 0.7506584525108337, Val Acc: 0.7068\n","Epoch [18/500], Loss: 0.7102314233779907, Train Acc: 0.7507,train F1-score:0.7153 Val Loss: 0.7370120286941528, Val Acc: 0.7135\n","Epoch [19/500], Loss: 0.7009242177009583, Train Acc: 0.7529,train F1-score:0.7205 Val Loss: 0.7223415970802307, Val Acc: 0.7403\n","Epoch [20/500], Loss: 0.6700063347816467, Train Acc: 0.7617,train F1-score:0.7290 Val Loss: 0.7114817500114441, Val Acc: 0.7577\n","Epoch [21/500], Loss: 0.6597789525985718, Train Acc: 0.7700,train F1-score:0.7440 Val Loss: 0.6917515397071838, Val Acc: 0.7590\n","Epoch [22/500], Loss: 0.6589130163192749, Train Acc: 0.7801,train F1-score:0.7619 Val Loss: 0.6782376170158386, Val Acc: 0.7657\n","Epoch [23/500], Loss: 0.6390475034713745, Train Acc: 0.7881,train F1-score:0.7748 Val Loss: 0.6742181181907654, Val Acc: 0.7684\n","Epoch [24/500], Loss: 0.6304280161857605, Train Acc: 0.7894,train F1-score:0.7750 Val Loss: 0.6752553582191467, Val Acc: 0.7684\n","Epoch [25/500], Loss: 0.6253838539123535, Train Acc: 0.7905,train F1-score:0.7740 Val Loss: 0.6560546159744263, Val Acc: 0.7697\n","Epoch [26/500], Loss: 0.6052372455596924, Train Acc: 0.7949,train F1-score:0.7798 Val Loss: 0.6279510259628296, Val Acc: 0.7697\n","Epoch [27/500], Loss: 0.5919893383979797, Train Acc: 0.8033,train F1-score:0.7936 Val Loss: 0.6180189847946167, Val Acc: 0.7657\n","Epoch [28/500], Loss: 0.5870346426963806, Train Acc: 0.7979,train F1-score:0.7879 Val Loss: 0.6178354620933533, Val Acc: 0.7764\n","Epoch [29/500], Loss: 0.5761043429374695, Train Acc: 0.8048,train F1-score:0.7926 Val Loss: 0.6100153923034668, Val Acc: 0.7805\n","Epoch [30/500], Loss: 0.5746524333953857, Train Acc: 0.8090,train F1-score:0.7973 Val Loss: 0.5997224450111389, Val Acc: 0.7791\n","Epoch [31/500], Loss: 0.5609833002090454, Train Acc: 0.8161,train F1-score:0.8058 Val Loss: 0.5965840220451355, Val Acc: 0.7805\n","Epoch [32/500], Loss: 0.5557940006256104, Train Acc: 0.8179,train F1-score:0.8074 Val Loss: 0.5954015254974365, Val Acc: 0.7885\n","Epoch [33/500], Loss: 0.5423824787139893, Train Acc: 0.8191,train F1-score:0.8075 Val Loss: 0.5860082507133484, Val Acc: 0.7885\n","Epoch [34/500], Loss: 0.5403603315353394, Train Acc: 0.8209,train F1-score:0.8105 Val Loss: 0.5706746578216553, Val Acc: 0.7871\n","Epoch [35/500], Loss: 0.5257730484008789, Train Acc: 0.8269,train F1-score:0.8184 Val Loss: 0.5608183741569519, Val Acc: 0.7925\n","Epoch [36/500], Loss: 0.5203796029090881, Train Acc: 0.8252,train F1-score:0.8173 Val Loss: 0.5600677728652954, Val Acc: 0.7965\n","Epoch [37/500], Loss: 0.5175613164901733, Train Acc: 0.8279,train F1-score:0.8193 Val Loss: 0.5608436465263367, Val Acc: 0.7885\n","Epoch [38/500], Loss: 0.5118064880371094, Train Acc: 0.8303,train F1-score:0.8211 Val Loss: 0.5572460293769836, Val Acc: 0.7912\n","Epoch [39/500], Loss: 0.503424882888794, Train Acc: 0.8343,train F1-score:0.8255 Val Loss: 0.5479952692985535, Val Acc: 0.7979\n","Epoch [40/500], Loss: 0.5008307099342346, Train Acc: 0.8333,train F1-score:0.8254 Val Loss: 0.5389124155044556, Val Acc: 0.8019\n","Epoch [41/500], Loss: 0.4911070764064789, Train Acc: 0.8326,train F1-score:0.8258 Val Loss: 0.5394842028617859, Val Acc: 0.8059\n","Epoch [42/500], Loss: 0.4904356598854065, Train Acc: 0.8419,train F1-score:0.8339 Val Loss: 0.5435813069343567, Val Acc: 0.7992\n","Epoch [43/500], Loss: 0.4816383421421051, Train Acc: 0.8421,train F1-score:0.8332 Val Loss: 0.5416669845581055, Val Acc: 0.8032\n","Epoch [44/500], Loss: 0.4757775068283081, Train Acc: 0.8381,train F1-score:0.8289 Val Loss: 0.5334277153015137, Val Acc: 0.8072\n","Epoch [45/500], Loss: 0.46209946274757385, Train Acc: 0.8474,train F1-score:0.8398 Val Loss: 0.5279517769813538, Val Acc: 0.8112\n","Epoch [46/500], Loss: 0.46511873602867126, Train Acc: 0.8453,train F1-score:0.8385 Val Loss: 0.5248184204101562, Val Acc: 0.8112\n","Epoch [47/500], Loss: 0.45603659749031067, Train Acc: 0.8478,train F1-score:0.8402 Val Loss: 0.5247572660446167, Val Acc: 0.8153\n","Epoch [48/500], Loss: 0.45576927065849304, Train Acc: 0.8502,train F1-score:0.8422 Val Loss: 0.5187241435050964, Val Acc: 0.8166\n","Epoch [49/500], Loss: 0.4500986635684967, Train Acc: 0.8503,train F1-score:0.8427 Val Loss: 0.5105734467506409, Val Acc: 0.8220\n","Epoch [50/500], Loss: 0.4445722699165344, Train Acc: 0.8493,train F1-score:0.8425 Val Loss: 0.506446361541748, Val Acc: 0.8206\n","Epoch [51/500], Loss: 0.4440005421638489, Train Acc: 0.8523,train F1-score:0.8458 Val Loss: 0.5077694654464722, Val Acc: 0.8193\n","Epoch [52/500], Loss: 0.4446714520454407, Train Acc: 0.8557,train F1-score:0.8490 Val Loss: 0.5076333284378052, Val Acc: 0.8179\n","Epoch [53/500], Loss: 0.43353527784347534, Train Acc: 0.8564,train F1-score:0.8497 Val Loss: 0.5012125372886658, Val Acc: 0.8233\n","Epoch [54/500], Loss: 0.4229460060596466, Train Acc: 0.8567,train F1-score:0.8503 Val Loss: 0.4981048107147217, Val Acc: 0.8313\n","Epoch [55/500], Loss: 0.4306873083114624, Train Acc: 0.8569,train F1-score:0.8499 Val Loss: 0.49432051181793213, Val Acc: 0.8286\n","Epoch [56/500], Loss: 0.42580080032348633, Train Acc: 0.8566,train F1-score:0.8505 Val Loss: 0.4942864775657654, Val Acc: 0.8246\n","Epoch [57/500], Loss: 0.4163835346698761, Train Acc: 0.8630,train F1-score:0.8569 Val Loss: 0.4901336431503296, Val Acc: 0.8300\n","Epoch [58/500], Loss: 0.41221749782562256, Train Acc: 0.8613,train F1-score:0.8549 Val Loss: 0.4861714541912079, Val Acc: 0.8313\n","Epoch [59/500], Loss: 0.4091584384441376, Train Acc: 0.8627,train F1-score:0.8565 Val Loss: 0.48349031805992126, Val Acc: 0.8353\n","Epoch [60/500], Loss: 0.40411248803138733, Train Acc: 0.8610,train F1-score:0.8549 Val Loss: 0.48384857177734375, Val Acc: 0.8367\n","Epoch [61/500], Loss: 0.4082467257976532, Train Acc: 0.8660,train F1-score:0.8603 Val Loss: 0.4872630536556244, Val Acc: 0.8353\n","Epoch [62/500], Loss: 0.403378427028656, Train Acc: 0.8665,train F1-score:0.8606 Val Loss: 0.48848581314086914, Val Acc: 0.8380\n","Epoch [63/500], Loss: 0.39300206303596497, Train Acc: 0.8713,train F1-score:0.8657 Val Loss: 0.4894586503505707, Val Acc: 0.8434\n","Epoch [64/500], Loss: 0.3954497277736664, Train Acc: 0.8690,train F1-score:0.8625 Val Loss: 0.48659101128578186, Val Acc: 0.8501\n","Epoch [65/500], Loss: 0.3887588381767273, Train Acc: 0.8687,train F1-score:0.8627 Val Loss: 0.48051124811172485, Val Acc: 0.8474\n","Epoch [66/500], Loss: 0.3867354691028595, Train Acc: 0.8701,train F1-score:0.8642 Val Loss: 0.47648006677627563, Val Acc: 0.8420\n","Epoch [67/500], Loss: 0.3832107186317444, Train Acc: 0.8718,train F1-score:0.8664 Val Loss: 0.47651907801628113, Val Acc: 0.8434\n","Epoch [68/500], Loss: 0.386375367641449, Train Acc: 0.8726,train F1-score:0.8671 Val Loss: 0.48564067482948303, Val Acc: 0.8447\n","Epoch [69/500], Loss: 0.38409218192100525, Train Acc: 0.8700,train F1-score:0.8637 Val Loss: 0.4818515479564667, Val Acc: 0.8420\n","Epoch [70/500], Loss: 0.37470516562461853, Train Acc: 0.8751,train F1-score:0.8699 Val Loss: 0.4734092652797699, Val Acc: 0.8487\n","Epoch [71/500], Loss: 0.37581971287727356, Train Acc: 0.8723,train F1-score:0.8673 Val Loss: 0.46839669346809387, Val Acc: 0.8581\n","Epoch [72/500], Loss: 0.37805646657943726, Train Acc: 0.8738,train F1-score:0.8685 Val Loss: 0.47045108675956726, Val Acc: 0.8541\n","Epoch [73/500], Loss: 0.3750848174095154, Train Acc: 0.8749,train F1-score:0.8696 Val Loss: 0.4783957302570343, Val Acc: 0.8541\n","Epoch [74/500], Loss: 0.3735322058200836, Train Acc: 0.8733,train F1-score:0.8674 Val Loss: 0.4786966145038605, Val Acc: 0.8554\n","Epoch [75/500], Loss: 0.36073142290115356, Train Acc: 0.8798,train F1-score:0.8739 Val Loss: 0.46823638677597046, Val Acc: 0.8594\n","Epoch [76/500], Loss: 0.36739423871040344, Train Acc: 0.8787,train F1-score:0.8738 Val Loss: 0.46562379598617554, Val Acc: 0.8594\n","Epoch [77/500], Loss: 0.3626289963722229, Train Acc: 0.8776,train F1-score:0.8735 Val Loss: 0.4714718759059906, Val Acc: 0.8568\n","Epoch [78/500], Loss: 0.3544044494628906, Train Acc: 0.8811,train F1-score:0.8758 Val Loss: 0.47551676630973816, Val Acc: 0.8541\n","Epoch [79/500], Loss: 0.3602369725704193, Train Acc: 0.8792,train F1-score:0.8736 Val Loss: 0.46018025279045105, Val Acc: 0.8514\n","Epoch [80/500], Loss: 0.3502868413925171, Train Acc: 0.8829,train F1-score:0.8778 Val Loss: 0.4490530788898468, Val Acc: 0.8568\n","Epoch [81/500], Loss: 0.3568342924118042, Train Acc: 0.8802,train F1-score:0.8760 Val Loss: 0.445643812417984, Val Acc: 0.8621\n","Epoch [82/500], Loss: 0.3467266857624054, Train Acc: 0.8852,train F1-score:0.8809 Val Loss: 0.45155778527259827, Val Acc: 0.8568\n","Epoch [83/500], Loss: 0.34867286682128906, Train Acc: 0.8814,train F1-score:0.8760 Val Loss: 0.45209094882011414, Val Acc: 0.8594\n","Epoch [84/500], Loss: 0.34087494015693665, Train Acc: 0.8838,train F1-score:0.8784 Val Loss: 0.44339725375175476, Val Acc: 0.8661\n","Epoch [85/500], Loss: 0.34958139061927795, Train Acc: 0.8826,train F1-score:0.8776 Val Loss: 0.4343384802341461, Val Acc: 0.8675\n","Epoch [86/500], Loss: 0.34073543548583984, Train Acc: 0.8851,train F1-score:0.8810 Val Loss: 0.4408046305179596, Val Acc: 0.8701\n","Epoch [87/500], Loss: 0.340425580739975, Train Acc: 0.8859,train F1-score:0.8812 Val Loss: 0.4463319480419159, Val Acc: 0.8635\n","Epoch [88/500], Loss: 0.336210161447525, Train Acc: 0.8869,train F1-score:0.8815 Val Loss: 0.4391311705112457, Val Acc: 0.8635\n","Epoch [89/500], Loss: 0.3450370728969574, Train Acc: 0.8843,train F1-score:0.8792 Val Loss: 0.4305284023284912, Val Acc: 0.8688\n","Epoch [90/500], Loss: 0.33933496475219727, Train Acc: 0.8868,train F1-score:0.8831 Val Loss: 0.4309306740760803, Val Acc: 0.8635\n","Epoch [91/500], Loss: 0.32977479696273804, Train Acc: 0.8905,train F1-score:0.8861 Val Loss: 0.4377322793006897, Val Acc: 0.8661\n","Epoch [92/500], Loss: 0.3279435634613037, Train Acc: 0.8892,train F1-score:0.8844 Val Loss: 0.43631210923194885, Val Acc: 0.8715\n","Epoch [93/500], Loss: 0.3317330479621887, Train Acc: 0.8876,train F1-score:0.8827 Val Loss: 0.4320431053638458, Val Acc: 0.8675\n","Epoch [94/500], Loss: 0.3254082500934601, Train Acc: 0.8909,train F1-score:0.8865 Val Loss: 0.4304388463497162, Val Acc: 0.8715\n","Epoch [95/500], Loss: 0.327780544757843, Train Acc: 0.8887,train F1-score:0.8850 Val Loss: 0.4316257834434509, Val Acc: 0.8728\n","Epoch [96/500], Loss: 0.32361844182014465, Train Acc: 0.8921,train F1-score:0.8875 Val Loss: 0.43652671575546265, Val Acc: 0.8688\n","Epoch [97/500], Loss: 0.3262380063533783, Train Acc: 0.8901,train F1-score:0.8852 Val Loss: 0.4381427764892578, Val Acc: 0.8675\n","Epoch [98/500], Loss: 0.31715917587280273, Train Acc: 0.8949,train F1-score:0.8905 Val Loss: 0.43172580003738403, Val Acc: 0.8742\n","Epoch [99/500], Loss: 0.31322598457336426, Train Acc: 0.8975,train F1-score:0.8930 Val Loss: 0.4290684759616852, Val Acc: 0.8715\n","Epoch [100/500], Loss: 0.31987014412879944, Train Acc: 0.8939,train F1-score:0.8894 Val Loss: 0.4303807020187378, Val Acc: 0.8701\n","Epoch [101/500], Loss: 0.3205602169036865, Train Acc: 0.8899,train F1-score:0.8857 Val Loss: 0.4249226748943329, Val Acc: 0.8768\n","Epoch [102/500], Loss: 0.3161791265010834, Train Acc: 0.8939,train F1-score:0.8898 Val Loss: 0.4216129183769226, Val Acc: 0.8701\n","Epoch [103/500], Loss: 0.3160785436630249, Train Acc: 0.8945,train F1-score:0.8906 Val Loss: 0.4121061861515045, Val Acc: 0.8795\n","Epoch [104/500], Loss: 0.3136419653892517, Train Acc: 0.8940,train F1-score:0.8897 Val Loss: 0.4061271846294403, Val Acc: 0.8809\n","Epoch [105/500], Loss: 0.30730530619621277, Train Acc: 0.8950,train F1-score:0.8910 Val Loss: 0.40598803758621216, Val Acc: 0.8755\n","Epoch [106/500], Loss: 0.3120751976966858, Train Acc: 0.8974,train F1-score:0.8933 Val Loss: 0.41181743144989014, Val Acc: 0.8768\n","Epoch [107/500], Loss: 0.30713117122650146, Train Acc: 0.8971,train F1-score:0.8928 Val Loss: 0.41856643557548523, Val Acc: 0.8795\n","Epoch [108/500], Loss: 0.31285250186920166, Train Acc: 0.8957,train F1-score:0.8912 Val Loss: 0.40764540433883667, Val Acc: 0.8862\n","Epoch [109/500], Loss: 0.3025436997413635, Train Acc: 0.8981,train F1-score:0.8936 Val Loss: 0.4008631408214569, Val Acc: 0.8862\n","Epoch [110/500], Loss: 0.30529913306236267, Train Acc: 0.8969,train F1-score:0.8930 Val Loss: 0.40218421816825867, Val Acc: 0.8849\n","Epoch [111/500], Loss: 0.2984052896499634, Train Acc: 0.8993,train F1-score:0.8955 Val Loss: 0.40638452768325806, Val Acc: 0.8849\n","Epoch [112/500], Loss: 0.3078913986682892, Train Acc: 0.8950,train F1-score:0.8907 Val Loss: 0.41079047322273254, Val Acc: 0.8849\n","Epoch [113/500], Loss: 0.3014199435710907, Train Acc: 0.9002,train F1-score:0.8958 Val Loss: 0.4139769971370697, Val Acc: 0.8889\n","Epoch [114/500], Loss: 0.2912967801094055, Train Acc: 0.9009,train F1-score:0.8969 Val Loss: 0.4145388603210449, Val Acc: 0.8862\n","Epoch [115/500], Loss: 0.2973853647708893, Train Acc: 0.9028,train F1-score:0.8993 Val Loss: 0.4130447506904602, Val Acc: 0.8822\n","Epoch [116/500], Loss: 0.29567039012908936, Train Acc: 0.9018,train F1-score:0.8981 Val Loss: 0.411004900932312, Val Acc: 0.8822\n","Epoch [117/500], Loss: 0.29673343896865845, Train Acc: 0.9017,train F1-score:0.8978 Val Loss: 0.40688976645469666, Val Acc: 0.8902\n","Epoch [118/500], Loss: 0.2916121184825897, Train Acc: 0.9020,train F1-score:0.8981 Val Loss: 0.4055670499801636, Val Acc: 0.8902\n","Epoch [119/500], Loss: 0.2952554523944855, Train Acc: 0.9009,train F1-score:0.8971 Val Loss: 0.406244695186615, Val Acc: 0.8889\n","Epoch [120/500], Loss: 0.2880866229534149, Train Acc: 0.9022,train F1-score:0.8986 Val Loss: 0.4077540636062622, Val Acc: 0.8835\n","Epoch [121/500], Loss: 0.28622257709503174, Train Acc: 0.9052,train F1-score:0.9018 Val Loss: 0.412321001291275, Val Acc: 0.8876\n","Epoch [122/500], Loss: 0.2846432626247406, Train Acc: 0.9045,train F1-score:0.9003 Val Loss: 0.4110060930252075, Val Acc: 0.8902\n","Epoch [123/500], Loss: 0.28519824147224426, Train Acc: 0.9042,train F1-score:0.9002 Val Loss: 0.4110686779022217, Val Acc: 0.8942\n","Epoch [124/500], Loss: 0.28140994906425476, Train Acc: 0.9047,train F1-score:0.9014 Val Loss: 0.4075968265533447, Val Acc: 0.8902\n","Epoch [125/500], Loss: 0.2870929539203644, Train Acc: 0.9024,train F1-score:0.8987 Val Loss: 0.4049850404262543, Val Acc: 0.8889\n","Epoch [126/500], Loss: 0.2785009443759918, Train Acc: 0.9060,train F1-score:0.9019 Val Loss: 0.39901623129844666, Val Acc: 0.8862\n","Epoch [127/500], Loss: 0.2842932939529419, Train Acc: 0.9024,train F1-score:0.8985 Val Loss: 0.3925184905529022, Val Acc: 0.8862\n","Epoch [128/500], Loss: 0.2724215090274811, Train Acc: 0.9087,train F1-score:0.9055 Val Loss: 0.39527246356010437, Val Acc: 0.8902\n","Epoch [129/500], Loss: 0.2802075743675232, Train Acc: 0.9070,train F1-score:0.9033 Val Loss: 0.40375441312789917, Val Acc: 0.8902\n","Epoch [130/500], Loss: 0.27596333622932434, Train Acc: 0.9080,train F1-score:0.9041 Val Loss: 0.4035258889198303, Val Acc: 0.8889\n","Epoch [131/500], Loss: 0.2772611379623413, Train Acc: 0.9048,train F1-score:0.9012 Val Loss: 0.3889702558517456, Val Acc: 0.8956\n","Epoch [132/500], Loss: 0.2746940851211548, Train Acc: 0.9069,train F1-score:0.9035 Val Loss: 0.38313886523246765, Val Acc: 0.8996\n","Epoch [133/500], Loss: 0.2734204828739166, Train Acc: 0.9087,train F1-score:0.9057 Val Loss: 0.39214083552360535, Val Acc: 0.8889\n","Epoch [134/500], Loss: 0.2707717716693878, Train Acc: 0.9094,train F1-score:0.9059 Val Loss: 0.3906704783439636, Val Acc: 0.8889\n","Epoch [135/500], Loss: 0.26495298743247986, Train Acc: 0.9122,train F1-score:0.9085 Val Loss: 0.38910219073295593, Val Acc: 0.8902\n","Epoch [136/500], Loss: 0.27123376727104187, Train Acc: 0.9081,train F1-score:0.9047 Val Loss: 0.38688409328460693, Val Acc: 0.8956\n","Epoch [137/500], Loss: 0.2669745981693268, Train Acc: 0.9096,train F1-score:0.9069 Val Loss: 0.3926670253276825, Val Acc: 0.8902\n","Epoch [138/500], Loss: 0.26647111773490906, Train Acc: 0.9079,train F1-score:0.9050 Val Loss: 0.39567604660987854, Val Acc: 0.8889\n","Epoch [139/500], Loss: 0.2648821175098419, Train Acc: 0.9100,train F1-score:0.9061 Val Loss: 0.3909008800983429, Val Acc: 0.8902\n","Epoch [140/500], Loss: 0.26753613352775574, Train Acc: 0.9097,train F1-score:0.9061 Val Loss: 0.39039918780326843, Val Acc: 0.8849\n","Epoch [141/500], Loss: 0.2675611972808838, Train Acc: 0.9115,train F1-score:0.9083 Val Loss: 0.3931322693824768, Val Acc: 0.8849\n","Epoch [142/500], Loss: 0.2640591263771057, Train Acc: 0.9111,train F1-score:0.9080 Val Loss: 0.39566561579704285, Val Acc: 0.8835\n","Epoch [143/500], Loss: 0.2609352767467499, Train Acc: 0.9113,train F1-score:0.9079 Val Loss: 0.3977956473827362, Val Acc: 0.8822\n","Epoch [144/500], Loss: 0.26605233550071716, Train Acc: 0.9119,train F1-score:0.9084 Val Loss: 0.39081108570098877, Val Acc: 0.8835\n","Epoch [145/500], Loss: 0.2608031630516052, Train Acc: 0.9121,train F1-score:0.9091 Val Loss: 0.38490208983421326, Val Acc: 0.8809\n","Epoch [146/500], Loss: 0.2589239478111267, Train Acc: 0.9134,train F1-score:0.9106 Val Loss: 0.3842209279537201, Val Acc: 0.8822\n","Epoch [147/500], Loss: 0.2660251557826996, Train Acc: 0.9092,train F1-score:0.9063 Val Loss: 0.38917505741119385, Val Acc: 0.8849\n","Epoch [148/500], Loss: 0.2671397626399994, Train Acc: 0.9114,train F1-score:0.9082 Val Loss: 0.3858453333377838, Val Acc: 0.8862\n","Epoch [149/500], Loss: 0.26162269711494446, Train Acc: 0.9114,train F1-score:0.9079 Val Loss: 0.3712325990200043, Val Acc: 0.8956\n","Epoch [150/500], Loss: 0.26121440529823303, Train Acc: 0.9125,train F1-score:0.9097 Val Loss: 0.37203249335289, Val Acc: 0.8956\n","Epoch [151/500], Loss: 0.2666861414909363, Train Acc: 0.9143,train F1-score:0.9116 Val Loss: 0.378513902425766, Val Acc: 0.8969\n","Epoch [152/500], Loss: 0.2516367435455322, Train Acc: 0.9143,train F1-score:0.9114 Val Loss: 0.3839672803878784, Val Acc: 0.8956\n","Epoch [153/500], Loss: 0.2562408447265625, Train Acc: 0.9134,train F1-score:0.9099 Val Loss: 0.37897756695747375, Val Acc: 0.8929\n","Epoch [154/500], Loss: 0.26217636466026306, Train Acc: 0.9137,train F1-score:0.9104 Val Loss: 0.36796244978904724, Val Acc: 0.9036\n","Epoch [155/500], Loss: 0.2502950429916382, Train Acc: 0.9166,train F1-score:0.9133 Val Loss: 0.3656436502933502, Val Acc: 0.8983\n","Epoch [156/500], Loss: 0.2475571185350418, Train Acc: 0.9164,train F1-score:0.9132 Val Loss: 0.3750530183315277, Val Acc: 0.8929\n","Epoch [157/500], Loss: 0.2507987320423126, Train Acc: 0.9163,train F1-score:0.9133 Val Loss: 0.3815176486968994, Val Acc: 0.8956\n","Epoch [158/500], Loss: 0.2573612630367279, Train Acc: 0.9144,train F1-score:0.9108 Val Loss: 0.3836787939071655, Val Acc: 0.8983\n","Epoch [159/500], Loss: 0.2566554844379425, Train Acc: 0.9122,train F1-score:0.9088 Val Loss: 0.3857439160346985, Val Acc: 0.8942\n","Epoch [160/500], Loss: 0.25980356335639954, Train Acc: 0.9098,train F1-score:0.9071 Val Loss: 0.37721338868141174, Val Acc: 0.8956\n","Epoch [161/500], Loss: 0.25091198086738586, Train Acc: 0.9157,train F1-score:0.9130 Val Loss: 0.37812623381614685, Val Acc: 0.8942\n","Epoch [162/500], Loss: 0.25484180450439453, Train Acc: 0.9146,train F1-score:0.9113 Val Loss: 0.3762347102165222, Val Acc: 0.8916\n","Epoch [163/500], Loss: 0.2524583339691162, Train Acc: 0.9159,train F1-score:0.9130 Val Loss: 0.37664803862571716, Val Acc: 0.8942\n","Epoch [164/500], Loss: 0.2518337070941925, Train Acc: 0.9143,train F1-score:0.9114 Val Loss: 0.3771425485610962, Val Acc: 0.8969\n","Epoch [165/500], Loss: 0.250948965549469, Train Acc: 0.9170,train F1-score:0.9143 Val Loss: 0.3866921067237854, Val Acc: 0.8956\n","Epoch [166/500], Loss: 0.25869399309158325, Train Acc: 0.9128,train F1-score:0.9093 Val Loss: 0.39324432611465454, Val Acc: 0.8889\n","Epoch [167/500], Loss: 0.25163525342941284, Train Acc: 0.9126,train F1-score:0.9092 Val Loss: 0.387615442276001, Val Acc: 0.8929\n","Epoch [168/500], Loss: 0.24537937343120575, Train Acc: 0.9170,train F1-score:0.9140 Val Loss: 0.38275304436683655, Val Acc: 0.8916\n","Epoch [169/500], Loss: 0.2431686818599701, Train Acc: 0.9157,train F1-score:0.9127 Val Loss: 0.38113489747047424, Val Acc: 0.8956\n","Epoch [170/500], Loss: 0.24463443458080292, Train Acc: 0.9159,train F1-score:0.9130 Val Loss: 0.38108259439468384, Val Acc: 0.8929\n","Epoch [171/500], Loss: 0.24160060286521912, Train Acc: 0.9200,train F1-score:0.9175 Val Loss: 0.37781283259391785, Val Acc: 0.8889\n","Epoch [172/500], Loss: 0.24851743876934052, Train Acc: 0.9135,train F1-score:0.9104 Val Loss: 0.3676737844944, Val Acc: 0.8916\n","Epoch [173/500], Loss: 0.2398795336484909, Train Acc: 0.9216,train F1-score:0.9190 Val Loss: 0.36162859201431274, Val Acc: 0.8983\n","Epoch [174/500], Loss: 0.245436429977417, Train Acc: 0.9175,train F1-score:0.9150 Val Loss: 0.3658633530139923, Val Acc: 0.9036\n","Epoch [175/500], Loss: 0.23737825453281403, Train Acc: 0.9203,train F1-score:0.9173 Val Loss: 0.37242448329925537, Val Acc: 0.8996\n","Epoch [176/500], Loss: 0.2403661012649536, Train Acc: 0.9202,train F1-score:0.9171 Val Loss: 0.37609994411468506, Val Acc: 0.9009\n","Epoch [177/500], Loss: 0.24298910796642303, Train Acc: 0.9175,train F1-score:0.9143 Val Loss: 0.3741270899772644, Val Acc: 0.8969\n","Epoch [178/500], Loss: 0.23844903707504272, Train Acc: 0.9217,train F1-score:0.9189 Val Loss: 0.3730778098106384, Val Acc: 0.8969\n","Epoch [179/500], Loss: 0.2383195012807846, Train Acc: 0.9183,train F1-score:0.9157 Val Loss: 0.37400755286216736, Val Acc: 0.8969\n","Epoch [180/500], Loss: 0.23682387173175812, Train Acc: 0.9192,train F1-score:0.9165 Val Loss: 0.37338995933532715, Val Acc: 0.8956\n","Epoch [181/500], Loss: 0.24138398468494415, Train Acc: 0.9163,train F1-score:0.9138 Val Loss: 0.37209105491638184, Val Acc: 0.9009\n","Epoch [182/500], Loss: 0.23276326060295105, Train Acc: 0.9197,train F1-score:0.9172 Val Loss: 0.3734704554080963, Val Acc: 0.9009\n","Epoch [183/500], Loss: 0.23824253678321838, Train Acc: 0.9231,train F1-score:0.9204 Val Loss: 0.3748641610145569, Val Acc: 0.9050\n","Epoch [184/500], Loss: 0.23882119357585907, Train Acc: 0.9195,train F1-score:0.9167 Val Loss: 0.3760741651058197, Val Acc: 0.8996\n","Epoch [185/500], Loss: 0.23672308027744293, Train Acc: 0.9194,train F1-score:0.9166 Val Loss: 0.37394025921821594, Val Acc: 0.8929\n","Epoch [186/500], Loss: 0.241326242685318, Train Acc: 0.9196,train F1-score:0.9167 Val Loss: 0.3706219792366028, Val Acc: 0.9063\n","Epoch [187/500], Loss: 0.23592793941497803, Train Acc: 0.9213,train F1-score:0.9184 Val Loss: 0.36928799748420715, Val Acc: 0.8996\n","Epoch [188/500], Loss: 0.23169103264808655, Train Acc: 0.9212,train F1-score:0.9186 Val Loss: 0.364620178937912, Val Acc: 0.8983\n","Epoch [189/500], Loss: 0.2365948110818863, Train Acc: 0.9215,train F1-score:0.9187 Val Loss: 0.3662548065185547, Val Acc: 0.8969\n","Epoch [190/500], Loss: 0.22666367888450623, Train Acc: 0.9232,train F1-score:0.9203 Val Loss: 0.3694290816783905, Val Acc: 0.8942\n","Epoch [191/500], Loss: 0.22195343673229218, Train Acc: 0.9227,train F1-score:0.9201 Val Loss: 0.3658501207828522, Val Acc: 0.8996\n","Epoch [192/500], Loss: 0.23337113857269287, Train Acc: 0.9226,train F1-score:0.9200 Val Loss: 0.36610421538352966, Val Acc: 0.8996\n","Epoch [193/500], Loss: 0.22680191695690155, Train Acc: 0.9215,train F1-score:0.9187 Val Loss: 0.36517706513404846, Val Acc: 0.8996\n","Epoch [194/500], Loss: 0.23138372600078583, Train Acc: 0.9236,train F1-score:0.9208 Val Loss: 0.3590075671672821, Val Acc: 0.8996\n","Epoch [195/500], Loss: 0.22383227944374084, Train Acc: 0.9228,train F1-score:0.9199 Val Loss: 0.35678374767303467, Val Acc: 0.9009\n","Epoch [196/500], Loss: 0.2266683578491211, Train Acc: 0.9252,train F1-score:0.9229 Val Loss: 0.35670414566993713, Val Acc: 0.9076\n","Epoch [197/500], Loss: 0.22866389155387878, Train Acc: 0.9214,train F1-score:0.9191 Val Loss: 0.3573167622089386, Val Acc: 0.9050\n","Epoch [198/500], Loss: 0.23060759902000427, Train Acc: 0.9230,train F1-score:0.9204 Val Loss: 0.35569313168525696, Val Acc: 0.9050\n","Epoch [199/500], Loss: 0.22553180158138275, Train Acc: 0.9233,train F1-score:0.9209 Val Loss: 0.35863879323005676, Val Acc: 0.8996\n","Epoch [200/500], Loss: 0.224322110414505, Train Acc: 0.9216,train F1-score:0.9188 Val Loss: 0.36520856618881226, Val Acc: 0.8969\n","Epoch [201/500], Loss: 0.22633026540279388, Train Acc: 0.9230,train F1-score:0.9201 Val Loss: 0.3680717349052429, Val Acc: 0.9023\n","Epoch [202/500], Loss: 0.22097168862819672, Train Acc: 0.9256,train F1-score:0.9227 Val Loss: 0.36678406596183777, Val Acc: 0.9023\n","Epoch [203/500], Loss: 0.22372116148471832, Train Acc: 0.9232,train F1-score:0.9203 Val Loss: 0.35834845900535583, Val Acc: 0.9009\n","Epoch [204/500], Loss: 0.22104743123054504, Train Acc: 0.9262,train F1-score:0.9238 Val Loss: 0.35284873843193054, Val Acc: 0.9036\n","Epoch [205/500], Loss: 0.2204502820968628, Train Acc: 0.9250,train F1-score:0.9225 Val Loss: 0.354792982339859, Val Acc: 0.8956\n","Epoch [206/500], Loss: 0.2175169736146927, Train Acc: 0.9257,train F1-score:0.9227 Val Loss: 0.35469064116477966, Val Acc: 0.9009\n","Epoch [207/500], Loss: 0.2259020060300827, Train Acc: 0.9223,train F1-score:0.9194 Val Loss: 0.3525231182575226, Val Acc: 0.9009\n","Epoch [208/500], Loss: 0.21985134482383728, Train Acc: 0.9250,train F1-score:0.9220 Val Loss: 0.3551217317581177, Val Acc: 0.9036\n","Epoch [209/500], Loss: 0.2202894687652588, Train Acc: 0.9233,train F1-score:0.9210 Val Loss: 0.3566955029964447, Val Acc: 0.9050\n","Epoch [210/500], Loss: 0.22204841673374176, Train Acc: 0.9259,train F1-score:0.9235 Val Loss: 0.35742607712745667, Val Acc: 0.9023\n","Epoch [211/500], Loss: 0.2166244089603424, Train Acc: 0.9241,train F1-score:0.9219 Val Loss: 0.35359251499176025, Val Acc: 0.9009\n","Epoch [212/500], Loss: 0.22090841829776764, Train Acc: 0.9234,train F1-score:0.9208 Val Loss: 0.3549826443195343, Val Acc: 0.8996\n","Epoch [213/500], Loss: 0.2174714356660843, Train Acc: 0.9261,train F1-score:0.9242 Val Loss: 0.34825998544692993, Val Acc: 0.9036\n","Epoch [214/500], Loss: 0.21477654576301575, Train Acc: 0.9256,train F1-score:0.9234 Val Loss: 0.3456564247608185, Val Acc: 0.9009\n","Epoch [215/500], Loss: 0.22091861069202423, Train Acc: 0.9239,train F1-score:0.9211 Val Loss: 0.3380396068096161, Val Acc: 0.9076\n","Epoch [216/500], Loss: 0.21643002331256866, Train Acc: 0.9256,train F1-score:0.9233 Val Loss: 0.34355705976486206, Val Acc: 0.8996\n","Epoch [217/500], Loss: 0.21563470363616943, Train Acc: 0.9252,train F1-score:0.9233 Val Loss: 0.34525707364082336, Val Acc: 0.9050\n","Epoch [218/500], Loss: 0.22322645783424377, Train Acc: 0.9215,train F1-score:0.9187 Val Loss: 0.3526896834373474, Val Acc: 0.9063\n","Epoch [219/500], Loss: 0.2169973999261856, Train Acc: 0.9254,train F1-score:0.9226 Val Loss: 0.35733261704444885, Val Acc: 0.9023\n","Epoch [220/500], Loss: 0.2211955487728119, Train Acc: 0.9253,train F1-score:0.9225 Val Loss: 0.3579910397529602, Val Acc: 0.9036\n","Epoch [221/500], Loss: 0.2172318398952484, Train Acc: 0.9262,train F1-score:0.9238 Val Loss: 0.35098862648010254, Val Acc: 0.9023\n","Epoch [222/500], Loss: 0.21473319828510284, Train Acc: 0.9278,train F1-score:0.9253 Val Loss: 0.35181742906570435, Val Acc: 0.8969\n","Epoch [223/500], Loss: 0.21707169711589813, Train Acc: 0.9271,train F1-score:0.9243 Val Loss: 0.3523714542388916, Val Acc: 0.8996\n","Epoch [224/500], Loss: 0.2134406417608261, Train Acc: 0.9268,train F1-score:0.9243 Val Loss: 0.36279505491256714, Val Acc: 0.8996\n","Epoch [225/500], Loss: 0.21927401423454285, Train Acc: 0.9260,train F1-score:0.9235 Val Loss: 0.36698952317237854, Val Acc: 0.8983\n","Epoch [226/500], Loss: 0.20787711441516876, Train Acc: 0.9287,train F1-score:0.9266 Val Loss: 0.365048885345459, Val Acc: 0.8969\n","Epoch [227/500], Loss: 0.215356707572937, Train Acc: 0.9271,train F1-score:0.9250 Val Loss: 0.36536088585853577, Val Acc: 0.9023\n","Epoch [228/500], Loss: 0.2156282216310501, Train Acc: 0.9297,train F1-score:0.9273 Val Loss: 0.36642977595329285, Val Acc: 0.8983\n","Epoch [229/500], Loss: 0.21355244517326355, Train Acc: 0.9278,train F1-score:0.9254 Val Loss: 0.36346977949142456, Val Acc: 0.9023\n","Epoch [230/500], Loss: 0.2142065465450287, Train Acc: 0.9294,train F1-score:0.9270 Val Loss: 0.36768361926078796, Val Acc: 0.9023\n","Epoch [231/500], Loss: 0.21318021416664124, Train Acc: 0.9296,train F1-score:0.9278 Val Loss: 0.3749646544456482, Val Acc: 0.8983\n","Epoch [232/500], Loss: 0.21144786477088928, Train Acc: 0.9294,train F1-score:0.9277 Val Loss: 0.3862163722515106, Val Acc: 0.9023\n","Epoch [233/500], Loss: 0.206345796585083, Train Acc: 0.9291,train F1-score:0.9268 Val Loss: 0.3809551000595093, Val Acc: 0.9009\n","Epoch [234/500], Loss: 0.21107932925224304, Train Acc: 0.9278,train F1-score:0.9257 Val Loss: 0.3680637776851654, Val Acc: 0.8983\n","Epoch [235/500], Loss: 0.21326425671577454, Train Acc: 0.9270,train F1-score:0.9248 Val Loss: 0.36774471402168274, Val Acc: 0.8996\n","Epoch [236/500], Loss: 0.2131585329771042, Train Acc: 0.9290,train F1-score:0.9270 Val Loss: 0.369193971157074, Val Acc: 0.8929\n","Epoch [237/500], Loss: 0.2082066684961319, Train Acc: 0.9288,train F1-score:0.9265 Val Loss: 0.37440332770347595, Val Acc: 0.8929\n","Epoch [238/500], Loss: 0.21078816056251526, Train Acc: 0.9264,train F1-score:0.9235 Val Loss: 0.37110012769699097, Val Acc: 0.8983\n","Epoch [239/500], Loss: 0.2048017680644989, Train Acc: 0.9306,train F1-score:0.9282 Val Loss: 0.37248387932777405, Val Acc: 0.9050\n","Epoch [240/500], Loss: 0.2061057835817337, Train Acc: 0.9297,train F1-score:0.9275 Val Loss: 0.3701610267162323, Val Acc: 0.9036\n","Epoch [241/500], Loss: 0.20032459497451782, Train Acc: 0.9338,train F1-score:0.9320 Val Loss: 0.36793163418769836, Val Acc: 0.9076\n","Epoch [242/500], Loss: 0.2007419317960739, Train Acc: 0.9321,train F1-score:0.9299 Val Loss: 0.369734525680542, Val Acc: 0.9050\n","Epoch [243/500], Loss: 0.20605577528476715, Train Acc: 0.9298,train F1-score:0.9275 Val Loss: 0.36575835943222046, Val Acc: 0.8969\n","Epoch [244/500], Loss: 0.21079616248607635, Train Acc: 0.9288,train F1-score:0.9263 Val Loss: 0.35527199506759644, Val Acc: 0.9036\n","Epoch [245/500], Loss: 0.19992762804031372, Train Acc: 0.9317,train F1-score:0.9295 Val Loss: 0.3556947112083435, Val Acc: 0.9090\n","Epoch [246/500], Loss: 0.20336756110191345, Train Acc: 0.9322,train F1-score:0.9301 Val Loss: 0.3595331907272339, Val Acc: 0.9009\n","Epoch [247/500], Loss: 0.20344552397727966, Train Acc: 0.9284,train F1-score:0.9261 Val Loss: 0.3729722797870636, Val Acc: 0.9076\n","Epoch [248/500], Loss: 0.20631058514118195, Train Acc: 0.9272,train F1-score:0.9251 Val Loss: 0.3713047206401825, Val Acc: 0.9103\n","Epoch [249/500], Loss: 0.20849035680294037, Train Acc: 0.9275,train F1-score:0.9251 Val Loss: 0.37549102306365967, Val Acc: 0.9116\n","Epoch [250/500], Loss: 0.1986190527677536, Train Acc: 0.9319,train F1-score:0.9299 Val Loss: 0.3868381381034851, Val Acc: 0.9023\n","Epoch [251/500], Loss: 0.19936397671699524, Train Acc: 0.9325,train F1-score:0.9304 Val Loss: 0.3974590301513672, Val Acc: 0.9036\n","Epoch [252/500], Loss: 0.20526570081710815, Train Acc: 0.9303,train F1-score:0.9282 Val Loss: 0.39671188592910767, Val Acc: 0.9036\n","Epoch [253/500], Loss: 0.20812179148197174, Train Acc: 0.9276,train F1-score:0.9252 Val Loss: 0.39158180356025696, Val Acc: 0.9023\n","Epoch [254/500], Loss: 0.19905753433704376, Train Acc: 0.9314,train F1-score:0.9293 Val Loss: 0.3844156861305237, Val Acc: 0.9076\n","Epoch [255/500], Loss: 0.2013019621372223, Train Acc: 0.9322,train F1-score:0.9303 Val Loss: 0.38426268100738525, Val Acc: 0.9063\n","Epoch [256/500], Loss: 0.20507952570915222, Train Acc: 0.9274,train F1-score:0.9257 Val Loss: 0.36787495017051697, Val Acc: 0.9009\n","Epoch [257/500], Loss: 0.2068692147731781, Train Acc: 0.9303,train F1-score:0.9281 Val Loss: 0.3767693340778351, Val Acc: 0.9063\n","Epoch [258/500], Loss: 0.20560914278030396, Train Acc: 0.9298,train F1-score:0.9273 Val Loss: 0.3779163360595703, Val Acc: 0.9076\n","Epoch [259/500], Loss: 0.2027977705001831, Train Acc: 0.9287,train F1-score:0.9267 Val Loss: 0.37372660636901855, Val Acc: 0.9050\n","Epoch [260/500], Loss: 0.201216921210289, Train Acc: 0.9317,train F1-score:0.9297 Val Loss: 0.3776107728481293, Val Acc: 0.9023\n","Epoch [261/500], Loss: 0.2068442404270172, Train Acc: 0.9308,train F1-score:0.9290 Val Loss: 0.3704463839530945, Val Acc: 0.9050\n","Epoch [262/500], Loss: 0.19569964706897736, Train Acc: 0.9349,train F1-score:0.9331 Val Loss: 0.37770894169807434, Val Acc: 0.9076\n","Epoch [263/500], Loss: 0.20409196615219116, Train Acc: 0.9316,train F1-score:0.9294 Val Loss: 0.3809947967529297, Val Acc: 0.9050\n","Epoch [264/500], Loss: 0.20021700859069824, Train Acc: 0.9326,train F1-score:0.9303 Val Loss: 0.37941494584083557, Val Acc: 0.9023\n","Epoch [265/500], Loss: 0.1972273588180542, Train Acc: 0.9310,train F1-score:0.9291 Val Loss: 0.37089481949806213, Val Acc: 0.9023\n","Epoch [266/500], Loss: 0.20416219532489777, Train Acc: 0.9314,train F1-score:0.9294 Val Loss: 0.36694300174713135, Val Acc: 0.9009\n","Epoch [267/500], Loss: 0.19355542957782745, Train Acc: 0.9335,train F1-score:0.9317 Val Loss: 0.3725956082344055, Val Acc: 0.9023\n","Epoch [268/500], Loss: 0.2004861980676651, Train Acc: 0.9323,train F1-score:0.9301 Val Loss: 0.37834885716438293, Val Acc: 0.8983\n","Epoch [269/500], Loss: 0.19364984333515167, Train Acc: 0.9325,train F1-score:0.9302 Val Loss: 0.387323260307312, Val Acc: 0.8942\n","Epoch [270/500], Loss: 0.197533518075943, Train Acc: 0.9323,train F1-score:0.9300 Val Loss: 0.3815600275993347, Val Acc: 0.8969\n","Epoch [271/500], Loss: 0.19711977243423462, Train Acc: 0.9335,train F1-score:0.9315 Val Loss: 0.3725452721118927, Val Acc: 0.9050\n","Epoch [272/500], Loss: 0.18988092243671417, Train Acc: 0.9344,train F1-score:0.9326 Val Loss: 0.37122124433517456, Val Acc: 0.9036\n","Epoch [273/500], Loss: 0.19732877612113953, Train Acc: 0.9328,train F1-score:0.9306 Val Loss: 0.37003347277641296, Val Acc: 0.9036\n","Epoch [274/500], Loss: 0.196135476231575, Train Acc: 0.9331,train F1-score:0.9309 Val Loss: 0.36607077717781067, Val Acc: 0.9063\n","Epoch [275/500], Loss: 0.19039110839366913, Train Acc: 0.9339,train F1-score:0.9322 Val Loss: 0.37057244777679443, Val Acc: 0.9076\n","Epoch [276/500], Loss: 0.1968752145767212, Train Acc: 0.9346,train F1-score:0.9328 Val Loss: 0.3728443682193756, Val Acc: 0.9050\n","Epoch [277/500], Loss: 0.1832626760005951, Train Acc: 0.9365,train F1-score:0.9349 Val Loss: 0.3791213035583496, Val Acc: 0.9023\n","Epoch [278/500], Loss: 0.1899825781583786, Train Acc: 0.9335,train F1-score:0.9315 Val Loss: 0.38654106855392456, Val Acc: 0.9076\n","Epoch [279/500], Loss: 0.19191892445087433, Train Acc: 0.9347,train F1-score:0.9325 Val Loss: 0.3742251992225647, Val Acc: 0.9090\n","Epoch [280/500], Loss: 0.19281861186027527, Train Acc: 0.9330,train F1-score:0.9304 Val Loss: 0.3586640954017639, Val Acc: 0.9090\n","Epoch [281/500], Loss: 0.1919889897108078, Train Acc: 0.9352,train F1-score:0.9335 Val Loss: 0.35306158661842346, Val Acc: 0.9076\n","Epoch [282/500], Loss: 0.18800358474254608, Train Acc: 0.9368,train F1-score:0.9356 Val Loss: 0.35664793848991394, Val Acc: 0.9063\n","Epoch [283/500], Loss: 0.19727462530136108, Train Acc: 0.9308,train F1-score:0.9292 Val Loss: 0.3610153794288635, Val Acc: 0.9090\n","Epoch [284/500], Loss: 0.18785327672958374, Train Acc: 0.9361,train F1-score:0.9345 Val Loss: 0.36299821734428406, Val Acc: 0.9090\n","Epoch [285/500], Loss: 0.18712416291236877, Train Acc: 0.9367,train F1-score:0.9348 Val Loss: 0.3617004156112671, Val Acc: 0.9036\n","Epoch [286/500], Loss: 0.18730026483535767, Train Acc: 0.9349,train F1-score:0.9333 Val Loss: 0.36038926243782043, Val Acc: 0.9063\n","Epoch [287/500], Loss: 0.19095951318740845, Train Acc: 0.9358,train F1-score:0.9343 Val Loss: 0.36322760581970215, Val Acc: 0.9063\n","Epoch [288/500], Loss: 0.18802021443843842, Train Acc: 0.9361,train F1-score:0.9345 Val Loss: 0.3696574568748474, Val Acc: 0.9076\n","Epoch [289/500], Loss: 0.19102485477924347, Train Acc: 0.9340,train F1-score:0.9322 Val Loss: 0.3759799599647522, Val Acc: 0.9103\n","Epoch [290/500], Loss: 0.18389533460140228, Train Acc: 0.9373,train F1-score:0.9355 Val Loss: 0.3792712688446045, Val Acc: 0.9076\n","Epoch [291/500], Loss: 0.18681879341602325, Train Acc: 0.9361,train F1-score:0.9344 Val Loss: 0.37263891100883484, Val Acc: 0.9090\n","Epoch [292/500], Loss: 0.19063684344291687, Train Acc: 0.9343,train F1-score:0.9324 Val Loss: 0.3678813576698303, Val Acc: 0.9036\n","Epoch [293/500], Loss: 0.1918812245130539, Train Acc: 0.9348,train F1-score:0.9329 Val Loss: 0.36027535796165466, Val Acc: 0.8996\n","Epoch [294/500], Loss: 0.18970663845539093, Train Acc: 0.9341,train F1-score:0.9323 Val Loss: 0.35232436656951904, Val Acc: 0.9063\n","Epoch [295/500], Loss: 0.18998900055885315, Train Acc: 0.9355,train F1-score:0.9337 Val Loss: 0.3515382409095764, Val Acc: 0.9050\n","Epoch [296/500], Loss: 0.18424338102340698, Train Acc: 0.9381,train F1-score:0.9366 Val Loss: 0.35488590598106384, Val Acc: 0.9063\n","Epoch [297/500], Loss: 0.18777260184288025, Train Acc: 0.9358,train F1-score:0.9339 Val Loss: 0.35720160603523254, Val Acc: 0.9076\n","Epoch [298/500], Loss: 0.18481561541557312, Train Acc: 0.9358,train F1-score:0.9337 Val Loss: 0.3545822203159332, Val Acc: 0.9076\n","Epoch [299/500], Loss: 0.18953895568847656, Train Acc: 0.9344,train F1-score:0.9325 Val Loss: 0.3505406379699707, Val Acc: 0.9076\n","Epoch [300/500], Loss: 0.1827026903629303, Train Acc: 0.9382,train F1-score:0.9363 Val Loss: 0.3505644202232361, Val Acc: 0.9050\n","Epoch [301/500], Loss: 0.18743851780891418, Train Acc: 0.9356,train F1-score:0.9338 Val Loss: 0.35669395327568054, Val Acc: 0.9009\n","Epoch [302/500], Loss: 0.18347668647766113, Train Acc: 0.9361,train F1-score:0.9343 Val Loss: 0.3541122376918793, Val Acc: 0.9050\n","Epoch [303/500], Loss: 0.17744742333889008, Train Acc: 0.9373,train F1-score:0.9354 Val Loss: 0.3497636318206787, Val Acc: 0.9009\n","Epoch [304/500], Loss: 0.1904553771018982, Train Acc: 0.9381,train F1-score:0.9362 Val Loss: 0.3451628088951111, Val Acc: 0.9023\n","Epoch [305/500], Loss: 0.1798187792301178, Train Acc: 0.9376,train F1-score:0.9360 Val Loss: 0.3461776375770569, Val Acc: 0.9036\n","Epoch [306/500], Loss: 0.18342483043670654, Train Acc: 0.9368,train F1-score:0.9351 Val Loss: 0.3493596017360687, Val Acc: 0.9023\n","Epoch [307/500], Loss: 0.18313303589820862, Train Acc: 0.9355,train F1-score:0.9341 Val Loss: 0.35753345489501953, Val Acc: 0.9063\n","Epoch [308/500], Loss: 0.18520556390285492, Train Acc: 0.9351,train F1-score:0.9333 Val Loss: 0.35610005259513855, Val Acc: 0.9063\n","Epoch [309/500], Loss: 0.19106830656528473, Train Acc: 0.9364,train F1-score:0.9349 Val Loss: 0.3590875267982483, Val Acc: 0.9090\n","Epoch [310/500], Loss: 0.18109601736068726, Train Acc: 0.9356,train F1-score:0.9340 Val Loss: 0.3666083514690399, Val Acc: 0.9090\n","Epoch [311/500], Loss: 0.18755903840065002, Train Acc: 0.9349,train F1-score:0.9331 Val Loss: 0.37183329463005066, Val Acc: 0.9023\n","Epoch [312/500], Loss: 0.17876219749450684, Train Acc: 0.9395,train F1-score:0.9374 Val Loss: 0.3674166798591614, Val Acc: 0.9036\n","Epoch [313/500], Loss: 0.17817480862140656, Train Acc: 0.9393,train F1-score:0.9374 Val Loss: 0.3623672425746918, Val Acc: 0.9036\n","Epoch [314/500], Loss: 0.18728546798229218, Train Acc: 0.9370,train F1-score:0.9355 Val Loss: 0.3592658042907715, Val Acc: 0.9036\n","Early stopping at epoch 314\n","Test Loss: 0.7374473810195923, Test Accuracy: 0.8739946380697051\n","Precision: 0.8712, Recall: 0.8740, F1-score: 0.8683\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT_90/edge_index_train.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT_90/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT_90/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT_90/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT_90/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT_90/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT_90/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT_90/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT_90/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GATNet(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_dim, out_channels, num_layers):\n","        super(GATNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GATConv(in_channels, hidden_dim, heads=8))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GATConv(hidden_dim * 8, hidden_dim, heads=8))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim * 8, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.fc(x)\n","        return x\n","\n","# Define model\n","model = GATNet(in_channels=X_train.shape[1], hidden_dim=32, out_channels=13, num_layers=2).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZUV1ypEUMMv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2101,"status":"ok","timestamp":1714810136808,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"sZJciy2xhiKJ","outputId":"53ca8a3b-9568-420c-bd82-fcba800e14d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.7121697059918124\n","Validation Accuracy (Decision Tree): 0.6907630522088354\n","Test Accuracy (Decision Tree): 0.6796246648793566\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=1)\n","decision_tree.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(train_outputs)\n","val_pred = decision_tree.predict(val_outputs)\n","test_pred = decision_tree.predict(test_outputs)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1838,"status":"ok","timestamp":1714810142189,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"bn2qG0LHhwUN","outputId":"af474aa0-13af-4c30-c929-f7bf4d8ebe4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.7733531819873465\n","Validation Accuracy (Decision Tree): 0.7295850066934404\n","Test Accuracy (Decision Tree): 0.7144772117962467\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Concatenate the GIN model output with the original feature matrices\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=2)\n","decision_tree.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(X_train_combined)\n","val_pred = decision_tree.predict(X_val_combined)\n","test_pred = decision_tree.predict(X_test_combined)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"BDVZuO2mimyx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714810166606,"user_tz":-330,"elapsed":11241,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"4c46c47e-1cb0-4e56-dbac-9078ea23e8c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9560848529959062\n","Validation Accuracy (SVM): 0.9089692101740294\n","Test Accuracy (SVM): 0.8699731903485255\n"]}],"source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(train_outputs)\n","val_pred = svm_classifier.predict(val_outputs)\n","test_pred = svm_classifier.predict(test_outputs)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAbAq_fciw4W"},"outputs":[],"source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Combine the output of the GIN model with the original features\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(X_train_combined)\n","val_pred = svm_classifier.predict(X_val_combined)\n","test_pred = svm_classifier.predict(X_test_combined)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1G7cH2KvNhFQ-zXs0dyaL7TR2m9zbI8hr","timestamp":1712729723829}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}