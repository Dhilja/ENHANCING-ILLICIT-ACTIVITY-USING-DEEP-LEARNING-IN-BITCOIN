{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27371,"status":"ok","timestamp":1714298466179,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"0A6-9RE9bEIq","outputId":"ffa94266-b85e-4f2f-8a4b-f569e570704d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14336,"status":"ok","timestamp":1714298480512,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"yVrzewfGj2il","outputId":"eeff6b14-4a3c-4a6f-8d84-1cda1cdf0640"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.5.3\n"]}],"source":["!pip install torch-geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27484,"status":"ok","timestamp":1714298753008,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"oVf_tDZLjS7h","outputId":"673ce04b-72a8-4365-9d17-36a847481885"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.980665683746338, Train Acc: 0.0461,train F1-score:0.0721 Val Loss: 2.6787383556365967, Val Acc: 0.0569\n","Epoch [2/500], Loss: 2.7581398487091064, Train Acc: 0.1293,train F1-score:0.1937 Val Loss: 2.5343284606933594, Val Acc: 0.1278\n","Epoch [3/500], Loss: 2.587641954421997, Train Acc: 0.1816,train F1-score:0.2523 Val Loss: 2.414271354675293, Val Acc: 0.2356\n","Epoch [4/500], Loss: 2.4727580547332764, Train Acc: 0.2320,train F1-score:0.3046 Val Loss: 2.307441234588623, Val Acc: 0.4431\n","Epoch [5/500], Loss: 2.3718323707580566, Train Acc: 0.3377,train F1-score:0.3921 Val Loss: 2.208617925643921, Val Acc: 0.5716\n","Epoch [6/500], Loss: 2.29046368598938, Train Acc: 0.4080,train F1-score:0.4408 Val Loss: 2.1170504093170166, Val Acc: 0.6238\n","Epoch [7/500], Loss: 2.2236833572387695, Train Acc: 0.4305,train F1-score:0.4612 Val Loss: 2.026702880859375, Val Acc: 0.6212\n","Epoch [8/500], Loss: 2.1559863090515137, Train Acc: 0.4404,train F1-score:0.4614 Val Loss: 1.9377758502960205, Val Acc: 0.6225\n","Epoch [9/500], Loss: 2.0825486183166504, Train Acc: 0.4594,train F1-score:0.4774 Val Loss: 1.8529621362686157, Val Acc: 0.6245\n","Epoch [10/500], Loss: 2.021040201187134, Train Acc: 0.4734,train F1-score:0.4802 Val Loss: 1.772696852684021, Val Acc: 0.6278\n","Epoch [11/500], Loss: 1.9861979484558105, Train Acc: 0.4740,train F1-score:0.4762 Val Loss: 1.6974838972091675, Val Acc: 0.6365\n","Epoch [12/500], Loss: 1.9163193702697754, Train Acc: 0.4909,train F1-score:0.4868 Val Loss: 1.6270729303359985, Val Acc: 0.6432\n","Epoch [13/500], Loss: 1.8835612535476685, Train Acc: 0.4930,train F1-score:0.4868 Val Loss: 1.561211109161377, Val Acc: 0.6513\n","Epoch [14/500], Loss: 1.8372786045074463, Train Acc: 0.5009,train F1-score:0.4911 Val Loss: 1.498964786529541, Val Acc: 0.6580\n","Epoch [15/500], Loss: 1.8064221143722534, Train Acc: 0.4997,train F1-score:0.4897 Val Loss: 1.4397906064987183, Val Acc: 0.6667\n","Epoch [16/500], Loss: 1.7542779445648193, Train Acc: 0.5141,train F1-score:0.4987 Val Loss: 1.3832554817199707, Val Acc: 0.6780\n","Epoch [17/500], Loss: 1.7189505100250244, Train Acc: 0.5155,train F1-score:0.5000 Val Loss: 1.3299518823623657, Val Acc: 0.6854\n","Epoch [18/500], Loss: 1.6849678754806519, Train Acc: 0.5267,train F1-score:0.5074 Val Loss: 1.2805626392364502, Val Acc: 0.6894\n","Epoch [19/500], Loss: 1.6554538011550903, Train Acc: 0.5338,train F1-score:0.5141 Val Loss: 1.2359702587127686, Val Acc: 0.6901\n","Epoch [20/500], Loss: 1.6321913003921509, Train Acc: 0.5264,train F1-score:0.5104 Val Loss: 1.1969050168991089, Val Acc: 0.6928\n","Epoch [21/500], Loss: 1.612636685371399, Train Acc: 0.5320,train F1-score:0.5128 Val Loss: 1.1629152297973633, Val Acc: 0.6954\n","Epoch [22/500], Loss: 1.5777934789657593, Train Acc: 0.5426,train F1-score:0.5236 Val Loss: 1.1332283020019531, Val Acc: 0.6975\n","Epoch [23/500], Loss: 1.5413076877593994, Train Acc: 0.5451,train F1-score:0.5249 Val Loss: 1.1072957515716553, Val Acc: 0.7015\n","Epoch [24/500], Loss: 1.5421415567398071, Train Acc: 0.5471,train F1-score:0.5275 Val Loss: 1.0849618911743164, Val Acc: 0.7001\n","Epoch [25/500], Loss: 1.516646385192871, Train Acc: 0.5570,train F1-score:0.5359 Val Loss: 1.0657532215118408, Val Acc: 0.7015\n","Epoch [26/500], Loss: 1.503256916999817, Train Acc: 0.5547,train F1-score:0.5343 Val Loss: 1.0493167638778687, Val Acc: 0.7028\n","Epoch [27/500], Loss: 1.4872446060180664, Train Acc: 0.5507,train F1-score:0.5297 Val Loss: 1.0353745222091675, Val Acc: 0.7035\n","Epoch [28/500], Loss: 1.4636787176132202, Train Acc: 0.5586,train F1-score:0.5383 Val Loss: 1.0236765146255493, Val Acc: 0.7062\n","Epoch [29/500], Loss: 1.4522439241409302, Train Acc: 0.5658,train F1-score:0.5418 Val Loss: 1.014413833618164, Val Acc: 0.7082\n","Epoch [30/500], Loss: 1.4270129203796387, Train Acc: 0.5667,train F1-score:0.5435 Val Loss: 1.0067827701568604, Val Acc: 0.7115\n","Epoch [31/500], Loss: 1.4048978090286255, Train Acc: 0.5802,train F1-score:0.5531 Val Loss: 1.0007621049880981, Val Acc: 0.7135\n","Epoch [32/500], Loss: 1.392966866493225, Train Acc: 0.5746,train F1-score:0.5484 Val Loss: 0.996284544467926, Val Acc: 0.7135\n","Epoch [33/500], Loss: 1.383754849433899, Train Acc: 0.5790,train F1-score:0.5533 Val Loss: 0.9931568503379822, Val Acc: 0.7129\n","Epoch [34/500], Loss: 1.3710945844650269, Train Acc: 0.5893,train F1-score:0.5590 Val Loss: 0.9906421899795532, Val Acc: 0.7175\n","Epoch [35/500], Loss: 1.3652417659759521, Train Acc: 0.5933,train F1-score:0.5613 Val Loss: 0.9885297417640686, Val Acc: 0.7175\n","Epoch [36/500], Loss: 1.3592522144317627, Train Acc: 0.5965,train F1-score:0.5658 Val Loss: 0.9853348135948181, Val Acc: 0.7202\n","Epoch [37/500], Loss: 1.3328702449798584, Train Acc: 0.6046,train F1-score:0.5733 Val Loss: 0.9816355109214783, Val Acc: 0.7242\n","Epoch [38/500], Loss: 1.3209201097488403, Train Acc: 0.6070,train F1-score:0.5755 Val Loss: 0.9773527979850769, Val Acc: 0.7410\n","Epoch [39/500], Loss: 1.3214226961135864, Train Acc: 0.6074,train F1-score:0.5772 Val Loss: 0.9728294014930725, Val Acc: 0.7450\n","Epoch [40/500], Loss: 1.3120094537734985, Train Acc: 0.6083,train F1-score:0.5756 Val Loss: 0.9681817293167114, Val Acc: 0.7477\n","Epoch [41/500], Loss: 1.2949509620666504, Train Acc: 0.6151,train F1-score:0.5833 Val Loss: 0.9631348252296448, Val Acc: 0.7477\n","Epoch [42/500], Loss: 1.2887355089187622, Train Acc: 0.6184,train F1-score:0.5827 Val Loss: 0.957909345626831, Val Acc: 0.7470\n","Epoch [43/500], Loss: 1.302162766456604, Train Acc: 0.6144,train F1-score:0.5831 Val Loss: 0.9527929425239563, Val Acc: 0.7497\n","Epoch [44/500], Loss: 1.2833058834075928, Train Acc: 0.6206,train F1-score:0.5899 Val Loss: 0.9475276470184326, Val Acc: 0.7483\n","Epoch [45/500], Loss: 1.2965730428695679, Train Acc: 0.6177,train F1-score:0.5877 Val Loss: 0.9426657557487488, Val Acc: 0.7523\n","Epoch [46/500], Loss: 1.2677700519561768, Train Acc: 0.6204,train F1-score:0.5874 Val Loss: 0.9380966424942017, Val Acc: 0.7517\n","Epoch [47/500], Loss: 1.2653777599334717, Train Acc: 0.6251,train F1-score:0.5905 Val Loss: 0.9338908195495605, Val Acc: 0.7517\n","Epoch [48/500], Loss: 1.258651852607727, Train Acc: 0.6311,train F1-score:0.5935 Val Loss: 0.9302241802215576, Val Acc: 0.7497\n","Epoch [49/500], Loss: 1.2536444664001465, Train Acc: 0.6299,train F1-score:0.5912 Val Loss: 0.9271896481513977, Val Acc: 0.7497\n","Epoch [50/500], Loss: 1.2567188739776611, Train Acc: 0.6300,train F1-score:0.5901 Val Loss: 0.9246665835380554, Val Acc: 0.7477\n","Epoch [51/500], Loss: 1.2593337297439575, Train Acc: 0.6273,train F1-score:0.5901 Val Loss: 0.9225221872329712, Val Acc: 0.7490\n","Epoch [52/500], Loss: 1.2485928535461426, Train Acc: 0.6547,train F1-score:0.5976 Val Loss: 0.9204586148262024, Val Acc: 0.7503\n","Epoch [53/500], Loss: 1.2397048473358154, Train Acc: 0.6562,train F1-score:0.5963 Val Loss: 0.9181411266326904, Val Acc: 0.7497\n","Epoch [54/500], Loss: 1.2394065856933594, Train Acc: 0.6582,train F1-score:0.6006 Val Loss: 0.9161193370819092, Val Acc: 0.7497\n","Epoch [55/500], Loss: 1.2160968780517578, Train Acc: 0.6600,train F1-score:0.6005 Val Loss: 0.9140999913215637, Val Acc: 0.7503\n","Epoch [56/500], Loss: 1.2243303060531616, Train Acc: 0.6578,train F1-score:0.5967 Val Loss: 0.9121724367141724, Val Acc: 0.7517\n","Epoch [57/500], Loss: 1.2233408689498901, Train Acc: 0.6609,train F1-score:0.6027 Val Loss: 0.9104583859443665, Val Acc: 0.7510\n","Epoch [58/500], Loss: 1.2079384326934814, Train Acc: 0.6670,train F1-score:0.6093 Val Loss: 0.9086244106292725, Val Acc: 0.7510\n","Epoch [59/500], Loss: 1.214024305343628, Train Acc: 0.6619,train F1-score:0.6001 Val Loss: 0.9068475365638733, Val Acc: 0.7510\n","Epoch [60/500], Loss: 1.199658989906311, Train Acc: 0.6653,train F1-score:0.6042 Val Loss: 0.9049651026725769, Val Acc: 0.7530\n","Epoch [61/500], Loss: 1.2053364515304565, Train Acc: 0.6674,train F1-score:0.6085 Val Loss: 0.9030781984329224, Val Acc: 0.7530\n","Epoch [62/500], Loss: 1.195553183555603, Train Acc: 0.6726,train F1-score:0.6111 Val Loss: 0.9009284973144531, Val Acc: 0.7530\n","Epoch [63/500], Loss: 1.1945492029190063, Train Acc: 0.6683,train F1-score:0.6094 Val Loss: 0.898594856262207, Val Acc: 0.7523\n","Epoch [64/500], Loss: 1.1914610862731934, Train Acc: 0.6695,train F1-score:0.6083 Val Loss: 0.8963354825973511, Val Acc: 0.7503\n","Epoch [65/500], Loss: 1.1998778581619263, Train Acc: 0.6647,train F1-score:0.6005 Val Loss: 0.893994927406311, Val Acc: 0.7490\n","Epoch [66/500], Loss: 1.1902964115142822, Train Acc: 0.6690,train F1-score:0.6091 Val Loss: 0.891473114490509, Val Acc: 0.7497\n","Epoch [67/500], Loss: 1.1890262365341187, Train Acc: 0.6693,train F1-score:0.6068 Val Loss: 0.889095664024353, Val Acc: 0.7490\n","Epoch [68/500], Loss: 1.1816949844360352, Train Acc: 0.6684,train F1-score:0.6039 Val Loss: 0.8865605592727661, Val Acc: 0.7490\n","Epoch [69/500], Loss: 1.1776577234268188, Train Acc: 0.6697,train F1-score:0.6049 Val Loss: 0.8840285539627075, Val Acc: 0.7483\n","Epoch [70/500], Loss: 1.1738883256912231, Train Acc: 0.6698,train F1-score:0.6043 Val Loss: 0.8815908432006836, Val Acc: 0.7483\n","Epoch [71/500], Loss: 1.1746724843978882, Train Acc: 0.6734,train F1-score:0.6085 Val Loss: 0.8792940974235535, Val Acc: 0.7470\n","Epoch [72/500], Loss: 1.1778035163879395, Train Acc: 0.6699,train F1-score:0.6033 Val Loss: 0.8772075772285461, Val Acc: 0.7477\n","Epoch [73/500], Loss: 1.1576138734817505, Train Acc: 0.6752,train F1-score:0.6113 Val Loss: 0.8754560947418213, Val Acc: 0.7470\n","Epoch [74/500], Loss: 1.1619093418121338, Train Acc: 0.6727,train F1-score:0.6083 Val Loss: 0.8738760352134705, Val Acc: 0.7490\n","Epoch [75/500], Loss: 1.1548380851745605, Train Acc: 0.6707,train F1-score:0.6063 Val Loss: 0.8724167346954346, Val Acc: 0.7483\n","Epoch [76/500], Loss: 1.1569157838821411, Train Acc: 0.6774,train F1-score:0.6121 Val Loss: 0.8711559176445007, Val Acc: 0.7477\n","Epoch [77/500], Loss: 1.1620420217514038, Train Acc: 0.6690,train F1-score:0.6063 Val Loss: 0.8700030446052551, Val Acc: 0.7477\n","Epoch [78/500], Loss: 1.1507664918899536, Train Acc: 0.6739,train F1-score:0.6081 Val Loss: 0.868839681148529, Val Acc: 0.7483\n","Epoch [79/500], Loss: 1.150624394416809, Train Acc: 0.6734,train F1-score:0.6066 Val Loss: 0.8676769137382507, Val Acc: 0.7490\n","Epoch [80/500], Loss: 1.1439495086669922, Train Acc: 0.6749,train F1-score:0.6082 Val Loss: 0.8662915229797363, Val Acc: 0.7490\n","Epoch [81/500], Loss: 1.1402587890625, Train Acc: 0.6780,train F1-score:0.6121 Val Loss: 0.8648644685745239, Val Acc: 0.7483\n","Epoch [82/500], Loss: 1.1506881713867188, Train Acc: 0.6760,train F1-score:0.6105 Val Loss: 0.8632480502128601, Val Acc: 0.7483\n","Epoch [83/500], Loss: 1.133121132850647, Train Acc: 0.6768,train F1-score:0.6096 Val Loss: 0.8617309927940369, Val Acc: 0.7483\n","Epoch [84/500], Loss: 1.1447618007659912, Train Acc: 0.6745,train F1-score:0.6105 Val Loss: 0.8602488040924072, Val Acc: 0.7470\n","Epoch [85/500], Loss: 1.132396936416626, Train Acc: 0.6807,train F1-score:0.6143 Val Loss: 0.859035849571228, Val Acc: 0.7463\n","Epoch [86/500], Loss: 1.1291404962539673, Train Acc: 0.6792,train F1-score:0.6141 Val Loss: 0.8578544855117798, Val Acc: 0.7470\n","Epoch [87/500], Loss: 1.126544713973999, Train Acc: 0.6795,train F1-score:0.6145 Val Loss: 0.8568522334098816, Val Acc: 0.7463\n","Epoch [88/500], Loss: 1.1294273138046265, Train Acc: 0.6821,train F1-score:0.6167 Val Loss: 0.8553887605667114, Val Acc: 0.7490\n","Epoch [89/500], Loss: 1.1271849870681763, Train Acc: 0.6779,train F1-score:0.6132 Val Loss: 0.853840708732605, Val Acc: 0.7483\n","Epoch [90/500], Loss: 1.1182489395141602, Train Acc: 0.6820,train F1-score:0.6177 Val Loss: 0.8520697951316833, Val Acc: 0.7483\n","Epoch [91/500], Loss: 1.120955467224121, Train Acc: 0.6837,train F1-score:0.6197 Val Loss: 0.8503719568252563, Val Acc: 0.7490\n","Epoch [92/500], Loss: 1.1266288757324219, Train Acc: 0.6789,train F1-score:0.6133 Val Loss: 0.8487672209739685, Val Acc: 0.7490\n","Epoch [93/500], Loss: 1.1185959577560425, Train Acc: 0.6777,train F1-score:0.6140 Val Loss: 0.8475227952003479, Val Acc: 0.7470\n","Epoch [94/500], Loss: 1.1074503660202026, Train Acc: 0.6821,train F1-score:0.6201 Val Loss: 0.8462713360786438, Val Acc: 0.7470\n","Epoch [95/500], Loss: 1.1106029748916626, Train Acc: 0.6838,train F1-score:0.6183 Val Loss: 0.845280110836029, Val Acc: 0.7477\n","Epoch [96/500], Loss: 1.1098495721817017, Train Acc: 0.6835,train F1-score:0.6169 Val Loss: 0.8444594740867615, Val Acc: 0.7477\n","Epoch [97/500], Loss: 1.1204386949539185, Train Acc: 0.6791,train F1-score:0.6138 Val Loss: 0.8431701064109802, Val Acc: 0.7470\n","Epoch [98/500], Loss: 1.1037824153900146, Train Acc: 0.6876,train F1-score:0.6224 Val Loss: 0.8421347141265869, Val Acc: 0.7470\n","Epoch [99/500], Loss: 1.1088587045669556, Train Acc: 0.6855,train F1-score:0.6213 Val Loss: 0.8408973813056946, Val Acc: 0.7470\n","Epoch [100/500], Loss: 1.1032462120056152, Train Acc: 0.6840,train F1-score:0.6184 Val Loss: 0.8402029871940613, Val Acc: 0.7463\n","Epoch [101/500], Loss: 1.1052100658416748, Train Acc: 0.6831,train F1-score:0.6182 Val Loss: 0.8395503163337708, Val Acc: 0.7450\n","Epoch [102/500], Loss: 1.097415566444397, Train Acc: 0.6819,train F1-score:0.6185 Val Loss: 0.8389356732368469, Val Acc: 0.7450\n","Epoch [103/500], Loss: 1.1022062301635742, Train Acc: 0.6846,train F1-score:0.6210 Val Loss: 0.8379300832748413, Val Acc: 0.7443\n","Epoch [104/500], Loss: 1.0937516689300537, Train Acc: 0.6870,train F1-score:0.6246 Val Loss: 0.8364096283912659, Val Acc: 0.7450\n","Epoch [105/500], Loss: 1.1030921936035156, Train Acc: 0.6836,train F1-score:0.6183 Val Loss: 0.8350329399108887, Val Acc: 0.7456\n","Epoch [106/500], Loss: 1.102564811706543, Train Acc: 0.6842,train F1-score:0.6206 Val Loss: 0.8338493704795837, Val Acc: 0.7470\n","Epoch [107/500], Loss: 1.0927189588546753, Train Acc: 0.6854,train F1-score:0.6200 Val Loss: 0.8326039910316467, Val Acc: 0.7477\n","Epoch [108/500], Loss: 1.0937910079956055, Train Acc: 0.6864,train F1-score:0.6222 Val Loss: 0.8313246369361877, Val Acc: 0.7483\n","Epoch [109/500], Loss: 1.0970536470413208, Train Acc: 0.6878,train F1-score:0.6240 Val Loss: 0.8303155899047852, Val Acc: 0.7483\n","Epoch [110/500], Loss: 1.0992588996887207, Train Acc: 0.6827,train F1-score:0.6177 Val Loss: 0.8293153047561646, Val Acc: 0.7490\n","Epoch [111/500], Loss: 1.0921728610992432, Train Acc: 0.6862,train F1-score:0.6247 Val Loss: 0.8283684849739075, Val Acc: 0.7463\n","Epoch [112/500], Loss: 1.0968698263168335, Train Acc: 0.6867,train F1-score:0.6238 Val Loss: 0.8274151086807251, Val Acc: 0.7470\n","Epoch [113/500], Loss: 1.091772198677063, Train Acc: 0.6868,train F1-score:0.6248 Val Loss: 0.8262310028076172, Val Acc: 0.7490\n","Epoch [114/500], Loss: 1.0945959091186523, Train Acc: 0.6870,train F1-score:0.6264 Val Loss: 0.8250266909599304, Val Acc: 0.7497\n","Epoch [115/500], Loss: 1.0821216106414795, Train Acc: 0.6872,train F1-score:0.6283 Val Loss: 0.8239189386367798, Val Acc: 0.7503\n","Epoch [116/500], Loss: 1.084170937538147, Train Acc: 0.6806,train F1-score:0.6227 Val Loss: 0.8226528167724609, Val Acc: 0.7490\n","Epoch [117/500], Loss: 1.075437307357788, Train Acc: 0.6872,train F1-score:0.6279 Val Loss: 0.8214301466941833, Val Acc: 0.7483\n","Epoch [118/500], Loss: 1.076393723487854, Train Acc: 0.6864,train F1-score:0.6237 Val Loss: 0.8205839991569519, Val Acc: 0.7450\n","Epoch [119/500], Loss: 1.0834342241287231, Train Acc: 0.6897,train F1-score:0.6289 Val Loss: 0.8200209736824036, Val Acc: 0.7456\n","Epoch [120/500], Loss: 1.0786716938018799, Train Acc: 0.6866,train F1-score:0.6259 Val Loss: 0.8198345899581909, Val Acc: 0.7463\n","Epoch [121/500], Loss: 1.0770797729492188, Train Acc: 0.6876,train F1-score:0.6244 Val Loss: 0.8198259472846985, Val Acc: 0.7463\n","Epoch [122/500], Loss: 1.0787614583969116, Train Acc: 0.6867,train F1-score:0.6249 Val Loss: 0.8193624019622803, Val Acc: 0.7456\n","Epoch [123/500], Loss: 1.0778615474700928, Train Acc: 0.6878,train F1-score:0.6242 Val Loss: 0.8188554048538208, Val Acc: 0.7470\n","Epoch [124/500], Loss: 1.0800279378890991, Train Acc: 0.6836,train F1-score:0.6219 Val Loss: 0.8188844323158264, Val Acc: 0.7483\n","Epoch [125/500], Loss: 1.0816043615341187, Train Acc: 0.6867,train F1-score:0.6281 Val Loss: 0.8185556530952454, Val Acc: 0.7497\n","Epoch [126/500], Loss: 1.0707728862762451, Train Acc: 0.6904,train F1-score:0.6326 Val Loss: 0.8179886937141418, Val Acc: 0.7510\n","Epoch [127/500], Loss: 1.0671855211257935, Train Acc: 0.6898,train F1-score:0.6296 Val Loss: 0.8172382116317749, Val Acc: 0.7497\n","Epoch [128/500], Loss: 1.0657756328582764, Train Acc: 0.6924,train F1-score:0.6355 Val Loss: 0.8163628578186035, Val Acc: 0.7490\n","Epoch [129/500], Loss: 1.0672861337661743, Train Acc: 0.6918,train F1-score:0.6345 Val Loss: 0.8157175183296204, Val Acc: 0.7497\n","Epoch [130/500], Loss: 1.065324306488037, Train Acc: 0.6899,train F1-score:0.6287 Val Loss: 0.8151156902313232, Val Acc: 0.7490\n","Epoch [131/500], Loss: 1.074999213218689, Train Acc: 0.6872,train F1-score:0.6259 Val Loss: 0.8151620030403137, Val Acc: 0.7483\n","Epoch [132/500], Loss: 1.0632597208023071, Train Acc: 0.6920,train F1-score:0.6330 Val Loss: 0.8145195245742798, Val Acc: 0.7483\n","Epoch [133/500], Loss: 1.0643953084945679, Train Acc: 0.6887,train F1-score:0.6285 Val Loss: 0.8137297630310059, Val Acc: 0.7497\n","Epoch [134/500], Loss: 1.0563538074493408, Train Acc: 0.6943,train F1-score:0.6350 Val Loss: 0.8125947117805481, Val Acc: 0.7483\n","Epoch [135/500], Loss: 1.061859369277954, Train Acc: 0.6921,train F1-score:0.6322 Val Loss: 0.8114577531814575, Val Acc: 0.7483\n","Epoch [136/500], Loss: 1.0603567361831665, Train Acc: 0.6885,train F1-score:0.6286 Val Loss: 0.8101227879524231, Val Acc: 0.7477\n","Epoch [137/500], Loss: 1.055822491645813, Train Acc: 0.6910,train F1-score:0.6335 Val Loss: 0.8072929978370667, Val Acc: 0.7490\n","Epoch [138/500], Loss: 1.0678070783615112, Train Acc: 0.6910,train F1-score:0.6322 Val Loss: 0.805521547794342, Val Acc: 0.7483\n","Epoch [139/500], Loss: 1.0603934526443481, Train Acc: 0.6903,train F1-score:0.6296 Val Loss: 0.8044458627700806, Val Acc: 0.7463\n","Epoch [140/500], Loss: 1.0539695024490356, Train Acc: 0.6915,train F1-score:0.6314 Val Loss: 0.8036019206047058, Val Acc: 0.7456\n","Epoch [141/500], Loss: 1.056087613105774, Train Acc: 0.6883,train F1-score:0.6274 Val Loss: 0.8030046820640564, Val Acc: 0.7456\n","Epoch [142/500], Loss: 1.0482852458953857, Train Acc: 0.6920,train F1-score:0.6320 Val Loss: 0.8025270700454712, Val Acc: 0.7450\n","Epoch [143/500], Loss: 1.0620037317276, Train Acc: 0.6879,train F1-score:0.6276 Val Loss: 0.802127480506897, Val Acc: 0.7477\n","Epoch [144/500], Loss: 1.0612883567810059, Train Acc: 0.6831,train F1-score:0.6245 Val Loss: 0.8019433617591858, Val Acc: 0.7456\n","Epoch [145/500], Loss: 1.0575045347213745, Train Acc: 0.6885,train F1-score:0.6273 Val Loss: 0.802060067653656, Val Acc: 0.7456\n","Epoch [146/500], Loss: 1.0509006977081299, Train Acc: 0.6901,train F1-score:0.6291 Val Loss: 0.8021296858787537, Val Acc: 0.7456\n","Epoch [147/500], Loss: 1.0482170581817627, Train Acc: 0.6952,train F1-score:0.6362 Val Loss: 0.8019905686378479, Val Acc: 0.7450\n","Epoch [148/500], Loss: 1.0531201362609863, Train Acc: 0.6931,train F1-score:0.6338 Val Loss: 0.8019167184829712, Val Acc: 0.7450\n","Epoch [149/500], Loss: 1.0424014329910278, Train Acc: 0.6928,train F1-score:0.6309 Val Loss: 0.8015594482421875, Val Acc: 0.7450\n","Epoch [150/500], Loss: 1.0603543519973755, Train Acc: 0.6872,train F1-score:0.6262 Val Loss: 0.8011581897735596, Val Acc: 0.7456\n","Epoch [151/500], Loss: 1.0521914958953857, Train Acc: 0.6892,train F1-score:0.6296 Val Loss: 0.8007936477661133, Val Acc: 0.7456\n","Epoch [152/500], Loss: 1.0430079698562622, Train Acc: 0.6904,train F1-score:0.6308 Val Loss: 0.7999657392501831, Val Acc: 0.7456\n","Epoch [153/500], Loss: 1.0510112047195435, Train Acc: 0.6915,train F1-score:0.6323 Val Loss: 0.7991073727607727, Val Acc: 0.7470\n","Epoch [154/500], Loss: 1.0508875846862793, Train Acc: 0.6934,train F1-score:0.6322 Val Loss: 0.7987332344055176, Val Acc: 0.7477\n","Epoch [155/500], Loss: 1.0421688556671143, Train Acc: 0.6922,train F1-score:0.6334 Val Loss: 0.798697292804718, Val Acc: 0.7483\n","Epoch [156/500], Loss: 1.0462626218795776, Train Acc: 0.6867,train F1-score:0.6258 Val Loss: 0.7987965941429138, Val Acc: 0.7477\n","Epoch [157/500], Loss: 1.041948676109314, Train Acc: 0.6924,train F1-score:0.6323 Val Loss: 0.7987102270126343, Val Acc: 0.7477\n","Epoch [158/500], Loss: 1.0407634973526, Train Acc: 0.6940,train F1-score:0.6338 Val Loss: 0.7986053824424744, Val Acc: 0.7470\n","Epoch [159/500], Loss: 1.04607355594635, Train Acc: 0.6923,train F1-score:0.6317 Val Loss: 0.7987531423568726, Val Acc: 0.7470\n","Epoch [160/500], Loss: 1.038901925086975, Train Acc: 0.6940,train F1-score:0.6346 Val Loss: 0.7967597842216492, Val Acc: 0.7470\n","Epoch [161/500], Loss: 1.0343466997146606, Train Acc: 0.6931,train F1-score:0.6335 Val Loss: 0.7949872612953186, Val Acc: 0.7477\n","Epoch [162/500], Loss: 1.0315991640090942, Train Acc: 0.6974,train F1-score:0.6376 Val Loss: 0.7933903336524963, Val Acc: 0.7470\n","Epoch [163/500], Loss: 1.0431989431381226, Train Acc: 0.6958,train F1-score:0.6350 Val Loss: 0.7923944592475891, Val Acc: 0.7463\n","Epoch [164/500], Loss: 1.0334137678146362, Train Acc: 0.6954,train F1-score:0.6343 Val Loss: 0.7916297912597656, Val Acc: 0.7470\n","Epoch [165/500], Loss: 1.0381113290786743, Train Acc: 0.6868,train F1-score:0.6267 Val Loss: 0.791105329990387, Val Acc: 0.7463\n","Epoch [166/500], Loss: 1.038343071937561, Train Acc: 0.6923,train F1-score:0.6333 Val Loss: 0.7908545136451721, Val Acc: 0.7463\n","Epoch [167/500], Loss: 1.0335042476654053, Train Acc: 0.6959,train F1-score:0.6355 Val Loss: 0.7905488610267639, Val Acc: 0.7470\n","Epoch [168/500], Loss: 1.0322192907333374, Train Acc: 0.6933,train F1-score:0.6356 Val Loss: 0.7902525067329407, Val Acc: 0.7470\n","Epoch [169/500], Loss: 1.0405545234680176, Train Acc: 0.6918,train F1-score:0.6298 Val Loss: 0.7898043394088745, Val Acc: 0.7470\n","Epoch [170/500], Loss: 1.03326416015625, Train Acc: 0.6939,train F1-score:0.6328 Val Loss: 0.7894423604011536, Val Acc: 0.7470\n","Epoch [171/500], Loss: 1.0344797372817993, Train Acc: 0.6926,train F1-score:0.6313 Val Loss: 0.788949728012085, Val Acc: 0.7470\n","Epoch [172/500], Loss: 1.028343677520752, Train Acc: 0.6949,train F1-score:0.6346 Val Loss: 0.7886006236076355, Val Acc: 0.7463\n","Epoch [173/500], Loss: 1.0258890390396118, Train Acc: 0.6924,train F1-score:0.6326 Val Loss: 0.7878753542900085, Val Acc: 0.7463\n","Epoch [174/500], Loss: 1.0296353101730347, Train Acc: 0.6958,train F1-score:0.6365 Val Loss: 0.787500262260437, Val Acc: 0.7477\n","Epoch [175/500], Loss: 1.03214430809021, Train Acc: 0.6926,train F1-score:0.6317 Val Loss: 0.7875757813453674, Val Acc: 0.7470\n","Epoch [176/500], Loss: 1.0258463621139526, Train Acc: 0.6934,train F1-score:0.6319 Val Loss: 0.7875801920890808, Val Acc: 0.7477\n","Epoch [177/500], Loss: 1.0238603353500366, Train Acc: 0.6974,train F1-score:0.6376 Val Loss: 0.78746497631073, Val Acc: 0.7490\n","Epoch [178/500], Loss: 1.0311942100524902, Train Acc: 0.6963,train F1-score:0.6352 Val Loss: 0.7872450947761536, Val Acc: 0.7503\n","Epoch [179/500], Loss: 1.0143213272094727, Train Acc: 0.6981,train F1-score:0.6387 Val Loss: 0.7864795923233032, Val Acc: 0.7510\n","Epoch [180/500], Loss: 1.0200858116149902, Train Acc: 0.6966,train F1-score:0.6374 Val Loss: 0.7858261466026306, Val Acc: 0.7510\n","Epoch [181/500], Loss: 1.0243360996246338, Train Acc: 0.6938,train F1-score:0.6347 Val Loss: 0.7850180268287659, Val Acc: 0.7497\n","Epoch [182/500], Loss: 1.0269302129745483, Train Acc: 0.6958,train F1-score:0.6356 Val Loss: 0.7845789790153503, Val Acc: 0.7523\n","Epoch [183/500], Loss: 1.0217092037200928, Train Acc: 0.6963,train F1-score:0.6358 Val Loss: 0.7843282222747803, Val Acc: 0.7510\n","Epoch [184/500], Loss: 1.03456449508667, Train Acc: 0.6915,train F1-score:0.6288 Val Loss: 0.784273624420166, Val Acc: 0.7503\n","Epoch [185/500], Loss: 1.0249251127243042, Train Acc: 0.6922,train F1-score:0.6297 Val Loss: 0.7844946384429932, Val Acc: 0.7497\n","Epoch [186/500], Loss: 1.0302400588989258, Train Acc: 0.6934,train F1-score:0.6312 Val Loss: 0.7845209836959839, Val Acc: 0.7490\n","Epoch [187/500], Loss: 1.0270427465438843, Train Acc: 0.6980,train F1-score:0.6372 Val Loss: 0.7846955060958862, Val Acc: 0.7497\n","Epoch [188/500], Loss: 1.016776442527771, Train Acc: 0.6986,train F1-score:0.6374 Val Loss: 0.784839391708374, Val Acc: 0.7490\n","Epoch [189/500], Loss: 1.0108891725540161, Train Acc: 0.6944,train F1-score:0.6316 Val Loss: 0.7848550081253052, Val Acc: 0.7503\n","Epoch [190/500], Loss: 1.0207253694534302, Train Acc: 0.6940,train F1-score:0.6317 Val Loss: 0.7846651673316956, Val Acc: 0.7497\n","Epoch [191/500], Loss: 1.013512372970581, Train Acc: 0.6953,train F1-score:0.6349 Val Loss: 0.7842053174972534, Val Acc: 0.7497\n","Epoch [192/500], Loss: 1.0162006616592407, Train Acc: 0.6954,train F1-score:0.6357 Val Loss: 0.7836387157440186, Val Acc: 0.7537\n","Epoch [193/500], Loss: 1.014229655265808, Train Acc: 0.6980,train F1-score:0.6383 Val Loss: 0.7828949093818665, Val Acc: 0.7537\n","Epoch [194/500], Loss: 1.0124609470367432, Train Acc: 0.6946,train F1-score:0.6321 Val Loss: 0.7821329832077026, Val Acc: 0.7530\n","Epoch [195/500], Loss: 1.0213215351104736, Train Acc: 0.6939,train F1-score:0.6325 Val Loss: 0.7817338109016418, Val Acc: 0.7523\n","Epoch [196/500], Loss: 1.01383638381958, Train Acc: 0.6959,train F1-score:0.6342 Val Loss: 0.7814214825630188, Val Acc: 0.7523\n","Epoch [197/500], Loss: 1.0071234703063965, Train Acc: 0.6964,train F1-score:0.6373 Val Loss: 0.7810560464859009, Val Acc: 0.7523\n","Epoch [198/500], Loss: 1.0133553743362427, Train Acc: 0.6964,train F1-score:0.6379 Val Loss: 0.7803518772125244, Val Acc: 0.7517\n","Epoch [199/500], Loss: 1.0136477947235107, Train Acc: 0.6969,train F1-score:0.6369 Val Loss: 0.7794420123100281, Val Acc: 0.7517\n","Epoch [200/500], Loss: 1.0111353397369385, Train Acc: 0.6990,train F1-score:0.6385 Val Loss: 0.7789870500564575, Val Acc: 0.7503\n","Epoch [201/500], Loss: 1.0242586135864258, Train Acc: 0.6964,train F1-score:0.6371 Val Loss: 0.7788463234901428, Val Acc: 0.7510\n","Epoch [202/500], Loss: 1.007287621498108, Train Acc: 0.7022,train F1-score:0.6448 Val Loss: 0.7788892984390259, Val Acc: 0.7497\n","Epoch [203/500], Loss: 1.0168043375015259, Train Acc: 0.6967,train F1-score:0.6369 Val Loss: 0.7797234058380127, Val Acc: 0.7523\n","Epoch [204/500], Loss: 1.0107342004776, Train Acc: 0.6968,train F1-score:0.6377 Val Loss: 0.7803838849067688, Val Acc: 0.7510\n","Epoch [205/500], Loss: 0.9984334111213684, Train Acc: 0.7006,train F1-score:0.6420 Val Loss: 0.780135989189148, Val Acc: 0.7523\n","Epoch [206/500], Loss: 1.0090309381484985, Train Acc: 0.6975,train F1-score:0.6386 Val Loss: 0.7781606316566467, Val Acc: 0.7503\n","Epoch [207/500], Loss: 1.0078198909759521, Train Acc: 0.6985,train F1-score:0.6391 Val Loss: 0.7768836617469788, Val Acc: 0.7510\n","Epoch [208/500], Loss: 1.0080300569534302, Train Acc: 0.6957,train F1-score:0.6356 Val Loss: 0.7751474380493164, Val Acc: 0.7544\n","Epoch [209/500], Loss: 1.0041046142578125, Train Acc: 0.6971,train F1-score:0.6358 Val Loss: 0.7732896208763123, Val Acc: 0.7530\n","Epoch [210/500], Loss: 1.007489800453186, Train Acc: 0.6964,train F1-score:0.6359 Val Loss: 0.7718666791915894, Val Acc: 0.7517\n","Epoch [211/500], Loss: 1.0059988498687744, Train Acc: 0.6960,train F1-score:0.6343 Val Loss: 0.7714301347732544, Val Acc: 0.7544\n","Epoch [212/500], Loss: 1.0033061504364014, Train Acc: 0.6961,train F1-score:0.6342 Val Loss: 0.7716819643974304, Val Acc: 0.7550\n","Epoch [213/500], Loss: 1.00509774684906, Train Acc: 0.6990,train F1-score:0.6389 Val Loss: 0.772030234336853, Val Acc: 0.7550\n","Epoch [214/500], Loss: 1.008249282836914, Train Acc: 0.6950,train F1-score:0.6374 Val Loss: 0.77226722240448, Val Acc: 0.7557\n","Epoch [215/500], Loss: 1.0100871324539185, Train Acc: 0.6964,train F1-score:0.6381 Val Loss: 0.7723670601844788, Val Acc: 0.7557\n","Epoch [216/500], Loss: 0.9998872876167297, Train Acc: 0.6989,train F1-score:0.6422 Val Loss: 0.772489607334137, Val Acc: 0.7537\n","Epoch [217/500], Loss: 1.0039386749267578, Train Acc: 0.6986,train F1-score:0.6391 Val Loss: 0.7726259827613831, Val Acc: 0.7510\n","Epoch [218/500], Loss: 1.0032641887664795, Train Acc: 0.6971,train F1-score:0.6360 Val Loss: 0.7726791501045227, Val Acc: 0.7503\n","Epoch [219/500], Loss: 1.0113259553909302, Train Acc: 0.6967,train F1-score:0.6393 Val Loss: 0.7727551460266113, Val Acc: 0.7503\n","Epoch [220/500], Loss: 1.0036064386367798, Train Acc: 0.6985,train F1-score:0.6396 Val Loss: 0.7729331851005554, Val Acc: 0.7523\n","Epoch [221/500], Loss: 1.009699821472168, Train Acc: 0.7001,train F1-score:0.6412 Val Loss: 0.7733951210975647, Val Acc: 0.7544\n","Epoch [222/500], Loss: 1.0075209140777588, Train Acc: 0.6961,train F1-score:0.6379 Val Loss: 0.7739757895469666, Val Acc: 0.7537\n","Epoch [223/500], Loss: 0.9997573494911194, Train Acc: 0.6988,train F1-score:0.6413 Val Loss: 0.7746888399124146, Val Acc: 0.7530\n","Epoch [224/500], Loss: 1.0024443864822388, Train Acc: 0.6996,train F1-score:0.6419 Val Loss: 0.7748467922210693, Val Acc: 0.7517\n","Epoch [225/500], Loss: 1.0034767389297485, Train Acc: 0.7020,train F1-score:0.6436 Val Loss: 0.7747758626937866, Val Acc: 0.7523\n","Epoch [226/500], Loss: 1.003931999206543, Train Acc: 0.6994,train F1-score:0.6371 Val Loss: 0.7740625739097595, Val Acc: 0.7517\n","Epoch [227/500], Loss: 0.9959638714790344, Train Acc: 0.7019,train F1-score:0.6431 Val Loss: 0.77366703748703, Val Acc: 0.7517\n","Epoch [228/500], Loss: 1.003960132598877, Train Acc: 0.6996,train F1-score:0.6400 Val Loss: 0.7731243968009949, Val Acc: 0.7530\n","Epoch [229/500], Loss: 1.0014698505401611, Train Acc: 0.7019,train F1-score:0.6431 Val Loss: 0.7725858688354492, Val Acc: 0.7550\n","Epoch [230/500], Loss: 0.9900161027908325, Train Acc: 0.7024,train F1-score:0.6437 Val Loss: 0.7723394632339478, Val Acc: 0.7550\n","Epoch [231/500], Loss: 0.9982938170433044, Train Acc: 0.7011,train F1-score:0.6427 Val Loss: 0.7713300585746765, Val Acc: 0.7557\n","Epoch [232/500], Loss: 0.9952645301818848, Train Acc: 0.6985,train F1-score:0.6380 Val Loss: 0.7701664566993713, Val Acc: 0.7550\n","Epoch [233/500], Loss: 1.0014548301696777, Train Acc: 0.7003,train F1-score:0.6393 Val Loss: 0.7693541049957275, Val Acc: 0.7550\n","Epoch [234/500], Loss: 0.9939296245574951, Train Acc: 0.7012,train F1-score:0.6418 Val Loss: 0.7687890529632568, Val Acc: 0.7544\n","Epoch [235/500], Loss: 0.9954316020011902, Train Acc: 0.7029,train F1-score:0.6452 Val Loss: 0.7684295177459717, Val Acc: 0.7530\n","Epoch [236/500], Loss: 0.9879428744316101, Train Acc: 0.7015,train F1-score:0.6412 Val Loss: 0.7680228352546692, Val Acc: 0.7530\n","Epoch [237/500], Loss: 0.9972838163375854, Train Acc: 0.6989,train F1-score:0.6379 Val Loss: 0.7678489685058594, Val Acc: 0.7510\n","Epoch [238/500], Loss: 0.9933616518974304, Train Acc: 0.6980,train F1-score:0.6371 Val Loss: 0.7679461240768433, Val Acc: 0.7510\n","Epoch [239/500], Loss: 0.9921167492866516, Train Acc: 0.7025,train F1-score:0.6422 Val Loss: 0.7681415677070618, Val Acc: 0.7530\n","Epoch [240/500], Loss: 1.0029295682907104, Train Acc: 0.6962,train F1-score:0.6351 Val Loss: 0.767831027507782, Val Acc: 0.7550\n","Epoch [241/500], Loss: 0.9886438250541687, Train Acc: 0.7003,train F1-score:0.6411 Val Loss: 0.7670074105262756, Val Acc: 0.7544\n","Epoch [242/500], Loss: 0.9977337718009949, Train Acc: 0.6945,train F1-score:0.6340 Val Loss: 0.7666251063346863, Val Acc: 0.7517\n","Epoch [243/500], Loss: 0.990200936794281, Train Acc: 0.7060,train F1-score:0.6481 Val Loss: 0.7660595178604126, Val Acc: 0.7530\n","Epoch [244/500], Loss: 0.9931812882423401, Train Acc: 0.7020,train F1-score:0.6440 Val Loss: 0.7649853825569153, Val Acc: 0.7544\n","Epoch [245/500], Loss: 0.9924526214599609, Train Acc: 0.7063,train F1-score:0.6514 Val Loss: 0.7643257975578308, Val Acc: 0.7557\n","Epoch [246/500], Loss: 0.9923479557037354, Train Acc: 0.7030,train F1-score:0.6452 Val Loss: 0.7639322876930237, Val Acc: 0.7564\n","Epoch [247/500], Loss: 0.9853167533874512, Train Acc: 0.7028,train F1-score:0.6441 Val Loss: 0.7638970613479614, Val Acc: 0.7570\n","Epoch [248/500], Loss: 0.9860653877258301, Train Acc: 0.7050,train F1-score:0.6464 Val Loss: 0.7635022401809692, Val Acc: 0.7550\n","Epoch [249/500], Loss: 0.9820314645767212, Train Acc: 0.7021,train F1-score:0.6421 Val Loss: 0.7628874182701111, Val Acc: 0.7530\n","Epoch [250/500], Loss: 0.9958673119544983, Train Acc: 0.6980,train F1-score:0.6382 Val Loss: 0.7621142864227295, Val Acc: 0.7530\n","Epoch [251/500], Loss: 0.984072208404541, Train Acc: 0.6999,train F1-score:0.6397 Val Loss: 0.7613988518714905, Val Acc: 0.7530\n","Epoch [252/500], Loss: 0.9848382472991943, Train Acc: 0.7046,train F1-score:0.6452 Val Loss: 0.7608149647712708, Val Acc: 0.7537\n","Epoch [253/500], Loss: 0.9821646213531494, Train Acc: 0.7000,train F1-score:0.6398 Val Loss: 0.7604173421859741, Val Acc: 0.7530\n","Epoch [254/500], Loss: 0.9862251877784729, Train Acc: 0.7009,train F1-score:0.6419 Val Loss: 0.7603273391723633, Val Acc: 0.7537\n","Epoch [255/500], Loss: 0.9980018734931946, Train Acc: 0.7013,train F1-score:0.6425 Val Loss: 0.7610055208206177, Val Acc: 0.7544\n","Epoch [256/500], Loss: 0.985878586769104, Train Acc: 0.7015,train F1-score:0.6412 Val Loss: 0.7615424394607544, Val Acc: 0.7550\n","Epoch [257/500], Loss: 0.9888319969177246, Train Acc: 0.7056,train F1-score:0.6483 Val Loss: 0.7622724771499634, Val Acc: 0.7544\n","Epoch [258/500], Loss: 0.9868935346603394, Train Acc: 0.7037,train F1-score:0.6462 Val Loss: 0.7638192176818848, Val Acc: 0.7537\n","Epoch [259/500], Loss: 0.985893726348877, Train Acc: 0.7001,train F1-score:0.6399 Val Loss: 0.7635141015052795, Val Acc: 0.7537\n","Epoch [260/500], Loss: 0.9879705905914307, Train Acc: 0.7052,train F1-score:0.6467 Val Loss: 0.762239396572113, Val Acc: 0.7517\n","Epoch [261/500], Loss: 0.9862432479858398, Train Acc: 0.7035,train F1-score:0.6440 Val Loss: 0.7592958807945251, Val Acc: 0.7550\n","Epoch [262/500], Loss: 0.9885775446891785, Train Acc: 0.7038,train F1-score:0.6443 Val Loss: 0.7572144865989685, Val Acc: 0.7577\n","Epoch [263/500], Loss: 0.9800805449485779, Train Acc: 0.7046,train F1-score:0.6460 Val Loss: 0.7559091448783875, Val Acc: 0.7584\n","Epoch [264/500], Loss: 0.9818534255027771, Train Acc: 0.7037,train F1-score:0.6444 Val Loss: 0.7554887533187866, Val Acc: 0.7557\n","Epoch [265/500], Loss: 0.9844847321510315, Train Acc: 0.7052,train F1-score:0.6480 Val Loss: 0.7558106184005737, Val Acc: 0.7557\n","Epoch [266/500], Loss: 0.9844239950180054, Train Acc: 0.7021,train F1-score:0.6445 Val Loss: 0.7566310167312622, Val Acc: 0.7564\n","Epoch [267/500], Loss: 0.9856653213500977, Train Acc: 0.7028,train F1-score:0.6456 Val Loss: 0.7573999762535095, Val Acc: 0.7557\n","Epoch [268/500], Loss: 0.981245756149292, Train Acc: 0.7056,train F1-score:0.6517 Val Loss: 0.7580640912055969, Val Acc: 0.7557\n","Epoch [269/500], Loss: 0.9795480370521545, Train Acc: 0.7044,train F1-score:0.6486 Val Loss: 0.7583387494087219, Val Acc: 0.7537\n","Epoch [270/500], Loss: 0.9803295731544495, Train Acc: 0.7046,train F1-score:0.6467 Val Loss: 0.758413553237915, Val Acc: 0.7510\n","Epoch [271/500], Loss: 0.978714108467102, Train Acc: 0.7062,train F1-score:0.6490 Val Loss: 0.7579431533813477, Val Acc: 0.7517\n","Epoch [272/500], Loss: 0.978774905204773, Train Acc: 0.7022,train F1-score:0.6433 Val Loss: 0.7572157382965088, Val Acc: 0.7503\n","Epoch [273/500], Loss: 0.9790523052215576, Train Acc: 0.7027,train F1-score:0.6443 Val Loss: 0.7565155625343323, Val Acc: 0.7510\n","Epoch [274/500], Loss: 0.97847580909729, Train Acc: 0.7026,train F1-score:0.6450 Val Loss: 0.7559972405433655, Val Acc: 0.7530\n","Epoch [275/500], Loss: 0.9827523231506348, Train Acc: 0.7049,train F1-score:0.6464 Val Loss: 0.7559199929237366, Val Acc: 0.7544\n","Epoch [276/500], Loss: 0.9872733950614929, Train Acc: 0.7011,train F1-score:0.6418 Val Loss: 0.7564589977264404, Val Acc: 0.7537\n","Epoch [277/500], Loss: 0.9776546359062195, Train Acc: 0.7042,train F1-score:0.6467 Val Loss: 0.7570534944534302, Val Acc: 0.7537\n","Epoch [278/500], Loss: 0.9760247468948364, Train Acc: 0.7048,train F1-score:0.6459 Val Loss: 0.7575112581253052, Val Acc: 0.7564\n","Epoch [279/500], Loss: 0.9849780201911926, Train Acc: 0.7022,train F1-score:0.6443 Val Loss: 0.7580938339233398, Val Acc: 0.7570\n","Epoch [280/500], Loss: 0.9711634516716003, Train Acc: 0.7073,train F1-score:0.6512 Val Loss: 0.7585597634315491, Val Acc: 0.7570\n","Epoch [281/500], Loss: 0.9675436615943909, Train Acc: 0.7063,train F1-score:0.6505 Val Loss: 0.75911545753479, Val Acc: 0.7564\n","Epoch [282/500], Loss: 0.9698458909988403, Train Acc: 0.7077,train F1-score:0.6527 Val Loss: 0.7581551671028137, Val Acc: 0.7557\n","Epoch [283/500], Loss: 0.9733703136444092, Train Acc: 0.7046,train F1-score:0.6471 Val Loss: 0.7570096254348755, Val Acc: 0.7530\n","Epoch [284/500], Loss: 0.9753720760345459, Train Acc: 0.7022,train F1-score:0.6439 Val Loss: 0.7555205225944519, Val Acc: 0.7530\n","Epoch [285/500], Loss: 0.9714570045471191, Train Acc: 0.7076,train F1-score:0.6490 Val Loss: 0.7547235488891602, Val Acc: 0.7523\n","Epoch [286/500], Loss: 0.9700475931167603, Train Acc: 0.7046,train F1-score:0.6467 Val Loss: 0.753896951675415, Val Acc: 0.7523\n","Epoch [287/500], Loss: 0.9731892347335815, Train Acc: 0.7031,train F1-score:0.6433 Val Loss: 0.7530400156974792, Val Acc: 0.7537\n","Epoch [288/500], Loss: 0.9731599688529968, Train Acc: 0.7051,train F1-score:0.6480 Val Loss: 0.7527509927749634, Val Acc: 0.7550\n","Epoch [289/500], Loss: 0.9662297368049622, Train Acc: 0.7074,train F1-score:0.6497 Val Loss: 0.7522724270820618, Val Acc: 0.7564\n","Epoch [290/500], Loss: 0.9695833921432495, Train Acc: 0.7044,train F1-score:0.6465 Val Loss: 0.7519511580467224, Val Acc: 0.7577\n","Epoch [291/500], Loss: 0.9664767384529114, Train Acc: 0.7070,train F1-score:0.6519 Val Loss: 0.7518457770347595, Val Acc: 0.7577\n","Epoch [292/500], Loss: 0.9691899418830872, Train Acc: 0.7059,train F1-score:0.6494 Val Loss: 0.7504860758781433, Val Acc: 0.7590\n","Epoch [293/500], Loss: 0.975092351436615, Train Acc: 0.7055,train F1-score:0.6485 Val Loss: 0.7494542002677917, Val Acc: 0.7557\n","Epoch [294/500], Loss: 0.9665823578834534, Train Acc: 0.7072,train F1-score:0.6511 Val Loss: 0.7488469481468201, Val Acc: 0.7570\n","Epoch [295/500], Loss: 0.9786316752433777, Train Acc: 0.7016,train F1-score:0.6441 Val Loss: 0.7493762373924255, Val Acc: 0.7570\n","Epoch [296/500], Loss: 0.9715934991836548, Train Acc: 0.7050,train F1-score:0.6486 Val Loss: 0.7505741715431213, Val Acc: 0.7564\n","Epoch [297/500], Loss: 0.9658674001693726, Train Acc: 0.7070,train F1-score:0.6502 Val Loss: 0.7524451017379761, Val Acc: 0.7550\n","Epoch [298/500], Loss: 0.970065712928772, Train Acc: 0.7062,train F1-score:0.6489 Val Loss: 0.7540653944015503, Val Acc: 0.7557\n","Epoch [299/500], Loss: 0.9764059782028198, Train Acc: 0.7050,train F1-score:0.6474 Val Loss: 0.7542634010314941, Val Acc: 0.7544\n","Epoch [300/500], Loss: 0.9698340892791748, Train Acc: 0.7047,train F1-score:0.6486 Val Loss: 0.7542057037353516, Val Acc: 0.7550\n","Epoch [301/500], Loss: 0.9674267172813416, Train Acc: 0.7039,train F1-score:0.6460 Val Loss: 0.7532366514205933, Val Acc: 0.7537\n","Epoch [302/500], Loss: 0.9698067903518677, Train Acc: 0.7028,train F1-score:0.6462 Val Loss: 0.7517579793930054, Val Acc: 0.7550\n","Epoch [303/500], Loss: 0.9653320908546448, Train Acc: 0.7076,train F1-score:0.6508 Val Loss: 0.7510387301445007, Val Acc: 0.7570\n","Epoch [304/500], Loss: 0.9603384137153625, Train Acc: 0.7054,train F1-score:0.6500 Val Loss: 0.7507057189941406, Val Acc: 0.7570\n","Epoch [305/500], Loss: 0.9746502637863159, Train Acc: 0.7026,train F1-score:0.6453 Val Loss: 0.7509080171585083, Val Acc: 0.7557\n","Epoch [306/500], Loss: 0.9652987718582153, Train Acc: 0.7077,train F1-score:0.6519 Val Loss: 0.7515250444412231, Val Acc: 0.7577\n","Epoch [307/500], Loss: 0.969038188457489, Train Acc: 0.7041,train F1-score:0.6471 Val Loss: 0.7520646452903748, Val Acc: 0.7590\n","Epoch [308/500], Loss: 0.9682700037956238, Train Acc: 0.7026,train F1-score:0.6431 Val Loss: 0.7524707317352295, Val Acc: 0.7570\n","Epoch [309/500], Loss: 0.9715858101844788, Train Acc: 0.7060,train F1-score:0.6500 Val Loss: 0.7530528903007507, Val Acc: 0.7544\n","Epoch [310/500], Loss: 0.9614513516426086, Train Acc: 0.7042,train F1-score:0.6446 Val Loss: 0.7533619403839111, Val Acc: 0.7517\n","Epoch [311/500], Loss: 0.9702374935150146, Train Acc: 0.7068,train F1-score:0.6492 Val Loss: 0.7537859082221985, Val Acc: 0.7497\n","Epoch [312/500], Loss: 0.9664071202278137, Train Acc: 0.7064,train F1-score:0.6484 Val Loss: 0.7535442113876343, Val Acc: 0.7497\n","Epoch [313/500], Loss: 0.9625990390777588, Train Acc: 0.7073,train F1-score:0.6491 Val Loss: 0.7531288862228394, Val Acc: 0.7503\n","Epoch [314/500], Loss: 0.9600592255592346, Train Acc: 0.7114,train F1-score:0.6567 Val Loss: 0.7527798414230347, Val Acc: 0.7550\n","Epoch [315/500], Loss: 0.9616497755050659, Train Acc: 0.7046,train F1-score:0.6479 Val Loss: 0.7525617480278015, Val Acc: 0.7564\n","Epoch [316/500], Loss: 0.9660582542419434, Train Acc: 0.7084,train F1-score:0.6534 Val Loss: 0.752171516418457, Val Acc: 0.7537\n","Epoch [317/500], Loss: 0.958315372467041, Train Acc: 0.7074,train F1-score:0.6504 Val Loss: 0.7512519955635071, Val Acc: 0.7550\n","Epoch [318/500], Loss: 0.9631282091140747, Train Acc: 0.7074,train F1-score:0.6511 Val Loss: 0.7506914734840393, Val Acc: 0.7564\n","Epoch [319/500], Loss: 0.9655876755714417, Train Acc: 0.7053,train F1-score:0.6489 Val Loss: 0.7506148219108582, Val Acc: 0.7557\n","Epoch [320/500], Loss: 0.9582287073135376, Train Acc: 0.7093,train F1-score:0.6520 Val Loss: 0.7506963610649109, Val Acc: 0.7557\n","Epoch [321/500], Loss: 0.9648975729942322, Train Acc: 0.7035,train F1-score:0.6445 Val Loss: 0.7511416077613831, Val Acc: 0.7544\n","Epoch [322/500], Loss: 0.9621506929397583, Train Acc: 0.7073,train F1-score:0.6487 Val Loss: 0.7512841820716858, Val Acc: 0.7517\n","Epoch [323/500], Loss: 0.9586431980133057, Train Acc: 0.7038,train F1-score:0.6453 Val Loss: 0.750914454460144, Val Acc: 0.7517\n","Epoch [324/500], Loss: 0.9670818448066711, Train Acc: 0.7049,train F1-score:0.6445 Val Loss: 0.7503891587257385, Val Acc: 0.7550\n","Epoch [325/500], Loss: 0.9722143411636353, Train Acc: 0.7030,train F1-score:0.6440 Val Loss: 0.7502157092094421, Val Acc: 0.7570\n","Epoch [326/500], Loss: 0.9642508029937744, Train Acc: 0.7057,train F1-score:0.6487 Val Loss: 0.7504582405090332, Val Acc: 0.7604\n","Epoch [327/500], Loss: 0.9690892696380615, Train Acc: 0.7058,train F1-score:0.6474 Val Loss: 0.7512180805206299, Val Acc: 0.7610\n","Epoch [328/500], Loss: 0.9589995741844177, Train Acc: 0.7070,train F1-score:0.6505 Val Loss: 0.7513755559921265, Val Acc: 0.7624\n","Epoch [329/500], Loss: 0.9562632441520691, Train Acc: 0.7059,train F1-score:0.6491 Val Loss: 0.7516486644744873, Val Acc: 0.7584\n","Epoch [330/500], Loss: 0.961685061454773, Train Acc: 0.7099,train F1-score:0.6539 Val Loss: 0.7517680525779724, Val Acc: 0.7557\n","Epoch [331/500], Loss: 0.9633392691612244, Train Acc: 0.7055,train F1-score:0.6493 Val Loss: 0.7518281936645508, Val Acc: 0.7537\n","Epoch [332/500], Loss: 0.9644600749015808, Train Acc: 0.7092,train F1-score:0.6522 Val Loss: 0.7513864040374756, Val Acc: 0.7544\n","Epoch [333/500], Loss: 0.9587292075157166, Train Acc: 0.7068,train F1-score:0.6493 Val Loss: 0.7507398724555969, Val Acc: 0.7537\n","Epoch [334/500], Loss: 0.9613409638404846, Train Acc: 0.7056,train F1-score:0.6473 Val Loss: 0.7503368258476257, Val Acc: 0.7557\n","Epoch [335/500], Loss: 0.9713239073753357, Train Acc: 0.7039,train F1-score:0.6443 Val Loss: 0.7501221299171448, Val Acc: 0.7577\n","Epoch [336/500], Loss: 0.9635064601898193, Train Acc: 0.7062,train F1-score:0.6476 Val Loss: 0.7502597570419312, Val Acc: 0.7584\n","Epoch [337/500], Loss: 0.9634858965873718, Train Acc: 0.7061,train F1-score:0.6470 Val Loss: 0.7504459023475647, Val Acc: 0.7590\n","Epoch [338/500], Loss: 0.9576536417007446, Train Acc: 0.7053,train F1-score:0.6476 Val Loss: 0.7506709694862366, Val Acc: 0.7564\n","Epoch [339/500], Loss: 0.9676549434661865, Train Acc: 0.7068,train F1-score:0.6485 Val Loss: 0.7510012984275818, Val Acc: 0.7570\n","Epoch [340/500], Loss: 0.9584699869155884, Train Acc: 0.7067,train F1-score:0.6494 Val Loss: 0.7516794800758362, Val Acc: 0.7590\n","Epoch [341/500], Loss: 0.9511938691139221, Train Acc: 0.7098,train F1-score:0.6531 Val Loss: 0.7509717345237732, Val Acc: 0.7584\n","Epoch [342/500], Loss: 0.95455002784729, Train Acc: 0.7090,train F1-score:0.6523 Val Loss: 0.7506890892982483, Val Acc: 0.7577\n","Epoch [343/500], Loss: 0.9585251808166504, Train Acc: 0.7075,train F1-score:0.6492 Val Loss: 0.7508818507194519, Val Acc: 0.7557\n","Epoch [344/500], Loss: 0.9490370154380798, Train Acc: 0.7087,train F1-score:0.6532 Val Loss: 0.7508679032325745, Val Acc: 0.7570\n","Epoch [345/500], Loss: 0.9607206583023071, Train Acc: 0.7075,train F1-score:0.6511 Val Loss: 0.7500958442687988, Val Acc: 0.7570\n","Epoch [346/500], Loss: 0.9629082083702087, Train Acc: 0.7055,train F1-score:0.6491 Val Loss: 0.7487635016441345, Val Acc: 0.7550\n","Epoch [347/500], Loss: 0.9521710872650146, Train Acc: 0.7079,train F1-score:0.6498 Val Loss: 0.7476943135261536, Val Acc: 0.7550\n","Epoch [348/500], Loss: 0.9593679904937744, Train Acc: 0.7088,train F1-score:0.6536 Val Loss: 0.7475190758705139, Val Acc: 0.7550\n","Epoch [349/500], Loss: 0.9568072557449341, Train Acc: 0.7080,train F1-score:0.6508 Val Loss: 0.7476328611373901, Val Acc: 0.7564\n","Epoch [350/500], Loss: 0.9539032578468323, Train Acc: 0.7095,train F1-score:0.6529 Val Loss: 0.7480373978614807, Val Acc: 0.7564\n","Epoch [351/500], Loss: 0.9515816569328308, Train Acc: 0.7106,train F1-score:0.6555 Val Loss: 0.7487015128135681, Val Acc: 0.7544\n","Epoch [352/500], Loss: 0.9532437920570374, Train Acc: 0.7077,train F1-score:0.6500 Val Loss: 0.7487502098083496, Val Acc: 0.7544\n","Epoch [353/500], Loss: 0.9543978571891785, Train Acc: 0.7088,train F1-score:0.6504 Val Loss: 0.747879683971405, Val Acc: 0.7544\n","Epoch [354/500], Loss: 0.960504412651062, Train Acc: 0.7031,train F1-score:0.6444 Val Loss: 0.7471392154693604, Val Acc: 0.7570\n","Epoch [355/500], Loss: 0.9537922143936157, Train Acc: 0.7060,train F1-score:0.6468 Val Loss: 0.7469207644462585, Val Acc: 0.7557\n","Epoch [356/500], Loss: 0.9433062076568604, Train Acc: 0.7103,train F1-score:0.6552 Val Loss: 0.7470972537994385, Val Acc: 0.7597\n","Epoch [357/500], Loss: 0.9598729610443115, Train Acc: 0.7041,train F1-score:0.6439 Val Loss: 0.747414767742157, Val Acc: 0.7604\n","Epoch [358/500], Loss: 0.9470078945159912, Train Acc: 0.7104,train F1-score:0.6531 Val Loss: 0.7479944825172424, Val Acc: 0.7610\n","Epoch [359/500], Loss: 0.9481850266456604, Train Acc: 0.7109,train F1-score:0.6564 Val Loss: 0.748473048210144, Val Acc: 0.7597\n","Epoch [360/500], Loss: 0.9518116116523743, Train Acc: 0.7081,train F1-score:0.6527 Val Loss: 0.7484375238418579, Val Acc: 0.7577\n","Epoch [361/500], Loss: 0.9545067548751831, Train Acc: 0.7083,train F1-score:0.6531 Val Loss: 0.7482919692993164, Val Acc: 0.7590\n","Epoch [362/500], Loss: 0.958989143371582, Train Acc: 0.7070,train F1-score:0.6518 Val Loss: 0.7478637099266052, Val Acc: 0.7564\n","Epoch [363/500], Loss: 0.9500160813331604, Train Acc: 0.7111,train F1-score:0.6558 Val Loss: 0.7476333379745483, Val Acc: 0.7577\n","Epoch [364/500], Loss: 0.9506047368049622, Train Acc: 0.7135,train F1-score:0.6591 Val Loss: 0.7479442358016968, Val Acc: 0.7557\n","Epoch [365/500], Loss: 0.9516031742095947, Train Acc: 0.7068,train F1-score:0.6508 Val Loss: 0.7470921277999878, Val Acc: 0.7564\n","Epoch [366/500], Loss: 0.9447693228721619, Train Acc: 0.7115,train F1-score:0.6560 Val Loss: 0.7468624711036682, Val Acc: 0.7564\n","Epoch [367/500], Loss: 0.9500828981399536, Train Acc: 0.7073,train F1-score:0.6505 Val Loss: 0.74593186378479, Val Acc: 0.7564\n","Epoch [368/500], Loss: 0.9508673548698425, Train Acc: 0.7093,train F1-score:0.6517 Val Loss: 0.7455505728721619, Val Acc: 0.7604\n","Epoch [369/500], Loss: 0.9541599750518799, Train Acc: 0.7087,train F1-score:0.6507 Val Loss: 0.745796799659729, Val Acc: 0.7624\n","Epoch [370/500], Loss: 0.9452880620956421, Train Acc: 0.7107,train F1-score:0.6533 Val Loss: 0.7457423806190491, Val Acc: 0.7597\n","Epoch [371/500], Loss: 0.951958417892456, Train Acc: 0.7061,train F1-score:0.6485 Val Loss: 0.7456870079040527, Val Acc: 0.7590\n","Epoch [372/500], Loss: 0.9445410966873169, Train Acc: 0.7103,train F1-score:0.6537 Val Loss: 0.7454779148101807, Val Acc: 0.7597\n","Epoch [373/500], Loss: 0.9421508312225342, Train Acc: 0.7107,train F1-score:0.6565 Val Loss: 0.7451138496398926, Val Acc: 0.7631\n","Epoch [374/500], Loss: 0.9525352120399475, Train Acc: 0.7060,train F1-score:0.6504 Val Loss: 0.7449708580970764, Val Acc: 0.7624\n","Epoch [375/500], Loss: 0.9434853792190552, Train Acc: 0.7123,train F1-score:0.6584 Val Loss: 0.7451253533363342, Val Acc: 0.7604\n","Epoch [376/500], Loss: 0.9562187194824219, Train Acc: 0.7053,train F1-score:0.6476 Val Loss: 0.7453901171684265, Val Acc: 0.7590\n","Epoch [377/500], Loss: 0.9427244663238525, Train Acc: 0.7109,train F1-score:0.6549 Val Loss: 0.7446175217628479, Val Acc: 0.7597\n","Epoch [378/500], Loss: 0.951755166053772, Train Acc: 0.7088,train F1-score:0.6518 Val Loss: 0.7442951202392578, Val Acc: 0.7604\n","Epoch [379/500], Loss: 0.9460292458534241, Train Acc: 0.7076,train F1-score:0.6506 Val Loss: 0.7439601421356201, Val Acc: 0.7590\n","Epoch [380/500], Loss: 0.9421814680099487, Train Acc: 0.7103,train F1-score:0.6541 Val Loss: 0.7444300055503845, Val Acc: 0.7604\n","Epoch [381/500], Loss: 0.9407520294189453, Train Acc: 0.7127,train F1-score:0.6585 Val Loss: 0.7440648078918457, Val Acc: 0.7604\n","Epoch [382/500], Loss: 0.9474111199378967, Train Acc: 0.7101,train F1-score:0.6547 Val Loss: 0.7432723045349121, Val Acc: 0.7624\n","Epoch [383/500], Loss: 0.9435300230979919, Train Acc: 0.7148,train F1-score:0.6597 Val Loss: 0.741953432559967, Val Acc: 0.7637\n","Epoch [384/500], Loss: 0.942862331867218, Train Acc: 0.7102,train F1-score:0.6551 Val Loss: 0.7407394647598267, Val Acc: 0.7664\n","Epoch [385/500], Loss: 0.9411143660545349, Train Acc: 0.7114,train F1-score:0.6565 Val Loss: 0.7402750849723816, Val Acc: 0.7664\n","Epoch [386/500], Loss: 0.9406625628471375, Train Acc: 0.7107,train F1-score:0.6546 Val Loss: 0.7405010461807251, Val Acc: 0.7657\n","Epoch [387/500], Loss: 0.937649667263031, Train Acc: 0.7141,train F1-score:0.6607 Val Loss: 0.7416117191314697, Val Acc: 0.7651\n","Epoch [388/500], Loss: 0.9431369304656982, Train Acc: 0.7103,train F1-score:0.6554 Val Loss: 0.7423155903816223, Val Acc: 0.7644\n","Epoch [389/500], Loss: 0.9471099972724915, Train Acc: 0.7062,train F1-score:0.6511 Val Loss: 0.7417730689048767, Val Acc: 0.7624\n","Epoch [390/500], Loss: 0.9383842349052429, Train Acc: 0.7088,train F1-score:0.6536 Val Loss: 0.7413017749786377, Val Acc: 0.7617\n","Epoch [391/500], Loss: 0.9425620436668396, Train Acc: 0.7090,train F1-score:0.6524 Val Loss: 0.7410275340080261, Val Acc: 0.7617\n","Epoch [392/500], Loss: 0.9477871060371399, Train Acc: 0.7117,train F1-score:0.6581 Val Loss: 0.7406814694404602, Val Acc: 0.7617\n","Epoch [393/500], Loss: 0.9461150169372559, Train Acc: 0.7074,train F1-score:0.6518 Val Loss: 0.7404022216796875, Val Acc: 0.7610\n","Epoch [394/500], Loss: 0.9362301230430603, Train Acc: 0.7128,train F1-score:0.6576 Val Loss: 0.7403720617294312, Val Acc: 0.7631\n","Epoch [395/500], Loss: 0.9437876343727112, Train Acc: 0.7109,train F1-score:0.6554 Val Loss: 0.7409364581108093, Val Acc: 0.7664\n","Epoch [396/500], Loss: 0.9473852515220642, Train Acc: 0.7070,train F1-score:0.6508 Val Loss: 0.7413885593414307, Val Acc: 0.7651\n","Epoch [397/500], Loss: 0.9507576823234558, Train Acc: 0.7093,train F1-score:0.6532 Val Loss: 0.7419120073318481, Val Acc: 0.7637\n","Epoch [398/500], Loss: 0.9460822343826294, Train Acc: 0.7117,train F1-score:0.6564 Val Loss: 0.7426230907440186, Val Acc: 0.7610\n","Epoch [399/500], Loss: 0.9329372644424438, Train Acc: 0.7134,train F1-score:0.6592 Val Loss: 0.7428675293922424, Val Acc: 0.7637\n","Epoch [400/500], Loss: 0.9456618428230286, Train Acc: 0.7085,train F1-score:0.6556 Val Loss: 0.7434174418449402, Val Acc: 0.7584\n","Epoch [401/500], Loss: 0.9426752328872681, Train Acc: 0.7048,train F1-score:0.6472 Val Loss: 0.7434800267219543, Val Acc: 0.7617\n","Epoch [402/500], Loss: 0.9420978426933289, Train Acc: 0.7097,train F1-score:0.6532 Val Loss: 0.7436038255691528, Val Acc: 0.7624\n","Epoch [403/500], Loss: 0.9483214020729065, Train Acc: 0.7096,train F1-score:0.6541 Val Loss: 0.7442399263381958, Val Acc: 0.7610\n","Epoch [404/500], Loss: 0.9427585601806641, Train Acc: 0.7086,train F1-score:0.6532 Val Loss: 0.7443104386329651, Val Acc: 0.7597\n","Epoch [405/500], Loss: 0.9397388696670532, Train Acc: 0.7111,train F1-score:0.6568 Val Loss: 0.7422741055488586, Val Acc: 0.7624\n","Epoch [406/500], Loss: 0.9350824952125549, Train Acc: 0.7123,train F1-score:0.6564 Val Loss: 0.7412550449371338, Val Acc: 0.7657\n","Epoch [407/500], Loss: 0.9424821138381958, Train Acc: 0.7123,train F1-score:0.6570 Val Loss: 0.74029541015625, Val Acc: 0.7657\n","Epoch [408/500], Loss: 0.9449005722999573, Train Acc: 0.7071,train F1-score:0.6492 Val Loss: 0.7393357753753662, Val Acc: 0.7671\n","Epoch [409/500], Loss: 0.9395626783370972, Train Acc: 0.7110,train F1-score:0.6553 Val Loss: 0.7376157641410828, Val Acc: 0.7671\n","Epoch [410/500], Loss: 0.9468133449554443, Train Acc: 0.7096,train F1-score:0.6532 Val Loss: 0.7373930811882019, Val Acc: 0.7651\n","Epoch [411/500], Loss: 0.9429967999458313, Train Acc: 0.7116,train F1-score:0.6556 Val Loss: 0.7377689480781555, Val Acc: 0.7637\n","Epoch [412/500], Loss: 0.9356098175048828, Train Acc: 0.7097,train F1-score:0.6555 Val Loss: 0.7378526926040649, Val Acc: 0.7631\n","Epoch [413/500], Loss: 0.9392663240432739, Train Acc: 0.7095,train F1-score:0.6541 Val Loss: 0.7377313375473022, Val Acc: 0.7637\n","Epoch [414/500], Loss: 0.9379785656929016, Train Acc: 0.7134,train F1-score:0.6588 Val Loss: 0.7382755875587463, Val Acc: 0.7644\n","Epoch [415/500], Loss: 0.9395516514778137, Train Acc: 0.7117,train F1-score:0.6574 Val Loss: 0.7375182509422302, Val Acc: 0.7664\n","Epoch [416/500], Loss: 0.9438265562057495, Train Acc: 0.7115,train F1-score:0.6582 Val Loss: 0.7376647591590881, Val Acc: 0.7664\n","Epoch [417/500], Loss: 0.940385103225708, Train Acc: 0.7103,train F1-score:0.6575 Val Loss: 0.7375420331954956, Val Acc: 0.7671\n","Epoch [418/500], Loss: 0.941535234451294, Train Acc: 0.7133,train F1-score:0.6599 Val Loss: 0.7371259331703186, Val Acc: 0.7671\n","Epoch [419/500], Loss: 0.9363723993301392, Train Acc: 0.7120,train F1-score:0.6586 Val Loss: 0.7369596362113953, Val Acc: 0.7677\n","Epoch [420/500], Loss: 0.937634289264679, Train Acc: 0.7150,train F1-score:0.6628 Val Loss: 0.7369021773338318, Val Acc: 0.7664\n","Epoch [421/500], Loss: 0.9377762079238892, Train Acc: 0.7137,train F1-score:0.6608 Val Loss: 0.7364110350608826, Val Acc: 0.7671\n","Epoch [422/500], Loss: 0.9388824105262756, Train Acc: 0.7140,train F1-score:0.6594 Val Loss: 0.7363832592964172, Val Acc: 0.7677\n","Epoch [423/500], Loss: 0.9363889098167419, Train Acc: 0.7088,train F1-score:0.6542 Val Loss: 0.7352398633956909, Val Acc: 0.7671\n","Epoch [424/500], Loss: 0.9358663558959961, Train Acc: 0.7120,train F1-score:0.6580 Val Loss: 0.7338532209396362, Val Acc: 0.7671\n","Epoch [425/500], Loss: 0.9352247714996338, Train Acc: 0.7119,train F1-score:0.6571 Val Loss: 0.7329459190368652, Val Acc: 0.7677\n","Epoch [426/500], Loss: 0.9357033967971802, Train Acc: 0.7129,train F1-score:0.6584 Val Loss: 0.7324174046516418, Val Acc: 0.7651\n","Epoch [427/500], Loss: 0.9365436434745789, Train Acc: 0.7103,train F1-score:0.6552 Val Loss: 0.7328779101371765, Val Acc: 0.7637\n","Epoch [428/500], Loss: 0.9410920143127441, Train Acc: 0.7111,train F1-score:0.6561 Val Loss: 0.7342262268066406, Val Acc: 0.7631\n","Epoch [429/500], Loss: 0.9378841519355774, Train Acc: 0.7110,train F1-score:0.6554 Val Loss: 0.7353982329368591, Val Acc: 0.7617\n","Epoch [430/500], Loss: 0.9446246027946472, Train Acc: 0.7078,train F1-score:0.6514 Val Loss: 0.7365701198577881, Val Acc: 0.7631\n","Epoch [431/500], Loss: 0.9349859952926636, Train Acc: 0.7108,train F1-score:0.6550 Val Loss: 0.7366641163825989, Val Acc: 0.7644\n","Epoch [432/500], Loss: 0.9236366748809814, Train Acc: 0.7177,train F1-score:0.6665 Val Loss: 0.7363682985305786, Val Acc: 0.7664\n","Epoch [433/500], Loss: 0.9316098093986511, Train Acc: 0.7129,train F1-score:0.6597 Val Loss: 0.7359456419944763, Val Acc: 0.7697\n","Epoch [434/500], Loss: 0.9392691850662231, Train Acc: 0.7081,train F1-score:0.6532 Val Loss: 0.736172616481781, Val Acc: 0.7691\n","Epoch [435/500], Loss: 0.9393340349197388, Train Acc: 0.7073,train F1-score:0.6527 Val Loss: 0.7353220582008362, Val Acc: 0.7711\n","Epoch [436/500], Loss: 0.9391103982925415, Train Acc: 0.7084,train F1-score:0.6543 Val Loss: 0.7356459498405457, Val Acc: 0.7718\n","Epoch [437/500], Loss: 0.930540919303894, Train Acc: 0.7104,train F1-score:0.6566 Val Loss: 0.7353432178497314, Val Acc: 0.7684\n","Epoch [438/500], Loss: 0.9346158504486084, Train Acc: 0.7128,train F1-score:0.6591 Val Loss: 0.7336537837982178, Val Acc: 0.7671\n","Epoch [439/500], Loss: 0.9333240389823914, Train Acc: 0.7145,train F1-score:0.6632 Val Loss: 0.7322920560836792, Val Acc: 0.7684\n","Epoch [440/500], Loss: 0.9326710104942322, Train Acc: 0.7123,train F1-score:0.6599 Val Loss: 0.7308517694473267, Val Acc: 0.7684\n","Epoch [441/500], Loss: 0.9286987781524658, Train Acc: 0.7102,train F1-score:0.6556 Val Loss: 0.7313036322593689, Val Acc: 0.7684\n","Epoch [442/500], Loss: 0.9291595816612244, Train Acc: 0.7119,train F1-score:0.6573 Val Loss: 0.73250412940979, Val Acc: 0.7664\n","Epoch [443/500], Loss: 0.9323762059211731, Train Acc: 0.7140,train F1-score:0.6593 Val Loss: 0.7343278527259827, Val Acc: 0.7671\n","Epoch [444/500], Loss: 0.9353963136672974, Train Acc: 0.7108,train F1-score:0.6552 Val Loss: 0.7358157634735107, Val Acc: 0.7637\n","Epoch [445/500], Loss: 0.9398006796836853, Train Acc: 0.7082,train F1-score:0.6528 Val Loss: 0.7350915670394897, Val Acc: 0.7631\n","Epoch [446/500], Loss: 0.9266826510429382, Train Acc: 0.7116,train F1-score:0.6575 Val Loss: 0.7342243194580078, Val Acc: 0.7664\n","Epoch [447/500], Loss: 0.9382280707359314, Train Acc: 0.7085,train F1-score:0.6540 Val Loss: 0.7333746552467346, Val Acc: 0.7671\n","Epoch [448/500], Loss: 0.9316099882125854, Train Acc: 0.7116,train F1-score:0.6564 Val Loss: 0.7333847880363464, Val Acc: 0.7697\n","Epoch [449/500], Loss: 0.9244751930236816, Train Acc: 0.7150,train F1-score:0.6637 Val Loss: 0.7339869141578674, Val Acc: 0.7731\n","Epoch [450/500], Loss: 0.93746417760849, Train Acc: 0.7099,train F1-score:0.6559 Val Loss: 0.7342291474342346, Val Acc: 0.7718\n","Epoch [451/500], Loss: 0.9264225959777832, Train Acc: 0.7158,train F1-score:0.6640 Val Loss: 0.7345882654190063, Val Acc: 0.7711\n","Epoch [452/500], Loss: 0.9395369291305542, Train Acc: 0.7087,train F1-score:0.6549 Val Loss: 0.7343582510948181, Val Acc: 0.7691\n","Epoch [453/500], Loss: 0.9384440779685974, Train Acc: 0.7100,train F1-score:0.6541 Val Loss: 0.7336238622665405, Val Acc: 0.7684\n","Epoch [454/500], Loss: 0.9332264065742493, Train Acc: 0.7115,train F1-score:0.6573 Val Loss: 0.7324503064155579, Val Acc: 0.7684\n","Epoch [455/500], Loss: 0.9283781051635742, Train Acc: 0.7134,train F1-score:0.6603 Val Loss: 0.731799840927124, Val Acc: 0.7691\n","Epoch [456/500], Loss: 0.9369145035743713, Train Acc: 0.7099,train F1-score:0.6551 Val Loss: 0.7317022681236267, Val Acc: 0.7677\n","Epoch [457/500], Loss: 0.9256018400192261, Train Acc: 0.7124,train F1-score:0.6573 Val Loss: 0.7316406965255737, Val Acc: 0.7664\n","Epoch [458/500], Loss: 0.929273784160614, Train Acc: 0.7137,train F1-score:0.6582 Val Loss: 0.7299452424049377, Val Acc: 0.7677\n","Epoch [459/500], Loss: 0.930905282497406, Train Acc: 0.7114,train F1-score:0.6577 Val Loss: 0.7293666005134583, Val Acc: 0.7691\n","Epoch [460/500], Loss: 0.9368494749069214, Train Acc: 0.7128,train F1-score:0.6588 Val Loss: 0.7292371988296509, Val Acc: 0.7704\n","Epoch [461/500], Loss: 0.9272353649139404, Train Acc: 0.7114,train F1-score:0.6577 Val Loss: 0.7298817038536072, Val Acc: 0.7697\n","Epoch [462/500], Loss: 0.9289466142654419, Train Acc: 0.7144,train F1-score:0.6620 Val Loss: 0.7302194833755493, Val Acc: 0.7711\n","Epoch [463/500], Loss: 0.9370227456092834, Train Acc: 0.7098,train F1-score:0.6553 Val Loss: 0.7306414246559143, Val Acc: 0.7691\n","Epoch [464/500], Loss: 0.9268761277198792, Train Acc: 0.7166,train F1-score:0.6665 Val Loss: 0.7295724153518677, Val Acc: 0.7691\n","Epoch [465/500], Loss: 0.9297780394554138, Train Acc: 0.7142,train F1-score:0.6633 Val Loss: 0.7290733456611633, Val Acc: 0.7684\n","Epoch [466/500], Loss: 0.9279785752296448, Train Acc: 0.7121,train F1-score:0.6583 Val Loss: 0.7291383743286133, Val Acc: 0.7657\n","Epoch [467/500], Loss: 0.9212183952331543, Train Acc: 0.7102,train F1-score:0.6561 Val Loss: 0.7292259931564331, Val Acc: 0.7664\n","Epoch [468/500], Loss: 0.9210826754570007, Train Acc: 0.7109,train F1-score:0.6558 Val Loss: 0.7287607192993164, Val Acc: 0.7677\n","Epoch [469/500], Loss: 0.9247183203697205, Train Acc: 0.7138,train F1-score:0.6611 Val Loss: 0.7294524908065796, Val Acc: 0.7691\n","Epoch [470/500], Loss: 0.9321300387382507, Train Acc: 0.7098,train F1-score:0.6541 Val Loss: 0.7310107946395874, Val Acc: 0.7684\n","Epoch [471/500], Loss: 0.9314883351325989, Train Acc: 0.7109,train F1-score:0.6568 Val Loss: 0.7304505109786987, Val Acc: 0.7718\n","Epoch [472/500], Loss: 0.9260808825492859, Train Acc: 0.7132,train F1-score:0.6595 Val Loss: 0.7290374040603638, Val Acc: 0.7711\n","Epoch [473/500], Loss: 0.9271856546401978, Train Acc: 0.7155,train F1-score:0.6624 Val Loss: 0.7281018495559692, Val Acc: 0.7711\n","Epoch [474/500], Loss: 0.9354109764099121, Train Acc: 0.7130,train F1-score:0.6587 Val Loss: 0.7268286943435669, Val Acc: 0.7704\n","Epoch [475/500], Loss: 0.921596884727478, Train Acc: 0.7109,train F1-score:0.6575 Val Loss: 0.7267282009124756, Val Acc: 0.7697\n","Epoch [476/500], Loss: 0.9230538606643677, Train Acc: 0.7161,train F1-score:0.6649 Val Loss: 0.7261173725128174, Val Acc: 0.7718\n","Epoch [477/500], Loss: 0.9231733679771423, Train Acc: 0.7143,train F1-score:0.6630 Val Loss: 0.7258727550506592, Val Acc: 0.7731\n","Epoch [478/500], Loss: 0.9289258718490601, Train Acc: 0.7119,train F1-score:0.6593 Val Loss: 0.725933849811554, Val Acc: 0.7704\n","Epoch [479/500], Loss: 0.9238144159317017, Train Acc: 0.7145,train F1-score:0.6631 Val Loss: 0.7253679633140564, Val Acc: 0.7691\n","Epoch [480/500], Loss: 0.9224840402603149, Train Acc: 0.7136,train F1-score:0.6621 Val Loss: 0.7257387042045593, Val Acc: 0.7704\n","Epoch [481/500], Loss: 0.9212676286697388, Train Acc: 0.7151,train F1-score:0.6630 Val Loss: 0.7265526056289673, Val Acc: 0.7718\n","Epoch [482/500], Loss: 0.9253495335578918, Train Acc: 0.7123,train F1-score:0.6593 Val Loss: 0.7280632853507996, Val Acc: 0.7711\n","Epoch [483/500], Loss: 0.9246402382850647, Train Acc: 0.7118,train F1-score:0.6587 Val Loss: 0.7291339039802551, Val Acc: 0.7751\n","Epoch [484/500], Loss: 0.9225800037384033, Train Acc: 0.7121,train F1-score:0.6599 Val Loss: 0.7303735017776489, Val Acc: 0.7738\n","Epoch [485/500], Loss: 0.916642427444458, Train Acc: 0.7194,train F1-score:0.6690 Val Loss: 0.7300087213516235, Val Acc: 0.7758\n","Epoch [486/500], Loss: 0.9251193404197693, Train Acc: 0.7150,train F1-score:0.6632 Val Loss: 0.7289007306098938, Val Acc: 0.7771\n","Epoch [487/500], Loss: 0.9281647801399231, Train Acc: 0.7140,train F1-score:0.6621 Val Loss: 0.7275564670562744, Val Acc: 0.7784\n","Epoch [488/500], Loss: 0.9203919172286987, Train Acc: 0.7122,train F1-score:0.6591 Val Loss: 0.7269176840782166, Val Acc: 0.7778\n","Epoch [489/500], Loss: 0.9185857176780701, Train Acc: 0.7105,train F1-score:0.6583 Val Loss: 0.7269501090049744, Val Acc: 0.7758\n","Epoch [490/500], Loss: 0.9148619174957275, Train Acc: 0.7153,train F1-score:0.6634 Val Loss: 0.7266926765441895, Val Acc: 0.7738\n","Epoch [491/500], Loss: 0.9172462821006775, Train Acc: 0.7154,train F1-score:0.6634 Val Loss: 0.7270104289054871, Val Acc: 0.7738\n","Epoch [492/500], Loss: 0.9256607890129089, Train Acc: 0.7090,train F1-score:0.6560 Val Loss: 0.7272800803184509, Val Acc: 0.7744\n","Epoch [493/500], Loss: 0.925327479839325, Train Acc: 0.7133,train F1-score:0.6603 Val Loss: 0.7272915840148926, Val Acc: 0.7731\n","Epoch [494/500], Loss: 0.9228707551956177, Train Acc: 0.7142,train F1-score:0.6626 Val Loss: 0.7275974154472351, Val Acc: 0.7758\n","Epoch [495/500], Loss: 0.9224057197570801, Train Acc: 0.7161,train F1-score:0.6650 Val Loss: 0.7272818684577942, Val Acc: 0.7771\n","Epoch [496/500], Loss: 0.9228730797767639, Train Acc: 0.7111,train F1-score:0.6601 Val Loss: 0.7249374985694885, Val Acc: 0.7818\n","Epoch [497/500], Loss: 0.906135618686676, Train Acc: 0.7202,train F1-score:0.6718 Val Loss: 0.723892092704773, Val Acc: 0.7825\n","Epoch [498/500], Loss: 0.9250085353851318, Train Acc: 0.7126,train F1-score:0.6622 Val Loss: 0.724398136138916, Val Acc: 0.7811\n","Epoch [499/500], Loss: 0.9159740209579468, Train Acc: 0.7145,train F1-score:0.6635 Val Loss: 0.7249621748924255, Val Acc: 0.7791\n","Epoch [500/500], Loss: 0.9130660891532898, Train Acc: 0.7195,train F1-score:0.6700 Val Loss: 0.7243767380714417, Val Acc: 0.7778\n","Test Loss: 0.7561821937561035, Test Accuracy: 0.7593833780160858\n","Precision: 0.7610, Recall: 0.7594, F1-score: 0.7143\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GCNNet(nn.Module):\n","    def __init__(self, num_features, hidden_dim, output_dim, num_layers=1):\n","        super(GCNNet, self).__init__()\n","\n","\n","        self.conv1 = GCNConv(num_features, hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=0.5, training=self.training)\n","\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=1)\n","\n","\n","# Define optimizer and loss function\n","model = GCNNet(num_features=X_train.shape[1], hidden_dim=8, output_dim=13).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = nn.CrossEntropyLoss()  # Define criterion here\n","\n","\n","\n","\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/train_f1_scores_GCN.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/train_loss_GCN.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/epochs_GCN.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1714298776226,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"ib8HHiYeJbfx","outputId":"99de81e1-a41c-439b-a16f-8a24b80ef1c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.716714118238151\n","Validation Accuracy (Decision Tree): 0.7423025435073628\n","Test Accuracy (Decision Tree): 0.7091152815013405\n","Precision: 0.6278, Recall: 0.7091, F1-score: 0.6617\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GCN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train,edge_index_train).cpu().numpy()\n","    val_outputs = model(X_val,edge_index_val).cpu().numpy()\n","    test_outputs = model(X_test,edge_index_test).cpu().numpy()\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=2)\n","decision_tree.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(train_outputs)\n","val_pred = decision_tree.predict(val_outputs)\n","test_pred = decision_tree.predict(test_outputs)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2461,"status":"ok","timestamp":1714298781536,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"ECDwmMuWNIns","outputId":"a7c52a58-e042-4b6e-c219-b75c356813f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.8913079886116229\n","Validation Accuracy (Decision Tree): 0.8681392235609103\n","Test Accuracy (Decision Tree): 0.8659517426273459\n","Precision: 0.8549, Recall: 0.8660, F1-score: 0.8569\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Concatenate the GIN model output with the original feature matrices\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=9)\n","decision_tree.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(X_train_combined)\n","val_pred = decision_tree.predict(X_val_combined)\n","test_pred = decision_tree.predict(X_test_combined)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lbm4QrVOPQZ"},"outputs":[],"source":["from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112729,"status":"ok","timestamp":1714298900018,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"},"user_tz":-330},"id":"pc5KOe42OUhs","outputId":"2f7625d9-16cb-4d9e-a672-156109bc6e30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.7816948584826662\n","Validation Accuracy (SVM): 0.8125836680053548\n","Test Accuracy (SVM): 0.7989276139410187\n","Precision: 0.7810, Recall: 0.7989, F1-score: 0.7775\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(train_outputs)\n","val_pred = svm_classifier.predict(val_outputs)\n","test_pred = svm_classifier.predict(test_outputs)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HabEU6wMOXf8","executionInfo":{"status":"ok","timestamp":1714298942569,"user_tz":-330,"elapsed":42559,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"bc17abfd-fd7f-49cd-e454-c318147ad86a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.8542957628537934\n","Validation Accuracy (SVM): 0.8500669344042838\n","Test Accuracy (SVM): 0.8438337801608579\n","Precision: 0.8371, Recall: 0.8438, F1-score: 0.8338\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(X_train_combined)\n","val_pred = svm_classifier.predict(X_val_combined)\n","test_pred = svm_classifier.predict(X_test_combined)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGRpSulNF+hLewI/+HnTnI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}