{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1tjKnM0hSQt58vFmKjB3HuCcLw1Zbx5eK","authorship_tag":"ABX9TyNe9qQZlu469K4S/pg0wC5i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PohgGJsLE4s_","executionInfo":{"status":"ok","timestamp":1714301625929,"user_tz":-330,"elapsed":14882,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"98d715cb-e4ab-4964-e2d0-4f090ba0f311"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["pip install torch-geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-1aVIRdFe7X","executionInfo":{"status":"ok","timestamp":1714301656281,"user_tz":-330,"elapsed":30359,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"9a9a776f-0ee6-4233-f2c0-22ef02e66322"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.5.3\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GINConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GINNet(torch.nn.Module):\n","    def __init__(self, num_features, hidden_dim, num_layers, output_dim):\n","        super(GINNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(num_features, hidden_dim), torch.nn.ReLU())))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU())))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","        x = F.relu(x)\n","        return self.fc(x)\n","\n","\n","# Define model\n","model = GINNet(num_features=X_train.shape[1], hidden_dim=128, num_layers=2, output_dim=13).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","        np.savetxt('/content/drive/MyDrive/PROJECT/train_f1_scores_GIN.txt',train_f1_scores)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/train_loss_GIN.txt', train_losses)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/epochs_GIN.txt', epochss)\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/f1_score_GIN.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n","\n","\n"],"metadata":{"id":"xF-9gTqNIiJ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714292481728,"user_tz":-330,"elapsed":47703,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"1ef93d53-4d5c-46aa-a4d9-87a365efed42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 3.0317959785461426, Train Acc: 0.0291, Val Loss: 2.1983256340026855, Val Acc: 0.6787\n","Epoch [2/500], Loss: 2.664438009262085, Train Acc: 0.6441, Val Loss: 1.9822043180465698, Val Acc: 0.6787\n","Epoch [3/500], Loss: 2.2680344581604004, Train Acc: 0.6444, Val Loss: 1.8471026420593262, Val Acc: 0.4618\n","Epoch [4/500], Loss: 2.358961582183838, Train Acc: 0.4313, Val Loss: 1.654290795326233, Val Acc: 0.6807\n","Epoch [5/500], Loss: 1.9083997011184692, Train Acc: 0.6347, Val Loss: 1.5664993524551392, Val Acc: 0.6787\n","Epoch [6/500], Loss: 1.798446536064148, Train Acc: 0.6442, Val Loss: 1.5926463603973389, Val Acc: 0.6232\n","Epoch [7/500], Loss: 1.777637004852295, Train Acc: 0.5896, Val Loss: 1.5116597414016724, Val Acc: 0.6299\n","Epoch [8/500], Loss: 1.7506234645843506, Train Acc: 0.5848, Val Loss: 1.4020618200302124, Val Acc: 0.6787\n","Epoch [9/500], Loss: 1.7494885921478271, Train Acc: 0.6440, Val Loss: 1.3518710136413574, Val Acc: 0.6787\n","Epoch [10/500], Loss: 1.7024526596069336, Train Acc: 0.6444, Val Loss: 1.323788046836853, Val Acc: 0.6801\n","Epoch [11/500], Loss: 1.5685298442840576, Train Acc: 0.6434, Val Loss: 1.3287959098815918, Val Acc: 0.6774\n","Epoch [12/500], Loss: 1.4465426206588745, Train Acc: 0.6408, Val Loss: 1.3554213047027588, Val Acc: 0.6687\n","Epoch [13/500], Loss: 1.3883076906204224, Train Acc: 0.6260, Val Loss: 1.3775088787078857, Val Acc: 0.6539\n","Epoch [14/500], Loss: 1.4175740480422974, Train Acc: 0.5896, Val Loss: 1.2979081869125366, Val Acc: 0.6707\n","Epoch [15/500], Loss: 1.3423889875411987, Train Acc: 0.6025, Val Loss: 1.211423397064209, Val Acc: 0.6754\n","Epoch [16/500], Loss: 1.288094162940979, Train Acc: 0.6467, Val Loss: 1.1671593189239502, Val Acc: 0.6801\n","Epoch [17/500], Loss: 1.2851450443267822, Train Acc: 0.6493, Val Loss: 1.1429067850112915, Val Acc: 0.6774\n","Epoch [18/500], Loss: 1.2663486003875732, Train Acc: 0.6475, Val Loss: 1.1159592866897583, Val Acc: 0.6814\n","Epoch [19/500], Loss: 1.2217350006103516, Train Acc: 0.6485, Val Loss: 1.0874769687652588, Val Acc: 0.6934\n","Epoch [20/500], Loss: 1.1853129863739014, Train Acc: 0.6556, Val Loss: 1.063706636428833, Val Acc: 0.6901\n","Epoch [21/500], Loss: 1.1441165208816528, Train Acc: 0.6562, Val Loss: 1.0510525703430176, Val Acc: 0.6867\n","Epoch [22/500], Loss: 1.12046217918396, Train Acc: 0.6516, Val Loss: 1.0412310361862183, Val Acc: 0.6894\n","Epoch [23/500], Loss: 1.1070021390914917, Train Acc: 0.6562, Val Loss: 1.0212759971618652, Val Acc: 0.6901\n","Epoch [24/500], Loss: 1.0882925987243652, Train Acc: 0.6568, Val Loss: 0.9936517477035522, Val Acc: 0.6867\n","Epoch [25/500], Loss: 1.0602664947509766, Train Acc: 0.6584, Val Loss: 0.9682548642158508, Val Acc: 0.6861\n","Epoch [26/500], Loss: 1.0365761518478394, Train Acc: 0.6569, Val Loss: 0.9504031538963318, Val Acc: 0.6841\n","Epoch [27/500], Loss: 1.0264201164245605, Train Acc: 0.6569, Val Loss: 0.9369738101959229, Val Acc: 0.6787\n","Epoch [28/500], Loss: 1.0113437175750732, Train Acc: 0.6542, Val Loss: 0.9242534637451172, Val Acc: 0.6774\n","Epoch [29/500], Loss: 1.0041546821594238, Train Acc: 0.6516, Val Loss: 0.9089280962944031, Val Acc: 0.6787\n","Epoch [30/500], Loss: 0.9868869781494141, Train Acc: 0.6542, Val Loss: 0.8962398171424866, Val Acc: 0.6861\n","Epoch [31/500], Loss: 0.9710560441017151, Train Acc: 0.6613, Val Loss: 0.8855468034744263, Val Acc: 0.6995\n","Epoch [32/500], Loss: 0.9586877226829529, Train Acc: 0.6688, Val Loss: 0.8747008442878723, Val Acc: 0.7095\n","Epoch [33/500], Loss: 0.9533065557479858, Train Acc: 0.6764, Val Loss: 0.8658658862113953, Val Acc: 0.7122\n","Epoch [34/500], Loss: 0.9405004978179932, Train Acc: 0.6819, Val Loss: 0.8603854179382324, Val Acc: 0.7149\n","Epoch [35/500], Loss: 0.9243513345718384, Train Acc: 0.6863, Val Loss: 0.8538187742233276, Val Acc: 0.7135\n","Epoch [36/500], Loss: 0.9155341386795044, Train Acc: 0.6851, Val Loss: 0.8434689044952393, Val Acc: 0.7122\n","Epoch [37/500], Loss: 0.9044560194015503, Train Acc: 0.6841, Val Loss: 0.8363592028617859, Val Acc: 0.7155\n","Epoch [38/500], Loss: 0.8939871788024902, Train Acc: 0.6883, Val Loss: 0.8290191292762756, Val Acc: 0.7229\n","Epoch [39/500], Loss: 0.8828125, Train Acc: 0.6944, Val Loss: 0.8219210505485535, Val Acc: 0.7282\n","Epoch [40/500], Loss: 0.8735216856002808, Train Acc: 0.7033, Val Loss: 0.8150301575660706, Val Acc: 0.7363\n","Epoch [41/500], Loss: 0.8639985918998718, Train Acc: 0.7099, Val Loss: 0.8100608587265015, Val Acc: 0.7383\n","Epoch [42/500], Loss: 0.8542668223381042, Train Acc: 0.7158, Val Loss: 0.8059113621711731, Val Acc: 0.7443\n","Epoch [43/500], Loss: 0.8483870625495911, Train Acc: 0.7182, Val Loss: 0.8004725575447083, Val Acc: 0.7450\n","Epoch [44/500], Loss: 0.8374282717704773, Train Acc: 0.7201, Val Loss: 0.7966539859771729, Val Acc: 0.7423\n","Epoch [45/500], Loss: 0.8324939608573914, Train Acc: 0.7196, Val Loss: 0.7966846227645874, Val Acc: 0.7456\n","Epoch [46/500], Loss: 0.8256787061691284, Train Acc: 0.7225, Val Loss: 0.7926388382911682, Val Acc: 0.7497\n","Epoch [47/500], Loss: 0.8190261125564575, Train Acc: 0.7242, Val Loss: 0.7837942838668823, Val Acc: 0.7510\n","Epoch [48/500], Loss: 0.8085691928863525, Train Acc: 0.7266, Val Loss: 0.776519238948822, Val Acc: 0.7530\n","Epoch [49/500], Loss: 0.8056944608688354, Train Acc: 0.7273, Val Loss: 0.7728108763694763, Val Acc: 0.7550\n","Epoch [50/500], Loss: 0.796164870262146, Train Acc: 0.7299, Val Loss: 0.7694849371910095, Val Acc: 0.7544\n","Epoch [51/500], Loss: 0.7887202501296997, Train Acc: 0.7320, Val Loss: 0.7648310661315918, Val Acc: 0.7550\n","Epoch [52/500], Loss: 0.7795847058296204, Train Acc: 0.7325, Val Loss: 0.7608224153518677, Val Acc: 0.7530\n","Epoch [53/500], Loss: 0.7737942337989807, Train Acc: 0.7320, Val Loss: 0.7577816247940063, Val Acc: 0.7503\n","Epoch [54/500], Loss: 0.7664536237716675, Train Acc: 0.7322, Val Loss: 0.7539214491844177, Val Acc: 0.7510\n","Epoch [55/500], Loss: 0.759067177772522, Train Acc: 0.7350, Val Loss: 0.7484476566314697, Val Acc: 0.7537\n","Epoch [56/500], Loss: 0.7530587315559387, Train Acc: 0.7369, Val Loss: 0.7404084801673889, Val Acc: 0.7564\n","Epoch [57/500], Loss: 0.7454992532730103, Train Acc: 0.7381, Val Loss: 0.734468400478363, Val Acc: 0.7604\n","Epoch [58/500], Loss: 0.7400826215744019, Train Acc: 0.7412, Val Loss: 0.732754111289978, Val Acc: 0.7624\n","Epoch [59/500], Loss: 0.7333223223686218, Train Acc: 0.7433, Val Loss: 0.7326796054840088, Val Acc: 0.7677\n","Epoch [60/500], Loss: 0.7274750471115112, Train Acc: 0.7467, Val Loss: 0.7348707914352417, Val Acc: 0.7671\n","Epoch [61/500], Loss: 0.7212060689926147, Train Acc: 0.7488, Val Loss: 0.7334225177764893, Val Acc: 0.7697\n","Epoch [62/500], Loss: 0.71612548828125, Train Acc: 0.7515, Val Loss: 0.7232919931411743, Val Acc: 0.7724\n","Epoch [63/500], Loss: 0.710884153842926, Train Acc: 0.7533, Val Loss: 0.7170956134796143, Val Acc: 0.7751\n","Epoch [64/500], Loss: 0.7042179107666016, Train Acc: 0.7564, Val Loss: 0.7132288217544556, Val Acc: 0.7744\n","Epoch [65/500], Loss: 0.6999622583389282, Train Acc: 0.7579, Val Loss: 0.7026287317276001, Val Acc: 0.7764\n","Epoch [66/500], Loss: 0.6932418346405029, Train Acc: 0.7605, Val Loss: 0.6972317099571228, Val Acc: 0.7784\n","Epoch [67/500], Loss: 0.6882718205451965, Train Acc: 0.7618, Val Loss: 0.6968233585357666, Val Acc: 0.7778\n","Epoch [68/500], Loss: 0.6818877458572388, Train Acc: 0.7644, Val Loss: 0.6946265697479248, Val Acc: 0.7798\n","Epoch [69/500], Loss: 0.677240252494812, Train Acc: 0.7663, Val Loss: 0.6863890290260315, Val Acc: 0.7811\n","Epoch [70/500], Loss: 0.6721294522285461, Train Acc: 0.7668, Val Loss: 0.6823869347572327, Val Acc: 0.7811\n","Epoch [71/500], Loss: 0.6667518019676208, Train Acc: 0.7695, Val Loss: 0.6802711486816406, Val Acc: 0.7825\n","Epoch [72/500], Loss: 0.6618040800094604, Train Acc: 0.7706, Val Loss: 0.6709949970245361, Val Acc: 0.7885\n","Epoch [73/500], Loss: 0.6559779644012451, Train Acc: 0.7718, Val Loss: 0.6649642586708069, Val Acc: 0.7918\n","Epoch [74/500], Loss: 0.651329755783081, Train Acc: 0.7732, Val Loss: 0.6616353392601013, Val Acc: 0.7952\n","Epoch [75/500], Loss: 0.6464294195175171, Train Acc: 0.7763, Val Loss: 0.6544369459152222, Val Acc: 0.7959\n","Epoch [76/500], Loss: 0.6416611671447754, Train Acc: 0.7783, Val Loss: 0.64752197265625, Val Acc: 0.7972\n","Epoch [77/500], Loss: 0.6368510127067566, Train Acc: 0.7790, Val Loss: 0.6451953053474426, Val Acc: 0.7972\n","Epoch [78/500], Loss: 0.6328675150871277, Train Acc: 0.7794, Val Loss: 0.643205463886261, Val Acc: 0.7979\n","Epoch [79/500], Loss: 0.6281947493553162, Train Acc: 0.7807, Val Loss: 0.6372920274734497, Val Acc: 0.7999\n","Epoch [80/500], Loss: 0.6232731938362122, Train Acc: 0.7822, Val Loss: 0.6341474056243896, Val Acc: 0.7979\n","Epoch [81/500], Loss: 0.6188293695449829, Train Acc: 0.7845, Val Loss: 0.6327341794967651, Val Acc: 0.7985\n","Epoch [82/500], Loss: 0.6140022873878479, Train Acc: 0.7855, Val Loss: 0.6353260278701782, Val Acc: 0.7979\n","Epoch [83/500], Loss: 0.6099711656570435, Train Acc: 0.7871, Val Loss: 0.627985954284668, Val Acc: 0.8005\n","Epoch [84/500], Loss: 0.6051319241523743, Train Acc: 0.7892, Val Loss: 0.6212549209594727, Val Acc: 0.8039\n","Epoch [85/500], Loss: 0.6009128093719482, Train Acc: 0.7923, Val Loss: 0.6197754740715027, Val Acc: 0.8046\n","Epoch [86/500], Loss: 0.5967622995376587, Train Acc: 0.7931, Val Loss: 0.6187753677368164, Val Acc: 0.8052\n","Epoch [87/500], Loss: 0.5924148559570312, Train Acc: 0.7938, Val Loss: 0.6126984357833862, Val Acc: 0.8072\n","Epoch [88/500], Loss: 0.5890198349952698, Train Acc: 0.7960, Val Loss: 0.6107816100120544, Val Acc: 0.8072\n","Epoch [89/500], Loss: 0.5851460695266724, Train Acc: 0.7981, Val Loss: 0.6016682982444763, Val Acc: 0.8079\n","Epoch [90/500], Loss: 0.5824083685874939, Train Acc: 0.7971, Val Loss: 0.6068549752235413, Val Acc: 0.8072\n","Epoch [91/500], Loss: 0.5760307312011719, Train Acc: 0.7985, Val Loss: 0.6039010286331177, Val Acc: 0.8092\n","Epoch [92/500], Loss: 0.5747961401939392, Train Acc: 0.7979, Val Loss: 0.5891157984733582, Val Acc: 0.8106\n","Epoch [93/500], Loss: 0.5704321265220642, Train Acc: 0.7999, Val Loss: 0.5864559412002563, Val Acc: 0.8112\n","Epoch [94/500], Loss: 0.5668141841888428, Train Acc: 0.8034, Val Loss: 0.5896369218826294, Val Acc: 0.8133\n","Epoch [95/500], Loss: 0.5615524649620056, Train Acc: 0.8041, Val Loss: 0.5885425806045532, Val Acc: 0.8133\n","Epoch [96/500], Loss: 0.558260440826416, Train Acc: 0.8041, Val Loss: 0.5810093283653259, Val Acc: 0.8146\n","Epoch [97/500], Loss: 0.5540510416030884, Train Acc: 0.8057, Val Loss: 0.5730206966400146, Val Acc: 0.8133\n","Epoch [98/500], Loss: 0.5496769547462463, Train Acc: 0.8066, Val Loss: 0.5706787109375, Val Acc: 0.8133\n","Epoch [99/500], Loss: 0.546062171459198, Train Acc: 0.8068, Val Loss: 0.5732582211494446, Val Acc: 0.8126\n","Epoch [100/500], Loss: 0.5420373678207397, Train Acc: 0.8073, Val Loss: 0.5726763606071472, Val Acc: 0.8159\n","Epoch [101/500], Loss: 0.5379530191421509, Train Acc: 0.8097, Val Loss: 0.5618149638175964, Val Acc: 0.8186\n","Epoch [102/500], Loss: 0.5333419442176819, Train Acc: 0.8104, Val Loss: 0.5570423603057861, Val Acc: 0.8179\n","Epoch [103/500], Loss: 0.5298556089401245, Train Acc: 0.8118, Val Loss: 0.5628397464752197, Val Acc: 0.8179\n","Epoch [104/500], Loss: 0.5255058407783508, Train Acc: 0.8140, Val Loss: 0.5581117272377014, Val Acc: 0.8193\n","Epoch [105/500], Loss: 0.5215196013450623, Train Acc: 0.8143, Val Loss: 0.553335428237915, Val Acc: 0.8199\n","Epoch [106/500], Loss: 0.5168987512588501, Train Acc: 0.8164, Val Loss: 0.5511236190795898, Val Acc: 0.8199\n","Epoch [107/500], Loss: 0.5130326747894287, Train Acc: 0.8190, Val Loss: 0.55073481798172, Val Acc: 0.8206\n","Epoch [108/500], Loss: 0.5098061561584473, Train Acc: 0.8196, Val Loss: 0.5405063033103943, Val Acc: 0.8226\n","Epoch [109/500], Loss: 0.5054060816764832, Train Acc: 0.8213, Val Loss: 0.5389678478240967, Val Acc: 0.8213\n","Epoch [110/500], Loss: 0.5014981031417847, Train Acc: 0.8221, Val Loss: 0.54140305519104, Val Acc: 0.8266\n","Epoch [111/500], Loss: 0.49777543544769287, Train Acc: 0.8241, Val Loss: 0.551723062992096, Val Acc: 0.8260\n","Epoch [112/500], Loss: 0.4935172200202942, Train Acc: 0.8263, Val Loss: 0.5450453162193298, Val Acc: 0.8266\n","Epoch [113/500], Loss: 0.4896976947784424, Train Acc: 0.8284, Val Loss: 0.5418749451637268, Val Acc: 0.8260\n","Epoch [114/500], Loss: 0.4867425262928009, Train Acc: 0.8295, Val Loss: 0.5379819869995117, Val Acc: 0.8240\n","Epoch [115/500], Loss: 0.48270702362060547, Train Acc: 0.8290, Val Loss: 0.5356534123420715, Val Acc: 0.8266\n","Epoch [116/500], Loss: 0.4787466824054718, Train Acc: 0.8329, Val Loss: 0.5391073822975159, Val Acc: 0.8320\n","Epoch [117/500], Loss: 0.47553595900535583, Train Acc: 0.8324, Val Loss: 0.5377457737922668, Val Acc: 0.8280\n","Epoch [118/500], Loss: 0.4728306233882904, Train Acc: 0.8346, Val Loss: 0.5341139435768127, Val Acc: 0.8340\n","Epoch [119/500], Loss: 0.4724036157131195, Train Acc: 0.8339, Val Loss: 0.5327522158622742, Val Acc: 0.8307\n","Epoch [120/500], Loss: 0.4668867886066437, Train Acc: 0.8370, Val Loss: 0.5280056595802307, Val Acc: 0.8347\n","Epoch [121/500], Loss: 0.4639628529548645, Train Acc: 0.8374, Val Loss: 0.5285085439682007, Val Acc: 0.8360\n","Epoch [122/500], Loss: 0.46395230293273926, Train Acc: 0.8382, Val Loss: 0.5387623906135559, Val Acc: 0.8367\n","Epoch [123/500], Loss: 0.46146532893180847, Train Acc: 0.8423, Val Loss: 0.5484917163848877, Val Acc: 0.8327\n","Epoch [124/500], Loss: 0.4598747491836548, Train Acc: 0.8365, Val Loss: 0.5286056995391846, Val Acc: 0.8307\n","Epoch [125/500], Loss: 0.45283976197242737, Train Acc: 0.8409, Val Loss: 0.5248983502388, Val Acc: 0.8347\n","Epoch [126/500], Loss: 0.45111143589019775, Train Acc: 0.8456, Val Loss: 0.5331001877784729, Val Acc: 0.8387\n","Epoch [127/500], Loss: 0.44748884439468384, Train Acc: 0.8433, Val Loss: 0.5382424592971802, Val Acc: 0.8347\n","Epoch [128/500], Loss: 0.4431615471839905, Train Acc: 0.8486, Val Loss: 0.533862829208374, Val Acc: 0.8367\n","Epoch [129/500], Loss: 0.44087710976600647, Train Acc: 0.8480, Val Loss: 0.5376709699630737, Val Acc: 0.8347\n","Epoch [130/500], Loss: 0.44352778792381287, Train Acc: 0.8462, Val Loss: 0.5318074226379395, Val Acc: 0.8420\n","Epoch [131/500], Loss: 0.4431302547454834, Train Acc: 0.8477, Val Loss: 0.5303255915641785, Val Acc: 0.8400\n","Epoch [132/500], Loss: 0.4452143609523773, Train Acc: 0.8497, Val Loss: 0.5213861465454102, Val Acc: 0.8394\n","Epoch [133/500], Loss: 0.4433244466781616, Train Acc: 0.8457, Val Loss: 0.5461031794548035, Val Acc: 0.8367\n","Epoch [134/500], Loss: 0.4450749158859253, Train Acc: 0.8470, Val Loss: 0.562551736831665, Val Acc: 0.8420\n","Epoch [135/500], Loss: 0.4371761679649353, Train Acc: 0.8492, Val Loss: 0.5575370192527771, Val Acc: 0.8440\n","Epoch [136/500], Loss: 0.43199723958969116, Train Acc: 0.8497, Val Loss: 0.5405085682868958, Val Acc: 0.8400\n","Epoch [137/500], Loss: 0.42908358573913574, Train Acc: 0.8504, Val Loss: 0.5365873575210571, Val Acc: 0.8380\n","Epoch [138/500], Loss: 0.43103519082069397, Train Acc: 0.8483, Val Loss: 0.5300381779670715, Val Acc: 0.8394\n","Epoch [139/500], Loss: 0.42536333203315735, Train Acc: 0.8559, Val Loss: 0.5342192053794861, Val Acc: 0.8394\n","Epoch [140/500], Loss: 0.41833677887916565, Train Acc: 0.8547, Val Loss: 0.5400156378746033, Val Acc: 0.8394\n","Epoch [141/500], Loss: 0.41618284583091736, Train Acc: 0.8536, Val Loss: 0.5336739420890808, Val Acc: 0.8387\n","Epoch [142/500], Loss: 0.41823697090148926, Train Acc: 0.8561, Val Loss: 0.520711362361908, Val Acc: 0.8467\n","Epoch [143/500], Loss: 0.4172813594341278, Train Acc: 0.8556, Val Loss: 0.5184735655784607, Val Acc: 0.8414\n","Epoch [144/500], Loss: 0.40483328700065613, Train Acc: 0.8571, Val Loss: 0.5355505347251892, Val Acc: 0.8400\n","Epoch [145/500], Loss: 0.41192424297332764, Train Acc: 0.8560, Val Loss: 0.5451685190200806, Val Acc: 0.8427\n","Epoch [146/500], Loss: 0.4063414931297302, Train Acc: 0.8567, Val Loss: 0.5535303950309753, Val Acc: 0.8461\n","Epoch [147/500], Loss: 0.40631574392318726, Train Acc: 0.8575, Val Loss: 0.5438751578330994, Val Acc: 0.8414\n","Epoch [148/500], Loss: 0.39967241883277893, Train Acc: 0.8604, Val Loss: 0.536285936832428, Val Acc: 0.8394\n","Epoch [149/500], Loss: 0.3945755362510681, Train Acc: 0.8587, Val Loss: 0.532944917678833, Val Acc: 0.8467\n","Epoch [150/500], Loss: 0.39271897077560425, Train Acc: 0.8592, Val Loss: 0.5219581723213196, Val Acc: 0.8481\n","Epoch [151/500], Loss: 0.3890759348869324, Train Acc: 0.8629, Val Loss: 0.5231715440750122, Val Acc: 0.8461\n","Epoch [152/500], Loss: 0.38386037945747375, Train Acc: 0.8664, Val Loss: 0.5139727592468262, Val Acc: 0.8481\n","Epoch [153/500], Loss: 0.38001886010169983, Train Acc: 0.8659, Val Loss: 0.5166881084442139, Val Acc: 0.8494\n","Epoch [154/500], Loss: 0.37854331731796265, Train Acc: 0.8648, Val Loss: 0.5217766761779785, Val Acc: 0.8487\n","Epoch [155/500], Loss: 0.3752918541431427, Train Acc: 0.8690, Val Loss: 0.5230514407157898, Val Acc: 0.8474\n","Epoch [156/500], Loss: 0.3733655512332916, Train Acc: 0.8684, Val Loss: 0.5272464156150818, Val Acc: 0.8527\n","Epoch [157/500], Loss: 0.3688899874687195, Train Acc: 0.8699, Val Loss: 0.5289362072944641, Val Acc: 0.8501\n","Epoch [158/500], Loss: 0.36703407764434814, Train Acc: 0.8698, Val Loss: 0.5270577073097229, Val Acc: 0.8521\n","Epoch [159/500], Loss: 0.366107314825058, Train Acc: 0.8708, Val Loss: 0.5310792922973633, Val Acc: 0.8501\n","Epoch [160/500], Loss: 0.36608564853668213, Train Acc: 0.8750, Val Loss: 0.5223886966705322, Val Acc: 0.8521\n","Epoch [161/500], Loss: 0.36232489347457886, Train Acc: 0.8711, Val Loss: 0.5269998908042908, Val Acc: 0.8534\n","Early stopping at epoch 161\n","Test Loss: 0.58343106508255, Test Accuracy: 0.8545576407506702\n","Precision: 0.8507, Recall: 0.8546, F1-score: 0.8504\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["GIN + DECISION TREE\n"],"metadata":{"id":"MmwEsMjEAZXH"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=2)\n","decision_tree.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(train_outputs)\n","val_pred = decision_tree.predict(val_outputs)\n","test_pred = decision_tree.predict(test_outputs)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GIN_DT.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n"],"metadata":{"id":"Ej6ApBEi-Dcw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714292487145,"user_tz":-330,"elapsed":616,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"1ccde546-5603-455d-96c4-7a8b39c56d3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.7230782113548819\n","Validation Accuracy (Decision Tree): 0.7101740294511378\n","Test Accuracy (Decision Tree): 0.7017426273458445\n","Precision: 0.6494, Recall: 0.7017, F1-score: 0.6670\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["GIN + DECISION TREE(COMBINED INPUT)"],"metadata":{"id":"DKG3q5uCAdHi"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Concatenate the GIN model output with the original feature matrices\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=9)\n","decision_tree.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(X_train_combined)\n","val_pred = decision_tree.predict(X_val_combined)\n","test_pred = decision_tree.predict(X_test_combined)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GIN_DT_COMB.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pt4JWFquADJr","executionInfo":{"status":"ok","timestamp":1714292495339,"user_tz":-330,"elapsed":2411,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"0c136e47-cc95-4b71-afa3-a8fd0c758a55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.9151733377993636\n","Validation Accuracy (Decision Tree): 0.8641231593038822\n","Test Accuracy (Decision Tree): 0.868632707774799\n","Precision: 0.8651, Recall: 0.8686, F1-score: 0.8634\n","F1-score saved to file.\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(train_outputs)\n","val_pred = svm_classifier.predict(val_outputs)\n","test_pred = svm_classifier.predict(test_outputs)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GIN_SVM.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTmIla3VE6Xx","executionInfo":{"status":"ok","timestamp":1714292513312,"user_tz":-330,"elapsed":15289,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"82024785-dc03-4f2d-ffe5-9c996dc34b66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.8725506615307319\n","Validation Accuracy (SVM): 0.8507362784471219\n","Test Accuracy (SVM): 0.8558981233243967\n","Precision: 0.8512, Recall: 0.8559, F1-score: 0.8524\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_test).cpu().numpy()\n","\n","# Combine the output of the GIN model with the original features\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(X_train_combined)\n","val_pred = svm_classifier.predict(X_val_combined)\n","test_pred = svm_classifier.predict(X_test_combined)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GIN_SVM_COMB.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n"],"metadata":{"id":"SaGeRzS0Flfv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714292541812,"user_tz":-330,"elapsed":28502,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"bffdfa0a-84eb-46ce-fd12-b90adff03724"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9024451515659019\n","Validation Accuracy (SVM): 0.8708165997322623\n","Test Accuracy (SVM): 0.8726541554959786\n","Precision: 0.8716, Recall: 0.8727, F1-score: 0.8711\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GINConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GINNet(torch.nn.Module):\n","    def __init__(self, num_features, hidden_dim, num_layers, output_dim):\n","        super(GINNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(num_features, hidden_dim), torch.nn.ReLU())))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU())))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","        x = F.relu(x)\n","        return self.fc(x)\n","\n","\n","# Define model\n","model = GINNet(num_features=X_train.shape[1], hidden_dim=128, num_layers=1, output_dim=13).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model1.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer1/train_f1_scores_GIN.txt',train_f1_scores)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer1/train_loss_GIN.txt', train_losses)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer1/epochs_GIN.txt', epochss)\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/layer1/f1_score_GIN.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n","\n","\n"],"metadata":{"id":"95EOrWymlA6-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714301740767,"user_tz":-330,"elapsed":77060,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"9e0fb255-236e-46b6-bfe9-b08f0e0c9007"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.681000232696533, Train Acc: 0.0265, Val Loss: 2.1388442516326904, Val Acc: 0.4665\n","Epoch [2/500], Loss: 2.1094863414764404, Train Acc: 0.4824, Val Loss: 1.663632869720459, Val Acc: 0.6888\n","Epoch [3/500], Loss: 1.6862798929214478, Train Acc: 0.6647, Val Loss: 1.2600630521774292, Val Acc: 0.7095\n","Epoch [4/500], Loss: 1.349033236503601, Train Acc: 0.6763, Val Loss: 1.0377941131591797, Val Acc: 0.7175\n","Epoch [5/500], Loss: 1.198499321937561, Train Acc: 0.6738, Val Loss: 0.9638200998306274, Val Acc: 0.7142\n","Epoch [6/500], Loss: 1.1645467281341553, Train Acc: 0.6671, Val Loss: 0.9305998682975769, Val Acc: 0.7175\n","Epoch [7/500], Loss: 1.1316581964492798, Train Acc: 0.6779, Val Loss: 0.9122809767723083, Val Acc: 0.7329\n","Epoch [8/500], Loss: 1.0889737606048584, Train Acc: 0.6944, Val Loss: 0.9195653200149536, Val Acc: 0.7323\n","Epoch [9/500], Loss: 1.0638959407806396, Train Acc: 0.6916, Val Loss: 0.9236361980438232, Val Acc: 0.7262\n","Epoch [10/500], Loss: 1.0417941808700562, Train Acc: 0.6963, Val Loss: 0.9007702469825745, Val Acc: 0.7443\n","Epoch [11/500], Loss: 1.005338191986084, Train Acc: 0.7093, Val Loss: 0.8745148777961731, Val Acc: 0.7530\n","Epoch [12/500], Loss: 0.9784943461418152, Train Acc: 0.7161, Val Loss: 0.8557226657867432, Val Acc: 0.7537\n","Epoch [13/500], Loss: 0.960240364074707, Train Acc: 0.7183, Val Loss: 0.8419392704963684, Val Acc: 0.7584\n","Epoch [14/500], Loss: 0.945663571357727, Train Acc: 0.7200, Val Loss: 0.8303576707839966, Val Acc: 0.7684\n","Epoch [15/500], Loss: 0.9258722066879272, Train Acc: 0.7312, Val Loss: 0.8201169967651367, Val Acc: 0.7758\n","Epoch [16/500], Loss: 0.9067318439483643, Train Acc: 0.7342, Val Loss: 0.8092918395996094, Val Acc: 0.7718\n","Epoch [17/500], Loss: 0.8880440592765808, Train Acc: 0.7356, Val Loss: 0.7915449142456055, Val Acc: 0.7764\n","Epoch [18/500], Loss: 0.8689342141151428, Train Acc: 0.7405, Val Loss: 0.7741277813911438, Val Acc: 0.7791\n","Epoch [19/500], Loss: 0.8566749095916748, Train Acc: 0.7431, Val Loss: 0.7678447961807251, Val Acc: 0.7791\n","Epoch [20/500], Loss: 0.8444646596908569, Train Acc: 0.7509, Val Loss: 0.7662540674209595, Val Acc: 0.7811\n","Epoch [21/500], Loss: 0.8326880931854248, Train Acc: 0.7571, Val Loss: 0.7577376365661621, Val Acc: 0.7825\n","Epoch [22/500], Loss: 0.8206843137741089, Train Acc: 0.7594, Val Loss: 0.7378932237625122, Val Acc: 0.7791\n","Epoch [23/500], Loss: 0.8069810271263123, Train Acc: 0.7583, Val Loss: 0.7211028933525085, Val Acc: 0.7798\n","Epoch [24/500], Loss: 0.7943606376647949, Train Acc: 0.7569, Val Loss: 0.71110600233078, Val Acc: 0.7858\n","Epoch [25/500], Loss: 0.7834480404853821, Train Acc: 0.7604, Val Loss: 0.7056601047515869, Val Acc: 0.7925\n","Epoch [26/500], Loss: 0.7755755186080933, Train Acc: 0.7652, Val Loss: 0.6969460844993591, Val Acc: 0.7905\n","Epoch [27/500], Loss: 0.7641404271125793, Train Acc: 0.7651, Val Loss: 0.6924850344657898, Val Acc: 0.7885\n","Epoch [28/500], Loss: 0.7562195062637329, Train Acc: 0.7675, Val Loss: 0.6829814314842224, Val Acc: 0.7892\n","Epoch [29/500], Loss: 0.7465928792953491, Train Acc: 0.7696, Val Loss: 0.6743735074996948, Val Acc: 0.7918\n","Epoch [30/500], Loss: 0.738481342792511, Train Acc: 0.7737, Val Loss: 0.6666362881660461, Val Acc: 0.7918\n","Epoch [31/500], Loss: 0.7305493950843811, Train Acc: 0.7773, Val Loss: 0.6646442413330078, Val Acc: 0.7972\n","Epoch [32/500], Loss: 0.7213907837867737, Train Acc: 0.7811, Val Loss: 0.6616737246513367, Val Acc: 0.7918\n","Epoch [33/500], Loss: 0.7132846713066101, Train Acc: 0.7827, Val Loss: 0.6568459272384644, Val Acc: 0.7972\n","Epoch [34/500], Loss: 0.7064258456230164, Train Acc: 0.7833, Val Loss: 0.6553176045417786, Val Acc: 0.7965\n","Epoch [35/500], Loss: 0.7007451057434082, Train Acc: 0.7839, Val Loss: 0.6472655534744263, Val Acc: 0.7965\n","Epoch [36/500], Loss: 0.6927974224090576, Train Acc: 0.7866, Val Loss: 0.6420981884002686, Val Acc: 0.8012\n","Epoch [37/500], Loss: 0.6851633191108704, Train Acc: 0.7885, Val Loss: 0.6478709578514099, Val Acc: 0.8019\n","Epoch [38/500], Loss: 0.6794468760490417, Train Acc: 0.7892, Val Loss: 0.639638364315033, Val Acc: 0.8032\n","Epoch [39/500], Loss: 0.6725362539291382, Train Acc: 0.7899, Val Loss: 0.6329903602600098, Val Acc: 0.8019\n","Epoch [40/500], Loss: 0.6668973565101624, Train Acc: 0.7903, Val Loss: 0.6270673871040344, Val Acc: 0.8025\n","Epoch [41/500], Loss: 0.6604182124137878, Train Acc: 0.7927, Val Loss: 0.6192037463188171, Val Acc: 0.8019\n","Epoch [42/500], Loss: 0.6542184352874756, Train Acc: 0.7948, Val Loss: 0.6207753419876099, Val Acc: 0.8052\n","Epoch [43/500], Loss: 0.6494466662406921, Train Acc: 0.7962, Val Loss: 0.613084077835083, Val Acc: 0.8079\n","Epoch [44/500], Loss: 0.6435050964355469, Train Acc: 0.7983, Val Loss: 0.6100746393203735, Val Acc: 0.8059\n","Epoch [45/500], Loss: 0.6385920643806458, Train Acc: 0.7991, Val Loss: 0.6087048649787903, Val Acc: 0.8052\n","Epoch [46/500], Loss: 0.6334680318832397, Train Acc: 0.8006, Val Loss: 0.6072719693183899, Val Acc: 0.8086\n","Epoch [47/500], Loss: 0.6288196444511414, Train Acc: 0.8018, Val Loss: 0.6069843173027039, Val Acc: 0.8126\n","Epoch [48/500], Loss: 0.6239572763442993, Train Acc: 0.8027, Val Loss: 0.5962707996368408, Val Acc: 0.8106\n","Epoch [49/500], Loss: 0.6186231374740601, Train Acc: 0.8040, Val Loss: 0.5928727388381958, Val Acc: 0.8126\n","Epoch [50/500], Loss: 0.6140711307525635, Train Acc: 0.8046, Val Loss: 0.5913438200950623, Val Acc: 0.8126\n","Epoch [51/500], Loss: 0.6101096868515015, Train Acc: 0.8040, Val Loss: 0.5881582498550415, Val Acc: 0.8139\n","Epoch [52/500], Loss: 0.6065743565559387, Train Acc: 0.8062, Val Loss: 0.5896504521369934, Val Acc: 0.8173\n","Epoch [53/500], Loss: 0.6022374629974365, Train Acc: 0.8084, Val Loss: 0.5838232636451721, Val Acc: 0.8199\n","Epoch [54/500], Loss: 0.5975515246391296, Train Acc: 0.8091, Val Loss: 0.5757877826690674, Val Acc: 0.8186\n","Epoch [55/500], Loss: 0.591966450214386, Train Acc: 0.8095, Val Loss: 0.5734131336212158, Val Acc: 0.8193\n","Epoch [56/500], Loss: 0.5889520645141602, Train Acc: 0.8110, Val Loss: 0.5760891437530518, Val Acc: 0.8220\n","Epoch [57/500], Loss: 0.5849464535713196, Train Acc: 0.8123, Val Loss: 0.5719500780105591, Val Acc: 0.8213\n","Epoch [58/500], Loss: 0.5816998481750488, Train Acc: 0.8136, Val Loss: 0.5693039894104004, Val Acc: 0.8206\n","Epoch [59/500], Loss: 0.5769356489181519, Train Acc: 0.8149, Val Loss: 0.5695845484733582, Val Acc: 0.8253\n","Epoch [60/500], Loss: 0.5715224146842957, Train Acc: 0.8173, Val Loss: 0.5586427450180054, Val Acc: 0.8240\n","Epoch [61/500], Loss: 0.5689461827278137, Train Acc: 0.8182, Val Loss: 0.5559209585189819, Val Acc: 0.8260\n","Epoch [62/500], Loss: 0.5634923577308655, Train Acc: 0.8190, Val Loss: 0.557316243648529, Val Acc: 0.8293\n","Epoch [63/500], Loss: 0.561082661151886, Train Acc: 0.8207, Val Loss: 0.5580372214317322, Val Acc: 0.8266\n","Epoch [64/500], Loss: 0.5575289726257324, Train Acc: 0.8195, Val Loss: 0.558503270149231, Val Acc: 0.8293\n","Epoch [65/500], Loss: 0.5552092790603638, Train Acc: 0.8217, Val Loss: 0.5534520149230957, Val Acc: 0.8293\n","Epoch [66/500], Loss: 0.5489998459815979, Train Acc: 0.8236, Val Loss: 0.5527079105377197, Val Acc: 0.8280\n","Epoch [67/500], Loss: 0.5465803742408752, Train Acc: 0.8227, Val Loss: 0.5476171374320984, Val Acc: 0.8307\n","Epoch [68/500], Loss: 0.5444743037223816, Train Acc: 0.8256, Val Loss: 0.5467694997787476, Val Acc: 0.8313\n","Epoch [69/500], Loss: 0.5400888919830322, Train Acc: 0.8256, Val Loss: 0.5441805124282837, Val Acc: 0.8320\n","Epoch [70/500], Loss: 0.5361309051513672, Train Acc: 0.8262, Val Loss: 0.5476503372192383, Val Acc: 0.8320\n","Epoch [71/500], Loss: 0.5348227620124817, Train Acc: 0.8280, Val Loss: 0.5373910069465637, Val Acc: 0.8373\n","Epoch [72/500], Loss: 0.5292547345161438, Train Acc: 0.8270, Val Loss: 0.5421207547187805, Val Acc: 0.8333\n","Epoch [73/500], Loss: 0.528009295463562, Train Acc: 0.8295, Val Loss: 0.5428887605667114, Val Acc: 0.8340\n","Epoch [74/500], Loss: 0.5232011675834656, Train Acc: 0.8324, Val Loss: 0.5431225895881653, Val Acc: 0.8353\n","Epoch [75/500], Loss: 0.520071268081665, Train Acc: 0.8332, Val Loss: 0.5296519994735718, Val Acc: 0.8373\n","Epoch [76/500], Loss: 0.515357494354248, Train Acc: 0.8341, Val Loss: 0.5333317518234253, Val Acc: 0.8340\n","Epoch [77/500], Loss: 0.5148202180862427, Train Acc: 0.8341, Val Loss: 0.5332117676734924, Val Acc: 0.8373\n","Epoch [78/500], Loss: 0.512371301651001, Train Acc: 0.8355, Val Loss: 0.5360614061355591, Val Acc: 0.8380\n","Epoch [79/500], Loss: 0.5095518827438354, Train Acc: 0.8369, Val Loss: 0.5377629995346069, Val Acc: 0.8373\n","Epoch [80/500], Loss: 0.5039343237876892, Train Acc: 0.8366, Val Loss: 0.5261819958686829, Val Acc: 0.8387\n","Epoch [81/500], Loss: 0.5032100677490234, Train Acc: 0.8379, Val Loss: 0.52142333984375, Val Acc: 0.8387\n","Epoch [82/500], Loss: 0.49833518266677856, Train Acc: 0.8391, Val Loss: 0.5274884700775146, Val Acc: 0.8380\n","Epoch [83/500], Loss: 0.4957178831100464, Train Acc: 0.8402, Val Loss: 0.5298101305961609, Val Acc: 0.8394\n","Epoch [84/500], Loss: 0.4932851195335388, Train Acc: 0.8408, Val Loss: 0.5350066423416138, Val Acc: 0.8380\n","Epoch [85/500], Loss: 0.49188753962516785, Train Acc: 0.8407, Val Loss: 0.5295934081077576, Val Acc: 0.8387\n","Epoch [86/500], Loss: 0.4887157082557678, Train Acc: 0.8421, Val Loss: 0.5214301347732544, Val Acc: 0.8394\n","Epoch [87/500], Loss: 0.4843170940876007, Train Acc: 0.8437, Val Loss: 0.5175824165344238, Val Acc: 0.8347\n","Epoch [88/500], Loss: 0.4807910621166229, Train Acc: 0.8428, Val Loss: 0.5159049034118652, Val Acc: 0.8367\n","Epoch [89/500], Loss: 0.47676318883895874, Train Acc: 0.8458, Val Loss: 0.5198893547058105, Val Acc: 0.8414\n","Epoch [90/500], Loss: 0.47493046522140503, Train Acc: 0.8469, Val Loss: 0.5192680954933167, Val Acc: 0.8407\n","Epoch [91/500], Loss: 0.4716770350933075, Train Acc: 0.8465, Val Loss: 0.5208558440208435, Val Acc: 0.8414\n","Epoch [92/500], Loss: 0.46972984075546265, Train Acc: 0.8468, Val Loss: 0.5151314735412598, Val Acc: 0.8387\n","Epoch [93/500], Loss: 0.4648650884628296, Train Acc: 0.8477, Val Loss: 0.515375554561615, Val Acc: 0.8407\n","Epoch [94/500], Loss: 0.4619264602661133, Train Acc: 0.8480, Val Loss: 0.5055209994316101, Val Acc: 0.8407\n","Epoch [95/500], Loss: 0.46099773049354553, Train Acc: 0.8495, Val Loss: 0.5112202167510986, Val Acc: 0.8427\n","Epoch [96/500], Loss: 0.4583270251750946, Train Acc: 0.8502, Val Loss: 0.5111061334609985, Val Acc: 0.8407\n","Epoch [97/500], Loss: 0.45578259229660034, Train Acc: 0.8520, Val Loss: 0.5190280675888062, Val Acc: 0.8481\n","Epoch [98/500], Loss: 0.4548468291759491, Train Acc: 0.8499, Val Loss: 0.5039825439453125, Val Acc: 0.8461\n","Epoch [99/500], Loss: 0.44976869225502014, Train Acc: 0.8533, Val Loss: 0.502545952796936, Val Acc: 0.8454\n","Epoch [100/500], Loss: 0.44908803701400757, Train Acc: 0.8503, Val Loss: 0.4964004456996918, Val Acc: 0.8467\n","Epoch [101/500], Loss: 0.44625574350357056, Train Acc: 0.8525, Val Loss: 0.5056732296943665, Val Acc: 0.8474\n","Epoch [102/500], Loss: 0.4453811049461365, Train Acc: 0.8527, Val Loss: 0.49403780698776245, Val Acc: 0.8501\n","Epoch [103/500], Loss: 0.44082093238830566, Train Acc: 0.8553, Val Loss: 0.499072790145874, Val Acc: 0.8481\n","Epoch [104/500], Loss: 0.43779656291007996, Train Acc: 0.8563, Val Loss: 0.49761518836021423, Val Acc: 0.8514\n","Epoch [105/500], Loss: 0.4348292648792267, Train Acc: 0.8573, Val Loss: 0.5003513097763062, Val Acc: 0.8494\n","Epoch [106/500], Loss: 0.43375083804130554, Train Acc: 0.8566, Val Loss: 0.49437806010246277, Val Acc: 0.8494\n","Epoch [107/500], Loss: 0.4308684170246124, Train Acc: 0.8597, Val Loss: 0.4910036027431488, Val Acc: 0.8521\n","Epoch [108/500], Loss: 0.4281121790409088, Train Acc: 0.8576, Val Loss: 0.4902268052101135, Val Acc: 0.8507\n","Epoch [109/500], Loss: 0.4247291684150696, Train Acc: 0.8600, Val Loss: 0.4974464476108551, Val Acc: 0.8541\n","Epoch [110/500], Loss: 0.42347848415374756, Train Acc: 0.8607, Val Loss: 0.48978665471076965, Val Acc: 0.8554\n","Epoch [111/500], Loss: 0.419261634349823, Train Acc: 0.8614, Val Loss: 0.4900033175945282, Val Acc: 0.8561\n","Epoch [112/500], Loss: 0.41783764958381653, Train Acc: 0.8636, Val Loss: 0.48110899329185486, Val Acc: 0.8561\n","Epoch [113/500], Loss: 0.41454219818115234, Train Acc: 0.8648, Val Loss: 0.48384758830070496, Val Acc: 0.8601\n","Epoch [114/500], Loss: 0.4119097888469696, Train Acc: 0.8653, Val Loss: 0.4891349673271179, Val Acc: 0.8568\n","Epoch [115/500], Loss: 0.40823227167129517, Train Acc: 0.8665, Val Loss: 0.4907276928424835, Val Acc: 0.8548\n","Epoch [116/500], Loss: 0.4064919352531433, Train Acc: 0.8638, Val Loss: 0.4842721223831177, Val Acc: 0.8541\n","Epoch [117/500], Loss: 0.40426304936408997, Train Acc: 0.8663, Val Loss: 0.47905096411705017, Val Acc: 0.8588\n","Epoch [118/500], Loss: 0.40189799666404724, Train Acc: 0.8676, Val Loss: 0.4997842609882355, Val Acc: 0.8574\n","Epoch [119/500], Loss: 0.40364572405815125, Train Acc: 0.8685, Val Loss: 0.5055067539215088, Val Acc: 0.8568\n","Epoch [120/500], Loss: 0.4002918004989624, Train Acc: 0.8678, Val Loss: 0.49680036306381226, Val Acc: 0.8527\n","Epoch [121/500], Loss: 0.4034790098667145, Train Acc: 0.8653, Val Loss: 0.4923921227455139, Val Acc: 0.8507\n","Epoch [122/500], Loss: 0.42572858929634094, Train Acc: 0.8614, Val Loss: 0.5573830604553223, Val Acc: 0.8347\n","Epoch [123/500], Loss: 0.4800289571285248, Train Acc: 0.8388, Val Loss: 0.5180328488349915, Val Acc: 0.8501\n","Epoch [124/500], Loss: 0.40746673941612244, Train Acc: 0.8647, Val Loss: 0.5268718600273132, Val Acc: 0.8501\n","Epoch [125/500], Loss: 0.43702512979507446, Train Acc: 0.8537, Val Loss: 0.5159892439842224, Val Acc: 0.8440\n","Epoch [126/500], Loss: 0.4376318156719208, Train Acc: 0.8525, Val Loss: 0.4799712300300598, Val Acc: 0.8534\n","Early stopping at epoch 126\n","Test Loss: 0.5514504313468933, Test Accuracy: 0.8518766756032171\n","Precision: 0.8480, Recall: 0.8519, F1-score: 0.8451\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GINConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GINNet(torch.nn.Module):\n","    def __init__(self, num_features, hidden_dim, num_layers, output_dim):\n","        super(GINNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(num_features, hidden_dim), torch.nn.ReLU())))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU())))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","        x = F.relu(x)\n","        return self.fc(x)\n","\n","\n","# Define model\n","model = GINNet(num_features=X_train.shape[1], hidden_dim=128, num_layers=3, output_dim=13).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model3.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer3/train_f1_scores_GIN.txt',train_f1_scores)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer3/train_loss_GIN.txt', train_losses)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer3/epochs_GIN.txt', epochss)\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model3.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/layer3/f1_score_GIN.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaHe8ysHV4EU","executionInfo":{"status":"ok","timestamp":1714301781213,"user_tz":-330,"elapsed":32841,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"599f8539-fa13-4897-9ee0-359cc1fa940e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.5974857807159424, Train Acc: 0.0713, Val Loss: 2.2117459774017334, Val Acc: 0.6787\n","Epoch [2/500], Loss: 4.825202465057373, Train Acc: 0.6441, Val Loss: 2.4012932777404785, Val Acc: 0.1332\n","Epoch [3/500], Loss: 5.4709367752075195, Train Acc: 0.1383, Val Loss: 2.3385980129241943, Val Acc: 0.6412\n","Epoch [4/500], Loss: 2.792297124862671, Train Acc: 0.5482, Val Loss: 2.2713801860809326, Val Acc: 0.6787\n","Epoch [5/500], Loss: 2.5263006687164307, Train Acc: 0.6441, Val Loss: 2.219942808151245, Val Acc: 0.6787\n","Epoch [6/500], Loss: 2.477268934249878, Train Acc: 0.6441, Val Loss: 2.1745407581329346, Val Acc: 0.6787\n","Epoch [7/500], Loss: 2.3570504188537598, Train Acc: 0.6441, Val Loss: 2.120152235031128, Val Acc: 0.6787\n","Epoch [8/500], Loss: 2.238430976867676, Train Acc: 0.6441, Val Loss: 2.0777242183685303, Val Acc: 0.6760\n","Epoch [9/500], Loss: 2.1314637660980225, Train Acc: 0.6271, Val Loss: 1.9868059158325195, Val Acc: 0.6767\n","Epoch [10/500], Loss: 2.0329854488372803, Train Acc: 0.6310, Val Loss: 1.861847162246704, Val Acc: 0.6787\n","Epoch [11/500], Loss: 2.0340805053710938, Train Acc: 0.6441, Val Loss: 1.7702710628509521, Val Acc: 0.6787\n","Epoch [12/500], Loss: 1.9298896789550781, Train Acc: 0.6441, Val Loss: 1.7077518701553345, Val Acc: 0.6774\n","Epoch [13/500], Loss: 1.7559155225753784, Train Acc: 0.6434, Val Loss: 1.6344166994094849, Val Acc: 0.6774\n","Epoch [14/500], Loss: 1.6688311100006104, Train Acc: 0.6413, Val Loss: 1.5415313243865967, Val Acc: 0.6760\n","Epoch [15/500], Loss: 1.599021553993225, Train Acc: 0.6419, Val Loss: 1.450005054473877, Val Acc: 0.6774\n","Epoch [16/500], Loss: 1.5435861349105835, Train Acc: 0.6437, Val Loss: 1.386345624923706, Val Acc: 0.6787\n","Epoch [17/500], Loss: 1.4773943424224854, Train Acc: 0.6440, Val Loss: 1.340133786201477, Val Acc: 0.6787\n","Epoch [18/500], Loss: 1.4124629497528076, Train Acc: 0.6417, Val Loss: 1.2997349500656128, Val Acc: 0.6787\n","Epoch [19/500], Loss: 1.362310528755188, Train Acc: 0.6407, Val Loss: 1.2632017135620117, Val Acc: 0.6760\n","Epoch [20/500], Loss: 1.3379584550857544, Train Acc: 0.6348, Val Loss: 1.2257698774337769, Val Acc: 0.6780\n","Epoch [21/500], Loss: 1.2967336177825928, Train Acc: 0.6434, Val Loss: 1.1920931339263916, Val Acc: 0.6787\n","Epoch [22/500], Loss: 1.2676184177398682, Train Acc: 0.6411, Val Loss: 1.1576224565505981, Val Acc: 0.6787\n","Epoch [23/500], Loss: 1.2366390228271484, Train Acc: 0.6436, Val Loss: 1.1309678554534912, Val Acc: 0.6787\n","Epoch [24/500], Loss: 1.2141306400299072, Train Acc: 0.6435, Val Loss: 1.1118500232696533, Val Acc: 0.6774\n","Epoch [25/500], Loss: 1.2185051441192627, Train Acc: 0.6358, Val Loss: 1.0882790088653564, Val Acc: 0.6787\n","Epoch [26/500], Loss: 1.176735520362854, Train Acc: 0.6439, Val Loss: 1.0713759660720825, Val Acc: 0.6787\n","Epoch [27/500], Loss: 1.1627063751220703, Train Acc: 0.6437, Val Loss: 1.0576939582824707, Val Acc: 0.6787\n","Epoch [28/500], Loss: 1.1506915092468262, Train Acc: 0.6429, Val Loss: 1.0473285913467407, Val Acc: 0.6787\n","Epoch [29/500], Loss: 1.1378111839294434, Train Acc: 0.6434, Val Loss: 1.0389872789382935, Val Acc: 0.6774\n","Epoch [30/500], Loss: 1.1364936828613281, Train Acc: 0.6414, Val Loss: 1.0293831825256348, Val Acc: 0.6774\n","Epoch [31/500], Loss: 1.1234179735183716, Train Acc: 0.6425, Val Loss: 1.0176278352737427, Val Acc: 0.6754\n","Epoch [32/500], Loss: 1.115988850593567, Train Acc: 0.6416, Val Loss: 1.0057400465011597, Val Acc: 0.6754\n","Epoch [33/500], Loss: 1.1078029870986938, Train Acc: 0.6417, Val Loss: 0.9962391257286072, Val Acc: 0.6760\n","Epoch [34/500], Loss: 1.1027131080627441, Train Acc: 0.6436, Val Loss: 0.9889098405838013, Val Acc: 0.6774\n","Epoch [35/500], Loss: 1.097450852394104, Train Acc: 0.6438, Val Loss: 0.9839622378349304, Val Acc: 0.6767\n","Epoch [36/500], Loss: 1.0852347612380981, Train Acc: 0.6423, Val Loss: 0.9814380407333374, Val Acc: 0.6760\n","Epoch [37/500], Loss: 1.0803204774856567, Train Acc: 0.6383, Val Loss: 0.9766924977302551, Val Acc: 0.6767\n","Epoch [38/500], Loss: 1.077672004699707, Train Acc: 0.6373, Val Loss: 0.9686294198036194, Val Acc: 0.6767\n","Epoch [39/500], Loss: 1.0650012493133545, Train Acc: 0.6402, Val Loss: 0.962312638759613, Val Acc: 0.6774\n","Epoch [40/500], Loss: 1.0622551441192627, Train Acc: 0.6421, Val Loss: 0.9559658169746399, Val Acc: 0.6774\n","Epoch [41/500], Loss: 1.0644646883010864, Train Acc: 0.6441, Val Loss: 0.9490084648132324, Val Acc: 0.6767\n","Epoch [42/500], Loss: 1.0502318143844604, Train Acc: 0.6436, Val Loss: 0.9429709315299988, Val Acc: 0.6760\n","Epoch [43/500], Loss: 1.0386606454849243, Train Acc: 0.6407, Val Loss: 0.9390296936035156, Val Acc: 0.6727\n","Epoch [44/500], Loss: 1.0298658609390259, Train Acc: 0.6414, Val Loss: 0.9334741830825806, Val Acc: 0.6720\n","Epoch [45/500], Loss: 1.026326060295105, Train Acc: 0.6419, Val Loss: 0.9251875877380371, Val Acc: 0.6740\n","Epoch [46/500], Loss: 1.0161446332931519, Train Acc: 0.6408, Val Loss: 0.9163129329681396, Val Acc: 0.6754\n","Epoch [47/500], Loss: 1.0078868865966797, Train Acc: 0.6420, Val Loss: 0.9093117117881775, Val Acc: 0.6767\n","Epoch [48/500], Loss: 1.0016660690307617, Train Acc: 0.6427, Val Loss: 0.9028831720352173, Val Acc: 0.6767\n","Epoch [49/500], Loss: 0.9918127059936523, Train Acc: 0.6414, Val Loss: 0.8942617177963257, Val Acc: 0.6747\n","Epoch [50/500], Loss: 0.9834005832672119, Train Acc: 0.6406, Val Loss: 0.884250283241272, Val Acc: 0.6774\n","Epoch [51/500], Loss: 0.9742982983589172, Train Acc: 0.6430, Val Loss: 0.8775920867919922, Val Acc: 0.6801\n","Epoch [52/500], Loss: 0.963021457195282, Train Acc: 0.6457, Val Loss: 0.8711130619049072, Val Acc: 0.6814\n","Epoch [53/500], Loss: 0.9604819416999817, Train Acc: 0.6452, Val Loss: 0.8605474233627319, Val Acc: 0.6841\n","Epoch [54/500], Loss: 0.9483919143676758, Train Acc: 0.6487, Val Loss: 0.8541073203086853, Val Acc: 0.6854\n","Epoch [55/500], Loss: 0.941571056842804, Train Acc: 0.6501, Val Loss: 0.8482593894004822, Val Acc: 0.6874\n","Epoch [56/500], Loss: 0.9275899529457092, Train Acc: 0.6510, Val Loss: 0.8454307317733765, Val Acc: 0.6861\n","Epoch [57/500], Loss: 0.9217641353607178, Train Acc: 0.6507, Val Loss: 0.8279440999031067, Val Acc: 0.6961\n","Epoch [58/500], Loss: 0.9092355370521545, Train Acc: 0.6578, Val Loss: 0.8166015148162842, Val Acc: 0.6995\n","Epoch [59/500], Loss: 0.9020310640335083, Train Acc: 0.6635, Val Loss: 0.805808424949646, Val Acc: 0.7001\n","Epoch [60/500], Loss: 0.8914780020713806, Train Acc: 0.6675, Val Loss: 0.7963781952857971, Val Acc: 0.7055\n","Epoch [61/500], Loss: 0.8782397508621216, Train Acc: 0.6698, Val Loss: 0.7865708470344543, Val Acc: 0.7122\n","Epoch [62/500], Loss: 0.8651642799377441, Train Acc: 0.6753, Val Loss: 0.7790655493736267, Val Acc: 0.7296\n","Epoch [63/500], Loss: 0.8562731742858887, Train Acc: 0.6887, Val Loss: 0.7704665660858154, Val Acc: 0.7343\n","Epoch [64/500], Loss: 0.8510293364524841, Train Acc: 0.6945, Val Loss: 0.7592495083808899, Val Acc: 0.7329\n","Epoch [65/500], Loss: 0.8473114967346191, Train Acc: 0.6918, Val Loss: 0.7551219463348389, Val Acc: 0.7349\n","Epoch [66/500], Loss: 0.837062656879425, Train Acc: 0.6951, Val Loss: 0.7508751749992371, Val Acc: 0.7356\n","Epoch [67/500], Loss: 0.8256354331970215, Train Acc: 0.6949, Val Loss: 0.7438823580741882, Val Acc: 0.7436\n","Epoch [68/500], Loss: 0.8162941336631775, Train Acc: 0.7055, Val Loss: 0.7354611158370972, Val Acc: 0.7550\n","Epoch [69/500], Loss: 0.809206485748291, Train Acc: 0.7129, Val Loss: 0.733008861541748, Val Acc: 0.7624\n","Epoch [70/500], Loss: 0.8017541170120239, Train Acc: 0.7150, Val Loss: 0.7438570261001587, Val Acc: 0.7671\n","Epoch [71/500], Loss: 0.8023808598518372, Train Acc: 0.7206, Val Loss: 0.7182229161262512, Val Acc: 0.7597\n","Epoch [72/500], Loss: 0.7834017872810364, Train Acc: 0.7192, Val Loss: 0.7021482586860657, Val Acc: 0.7677\n","Epoch [73/500], Loss: 0.774935781955719, Train Acc: 0.7292, Val Loss: 0.6953396797180176, Val Acc: 0.7744\n","Epoch [74/500], Loss: 0.7673422694206238, Train Acc: 0.7346, Val Loss: 0.6859997510910034, Val Acc: 0.7778\n","Epoch [75/500], Loss: 0.7568267583847046, Train Acc: 0.7382, Val Loss: 0.6797733902931213, Val Acc: 0.7784\n","Epoch [76/500], Loss: 0.749188244342804, Train Acc: 0.7407, Val Loss: 0.6754543781280518, Val Acc: 0.7831\n","Epoch [77/500], Loss: 0.7453482151031494, Train Acc: 0.7522, Val Loss: 0.6661511659622192, Val Acc: 0.7932\n","Epoch [78/500], Loss: 0.7349433302879333, Train Acc: 0.7567, Val Loss: 0.6740871667861938, Val Acc: 0.7992\n","Epoch [79/500], Loss: 0.7438918352127075, Train Acc: 0.7597, Val Loss: 0.6543713212013245, Val Acc: 0.7985\n","Epoch [80/500], Loss: 0.7236970663070679, Train Acc: 0.7653, Val Loss: 0.6538283824920654, Val Acc: 0.8019\n","Epoch [81/500], Loss: 0.7191551923751831, Train Acc: 0.7729, Val Loss: 0.6465795636177063, Val Acc: 0.7992\n","Epoch [82/500], Loss: 0.707305371761322, Train Acc: 0.7737, Val Loss: 0.643781840801239, Val Acc: 0.8046\n","Epoch [83/500], Loss: 0.6998736262321472, Train Acc: 0.7763, Val Loss: 0.643654465675354, Val Acc: 0.8032\n","Epoch [84/500], Loss: 0.6905114650726318, Train Acc: 0.7785, Val Loss: 0.6426156163215637, Val Acc: 0.8052\n","Epoch [85/500], Loss: 0.6818589568138123, Train Acc: 0.7802, Val Loss: 0.6434799432754517, Val Acc: 0.8019\n","Epoch [86/500], Loss: 0.6722326278686523, Train Acc: 0.7834, Val Loss: 0.6439992785453796, Val Acc: 0.8039\n","Epoch [87/500], Loss: 0.6651959419250488, Train Acc: 0.7876, Val Loss: 0.6348604559898376, Val Acc: 0.8086\n","Epoch [88/500], Loss: 0.6554776430130005, Train Acc: 0.7915, Val Loss: 0.627999484539032, Val Acc: 0.8092\n","Epoch [89/500], Loss: 0.6492065787315369, Train Acc: 0.7917, Val Loss: 0.6275794506072998, Val Acc: 0.8059\n","Epoch [90/500], Loss: 0.641986608505249, Train Acc: 0.7919, Val Loss: 0.6242777705192566, Val Acc: 0.8112\n","Epoch [91/500], Loss: 0.6363984942436218, Train Acc: 0.7940, Val Loss: 0.609961986541748, Val Acc: 0.8106\n","Epoch [92/500], Loss: 0.6270223259925842, Train Acc: 0.7981, Val Loss: 0.6038578152656555, Val Acc: 0.8153\n","Epoch [93/500], Loss: 0.6190975904464722, Train Acc: 0.7999, Val Loss: 0.6068741083145142, Val Acc: 0.8153\n","Epoch [94/500], Loss: 0.6137700080871582, Train Acc: 0.7995, Val Loss: 0.5993425846099854, Val Acc: 0.8153\n","Epoch [95/500], Loss: 0.6053553819656372, Train Acc: 0.8045, Val Loss: 0.5966190695762634, Val Acc: 0.8159\n","Epoch [96/500], Loss: 0.5972065329551697, Train Acc: 0.8053, Val Loss: 0.5991938710212708, Val Acc: 0.8179\n","Epoch [97/500], Loss: 0.5971119999885559, Train Acc: 0.8068, Val Loss: 0.6001327633857727, Val Acc: 0.8153\n","Epoch [98/500], Loss: 0.598888635635376, Train Acc: 0.8062, Val Loss: 0.598368763923645, Val Acc: 0.8193\n","Epoch [99/500], Loss: 0.5958028435707092, Train Acc: 0.8082, Val Loss: 0.598000705242157, Val Acc: 0.8186\n","Epoch [100/500], Loss: 0.5877560377120972, Train Acc: 0.8087, Val Loss: 0.5891396403312683, Val Acc: 0.8186\n","Epoch [101/500], Loss: 0.5815110206604004, Train Acc: 0.8109, Val Loss: 0.5844018459320068, Val Acc: 0.8139\n","Epoch [102/500], Loss: 0.5797935128211975, Train Acc: 0.8091, Val Loss: 0.5898202061653137, Val Acc: 0.8146\n","Epoch [103/500], Loss: 0.5865945219993591, Train Acc: 0.8081, Val Loss: 0.5909639000892639, Val Acc: 0.8119\n","Epoch [104/500], Loss: 0.5898714065551758, Train Acc: 0.8072, Val Loss: 0.5687633752822876, Val Acc: 0.8193\n","Epoch [105/500], Loss: 0.5634481310844421, Train Acc: 0.8138, Val Loss: 0.5826037526130676, Val Acc: 0.8179\n","Epoch [106/500], Loss: 0.5628193616867065, Train Acc: 0.8143, Val Loss: 0.6035161018371582, Val Acc: 0.8106\n","Epoch [107/500], Loss: 0.5919509530067444, Train Acc: 0.8113, Val Loss: 0.5678609609603882, Val Acc: 0.8280\n","Epoch [108/500], Loss: 0.5529868006706238, Train Acc: 0.8204, Val Loss: 0.5833021998405457, Val Acc: 0.8246\n","Epoch [109/500], Loss: 0.5679664611816406, Train Acc: 0.8182, Val Loss: 0.6055343747138977, Val Acc: 0.8126\n","Epoch [110/500], Loss: 0.5851151943206787, Train Acc: 0.8089, Val Loss: 0.5734896659851074, Val Acc: 0.8159\n","Epoch [111/500], Loss: 0.5449591875076294, Train Acc: 0.8189, Val Loss: 0.6148160099983215, Val Acc: 0.8233\n","Epoch [112/500], Loss: 0.5844972729682922, Train Acc: 0.8143, Val Loss: 0.5871612429618835, Val Acc: 0.8173\n","Epoch [113/500], Loss: 0.5704452395439148, Train Acc: 0.8202, Val Loss: 0.6066334843635559, Val Acc: 0.8193\n","Epoch [114/500], Loss: 0.5799803733825684, Train Acc: 0.8180, Val Loss: 0.5596759915351868, Val Acc: 0.8273\n","Epoch [115/500], Loss: 0.5478151440620422, Train Acc: 0.8211, Val Loss: 0.5928511619567871, Val Acc: 0.8260\n","Epoch [116/500], Loss: 0.5662403106689453, Train Acc: 0.8215, Val Loss: 0.5675333738327026, Val Acc: 0.8246\n","Epoch [117/500], Loss: 0.537537693977356, Train Acc: 0.8247, Val Loss: 0.594615638256073, Val Acc: 0.8086\n","Epoch [118/500], Loss: 0.549354612827301, Train Acc: 0.8164, Val Loss: 0.5661884546279907, Val Acc: 0.8193\n","Epoch [119/500], Loss: 0.5300155282020569, Train Acc: 0.8257, Val Loss: 0.5804972648620605, Val Acc: 0.8233\n","Epoch [120/500], Loss: 0.533409059047699, Train Acc: 0.8261, Val Loss: 0.5867507457733154, Val Acc: 0.8220\n","Epoch [121/500], Loss: 0.5317001342773438, Train Acc: 0.8257, Val Loss: 0.5831692814826965, Val Acc: 0.8266\n","Epoch [122/500], Loss: 0.536588728427887, Train Acc: 0.8262, Val Loss: 0.5779648423194885, Val Acc: 0.8193\n","Epoch [123/500], Loss: 0.5333647131919861, Train Acc: 0.8252, Val Loss: 0.5540574193000793, Val Acc: 0.8253\n","Epoch [124/500], Loss: 0.5091005563735962, Train Acc: 0.8323, Val Loss: 0.5614972710609436, Val Acc: 0.8327\n","Epoch [125/500], Loss: 0.5070450901985168, Train Acc: 0.8335, Val Loss: 0.5579510927200317, Val Acc: 0.8273\n","Epoch [126/500], Loss: 0.4981154799461365, Train Acc: 0.8350, Val Loss: 0.5582913756370544, Val Acc: 0.8293\n","Epoch [127/500], Loss: 0.493566632270813, Train Acc: 0.8371, Val Loss: 0.5584503412246704, Val Acc: 0.8313\n","Epoch [128/500], Loss: 0.48868492245674133, Train Acc: 0.8360, Val Loss: 0.5626957416534424, Val Acc: 0.8320\n","Epoch [129/500], Loss: 0.48911774158477783, Train Acc: 0.8371, Val Loss: 0.5574747920036316, Val Acc: 0.8367\n","Epoch [130/500], Loss: 0.4843941032886505, Train Acc: 0.8410, Val Loss: 0.5800968408584595, Val Acc: 0.8253\n","Epoch [131/500], Loss: 0.49767303466796875, Train Acc: 0.8316, Val Loss: 0.5741310119628906, Val Acc: 0.8286\n","Epoch [132/500], Loss: 0.4874531030654907, Train Acc: 0.8390, Val Loss: 0.5701690316200256, Val Acc: 0.8260\n","Early stopping at epoch 132\n","Test Loss: 0.5759005546569824, Test Accuracy: 0.8344504021447721\n","Precision: 0.8273, Recall: 0.8345, F1-score: 0.8254\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GINConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GINNet(torch.nn.Module):\n","    def __init__(self, num_features, hidden_dim, num_layers, output_dim):\n","        super(GINNet, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(num_features, hidden_dim), torch.nn.ReLU())))\n","        for _ in range(num_layers - 1):\n","            self.convs.append(GINConv(torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU())))\n","\n","        # Output layer\n","        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","        x = F.relu(x)\n","        return self.fc(x)\n","\n","\n","# Define model\n","model = GINNet(num_features=X_train.shape[1], hidden_dim=128, num_layers=1, output_dim=13).to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model4.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer4/train_f1_scores_GIN.txt',train_f1_scores)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer4/train_loss_GIN.txt', train_losses)\n","        np.savetxt('/content/drive/MyDrive/PROJECT/layer4/epochs_GIN.txt', epochss)\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=10)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model4.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/layer4/f1_score_GIN.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sD9f31lfWDQ7","executionInfo":{"status":"ok","timestamp":1714301798757,"user_tz":-330,"elapsed":17579,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"8ef5a04c-3369-406d-9ca6-d60f0184301a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.6854968070983887, Train Acc: 0.0608, Val Loss: 2.1256093978881836, Val Acc: 0.6292\n","Epoch [2/500], Loss: 2.1139743328094482, Train Acc: 0.5862, Val Loss: 1.7023664712905884, Val Acc: 0.6954\n","Epoch [3/500], Loss: 1.7262715101242065, Train Acc: 0.6594, Val Loss: 1.332602620124817, Val Acc: 0.7209\n","Epoch [4/500], Loss: 1.4312572479248047, Train Acc: 0.6800, Val Loss: 1.0852720737457275, Val Acc: 0.7088\n","Epoch [5/500], Loss: 1.2455816268920898, Train Acc: 0.6679, Val Loss: 0.9688936471939087, Val Acc: 0.7062\n","Epoch [6/500], Loss: 1.1738702058792114, Train Acc: 0.6628, Val Loss: 0.9252703189849854, Val Acc: 0.7162\n","Epoch [7/500], Loss: 1.1190967559814453, Train Acc: 0.6741, Val Loss: 0.9112683534622192, Val Acc: 0.7336\n","Epoch [8/500], Loss: 1.082969307899475, Train Acc: 0.6967, Val Loss: 0.916183352470398, Val Acc: 0.7390\n","Epoch [9/500], Loss: 1.0565087795257568, Train Acc: 0.7035, Val Loss: 0.9237946271896362, Val Acc: 0.7356\n","Epoch [10/500], Loss: 1.0356868505477905, Train Acc: 0.7018, Val Loss: 0.9130383133888245, Val Acc: 0.7403\n","Epoch [11/500], Loss: 1.0057042837142944, Train Acc: 0.7052, Val Loss: 0.8869073987007141, Val Acc: 0.7550\n","Epoch [12/500], Loss: 0.9743910431861877, Train Acc: 0.7078, Val Loss: 0.8512274026870728, Val Acc: 0.7584\n","Epoch [13/500], Loss: 0.9491091966629028, Train Acc: 0.7084, Val Loss: 0.8207400441169739, Val Acc: 0.7677\n","Epoch [14/500], Loss: 0.9287635087966919, Train Acc: 0.7172, Val Loss: 0.8094784021377563, Val Acc: 0.7677\n","Epoch [15/500], Loss: 0.9087382555007935, Train Acc: 0.7266, Val Loss: 0.8019638657569885, Val Acc: 0.7751\n","Epoch [16/500], Loss: 0.8922041654586792, Train Acc: 0.7335, Val Loss: 0.7938111424446106, Val Acc: 0.7811\n","Epoch [17/500], Loss: 0.878218948841095, Train Acc: 0.7371, Val Loss: 0.7840167880058289, Val Acc: 0.7838\n","Epoch [18/500], Loss: 0.8614344596862793, Train Acc: 0.7456, Val Loss: 0.782291054725647, Val Acc: 0.7805\n","Epoch [19/500], Loss: 0.8519138097763062, Train Acc: 0.7464, Val Loss: 0.7832408547401428, Val Acc: 0.7791\n","Epoch [20/500], Loss: 0.8436273336410522, Train Acc: 0.7502, Val Loss: 0.7773347496986389, Val Acc: 0.7851\n","Epoch [21/500], Loss: 0.8275549411773682, Train Acc: 0.7583, Val Loss: 0.7614668607711792, Val Acc: 0.7851\n","Epoch [22/500], Loss: 0.8111275434494019, Train Acc: 0.7612, Val Loss: 0.7388532161712646, Val Acc: 0.7851\n","Epoch [23/500], Loss: 0.7953587174415588, Train Acc: 0.7617, Val Loss: 0.717130184173584, Val Acc: 0.7838\n","Epoch [24/500], Loss: 0.7831407189369202, Train Acc: 0.7641, Val Loss: 0.7004324197769165, Val Acc: 0.7858\n","Epoch [25/500], Loss: 0.7734224796295166, Train Acc: 0.7660, Val Loss: 0.6867567896842957, Val Acc: 0.7905\n","Epoch [26/500], Loss: 0.7633585333824158, Train Acc: 0.7708, Val Loss: 0.6772363781929016, Val Acc: 0.7952\n","Epoch [27/500], Loss: 0.754738450050354, Train Acc: 0.7757, Val Loss: 0.6705131530761719, Val Acc: 0.7959\n","Epoch [28/500], Loss: 0.746561586856842, Train Acc: 0.7782, Val Loss: 0.6616094708442688, Val Acc: 0.7965\n","Epoch [29/500], Loss: 0.7377762198448181, Train Acc: 0.7824, Val Loss: 0.6539973616600037, Val Acc: 0.7992\n","Epoch [30/500], Loss: 0.7282602190971375, Train Acc: 0.7833, Val Loss: 0.6507319211959839, Val Acc: 0.7985\n","Epoch [31/500], Loss: 0.7203924655914307, Train Acc: 0.7839, Val Loss: 0.6483167409896851, Val Acc: 0.8019\n","Epoch [32/500], Loss: 0.7114733457565308, Train Acc: 0.7850, Val Loss: 0.6459033489227295, Val Acc: 0.8025\n","Epoch [33/500], Loss: 0.7047285437583923, Train Acc: 0.7855, Val Loss: 0.6413731575012207, Val Acc: 0.8032\n","Epoch [34/500], Loss: 0.6968134641647339, Train Acc: 0.7881, Val Loss: 0.6340774297714233, Val Acc: 0.8059\n","Epoch [35/500], Loss: 0.6901348829269409, Train Acc: 0.7893, Val Loss: 0.6291329264640808, Val Acc: 0.8066\n","Epoch [36/500], Loss: 0.6839718818664551, Train Acc: 0.7896, Val Loss: 0.6246812343597412, Val Acc: 0.8099\n","Epoch [37/500], Loss: 0.6777968406677246, Train Acc: 0.7907, Val Loss: 0.6209964752197266, Val Acc: 0.8086\n","Epoch [38/500], Loss: 0.6713864207267761, Train Acc: 0.7927, Val Loss: 0.6162531971931458, Val Acc: 0.8099\n","Epoch [39/500], Loss: 0.6643961668014526, Train Acc: 0.7946, Val Loss: 0.6097882986068726, Val Acc: 0.8066\n","Epoch [40/500], Loss: 0.6583621501922607, Train Acc: 0.7963, Val Loss: 0.6040961742401123, Val Acc: 0.8066\n","Epoch [41/500], Loss: 0.6522871255874634, Train Acc: 0.7983, Val Loss: 0.5998560190200806, Val Acc: 0.8099\n","Epoch [42/500], Loss: 0.646159291267395, Train Acc: 0.8003, Val Loss: 0.5927258133888245, Val Acc: 0.8133\n","Epoch [43/500], Loss: 0.6405421495437622, Train Acc: 0.8023, Val Loss: 0.5882969498634338, Val Acc: 0.8139\n","Epoch [44/500], Loss: 0.6348980069160461, Train Acc: 0.8019, Val Loss: 0.5872327089309692, Val Acc: 0.8112\n","Epoch [45/500], Loss: 0.6290761232376099, Train Acc: 0.8025, Val Loss: 0.5882558822631836, Val Acc: 0.8166\n","Epoch [46/500], Loss: 0.6236798763275146, Train Acc: 0.8036, Val Loss: 0.5827616453170776, Val Acc: 0.8159\n","Epoch [47/500], Loss: 0.6183560490608215, Train Acc: 0.8048, Val Loss: 0.5784640908241272, Val Acc: 0.8153\n","Epoch [48/500], Loss: 0.6129878163337708, Train Acc: 0.8051, Val Loss: 0.576221764087677, Val Acc: 0.8199\n","Epoch [49/500], Loss: 0.6080287098884583, Train Acc: 0.8074, Val Loss: 0.5698517560958862, Val Acc: 0.8199\n","Epoch [50/500], Loss: 0.6030892729759216, Train Acc: 0.8080, Val Loss: 0.5647475123405457, Val Acc: 0.8213\n","Epoch [51/500], Loss: 0.5979604125022888, Train Acc: 0.8092, Val Loss: 0.5629063248634338, Val Acc: 0.8240\n","Epoch [52/500], Loss: 0.592893660068512, Train Acc: 0.8112, Val Loss: 0.5618767142295837, Val Acc: 0.8260\n","Epoch [53/500], Loss: 0.5886069536209106, Train Acc: 0.8121, Val Loss: 0.554129421710968, Val Acc: 0.8240\n","Epoch [54/500], Loss: 0.5836696028709412, Train Acc: 0.8130, Val Loss: 0.5509387254714966, Val Acc: 0.8246\n","Epoch [55/500], Loss: 0.5791475772857666, Train Acc: 0.8132, Val Loss: 0.5490721464157104, Val Acc: 0.8253\n","Epoch [56/500], Loss: 0.5746103525161743, Train Acc: 0.8145, Val Loss: 0.5503350496292114, Val Acc: 0.8260\n","Epoch [57/500], Loss: 0.569987416267395, Train Acc: 0.8155, Val Loss: 0.5454406142234802, Val Acc: 0.8253\n","Epoch [58/500], Loss: 0.5657335519790649, Train Acc: 0.8161, Val Loss: 0.5418757796287537, Val Acc: 0.8246\n","Epoch [59/500], Loss: 0.5614232420921326, Train Acc: 0.8184, Val Loss: 0.5400985479354858, Val Acc: 0.8307\n","Epoch [60/500], Loss: 0.5571243762969971, Train Acc: 0.8198, Val Loss: 0.5392580032348633, Val Acc: 0.8266\n","Epoch [61/500], Loss: 0.5530214309692383, Train Acc: 0.8195, Val Loss: 0.5349343419075012, Val Acc: 0.8273\n","Epoch [62/500], Loss: 0.5489693880081177, Train Acc: 0.8205, Val Loss: 0.5339811444282532, Val Acc: 0.8293\n","Epoch [63/500], Loss: 0.5452172160148621, Train Acc: 0.8230, Val Loss: 0.5303815603256226, Val Acc: 0.8320\n","Epoch [64/500], Loss: 0.540966808795929, Train Acc: 0.8241, Val Loss: 0.5306566953659058, Val Acc: 0.8307\n","Epoch [65/500], Loss: 0.5370287299156189, Train Acc: 0.8247, Val Loss: 0.5304346084594727, Val Acc: 0.8300\n","Epoch [66/500], Loss: 0.5330552458763123, Train Acc: 0.8267, Val Loss: 0.5295701026916504, Val Acc: 0.8300\n","Epoch [67/500], Loss: 0.5292707085609436, Train Acc: 0.8279, Val Loss: 0.528244137763977, Val Acc: 0.8353\n","Epoch [68/500], Loss: 0.5255053043365479, Train Acc: 0.8296, Val Loss: 0.5279002785682678, Val Acc: 0.8340\n","Epoch [69/500], Loss: 0.521963894367218, Train Acc: 0.8295, Val Loss: 0.5258472561836243, Val Acc: 0.8347\n","Epoch [70/500], Loss: 0.5183674693107605, Train Acc: 0.8313, Val Loss: 0.5186055302619934, Val Acc: 0.8327\n","Epoch [71/500], Loss: 0.5146545171737671, Train Acc: 0.8327, Val Loss: 0.5164838433265686, Val Acc: 0.8347\n","Epoch [72/500], Loss: 0.5112981200218201, Train Acc: 0.8333, Val Loss: 0.5193424820899963, Val Acc: 0.8353\n","Epoch [73/500], Loss: 0.5074846744537354, Train Acc: 0.8348, Val Loss: 0.5180956125259399, Val Acc: 0.8380\n","Epoch [74/500], Loss: 0.5039969086647034, Train Acc: 0.8369, Val Loss: 0.5154868960380554, Val Acc: 0.8414\n","Epoch [75/500], Loss: 0.5005599856376648, Train Acc: 0.8372, Val Loss: 0.5136699676513672, Val Acc: 0.8434\n","Epoch [76/500], Loss: 0.4971087872982025, Train Acc: 0.8399, Val Loss: 0.5119413137435913, Val Acc: 0.8414\n","Epoch [77/500], Loss: 0.4936099350452423, Train Acc: 0.8393, Val Loss: 0.5113065242767334, Val Acc: 0.8407\n","Epoch [78/500], Loss: 0.490363210439682, Train Acc: 0.8408, Val Loss: 0.5097284317016602, Val Acc: 0.8440\n","Epoch [79/500], Loss: 0.4869276285171509, Train Acc: 0.8426, Val Loss: 0.5082491636276245, Val Acc: 0.8427\n","Epoch [80/500], Loss: 0.48367777466773987, Train Acc: 0.8440, Val Loss: 0.5066081285476685, Val Acc: 0.8420\n","Epoch [81/500], Loss: 0.48075807094573975, Train Acc: 0.8429, Val Loss: 0.5093007683753967, Val Acc: 0.8474\n","Epoch [82/500], Loss: 0.4846658408641815, Train Acc: 0.8440, Val Loss: 0.5112758278846741, Val Acc: 0.8394\n","Epoch [83/500], Loss: 0.49190664291381836, Train Acc: 0.8379, Val Loss: 0.5073487162590027, Val Acc: 0.8420\n","Epoch [84/500], Loss: 0.4834286570549011, Train Acc: 0.8435, Val Loss: 0.5026645660400391, Val Acc: 0.8487\n","Epoch [85/500], Loss: 0.4762939512729645, Train Acc: 0.8425, Val Loss: 0.5050027370452881, Val Acc: 0.8447\n","Epoch [86/500], Loss: 0.47164657711982727, Train Acc: 0.8484, Val Loss: 0.5141626000404358, Val Acc: 0.8434\n","Epoch [87/500], Loss: 0.47251057624816895, Train Acc: 0.8469, Val Loss: 0.5084652304649353, Val Acc: 0.8467\n","Epoch [88/500], Loss: 0.46317851543426514, Train Acc: 0.8475, Val Loss: 0.5092331171035767, Val Acc: 0.8514\n","Epoch [89/500], Loss: 0.46424388885498047, Train Acc: 0.8496, Val Loss: 0.5004836320877075, Val Acc: 0.8514\n","Epoch [90/500], Loss: 0.45797237753868103, Train Acc: 0.8505, Val Loss: 0.5016072392463684, Val Acc: 0.8501\n","Epoch [91/500], Loss: 0.4560566544532776, Train Acc: 0.8505, Val Loss: 0.5045335292816162, Val Acc: 0.8507\n","Epoch [92/500], Loss: 0.4530060291290283, Train Acc: 0.8524, Val Loss: 0.5078651905059814, Val Acc: 0.8574\n","Epoch [93/500], Loss: 0.449942409992218, Train Acc: 0.8550, Val Loss: 0.5082288980484009, Val Acc: 0.8574\n","Epoch [94/500], Loss: 0.44512343406677246, Train Acc: 0.8567, Val Loss: 0.5113329887390137, Val Acc: 0.8568\n","Epoch [95/500], Loss: 0.4442886412143707, Train Acc: 0.8549, Val Loss: 0.5056136250495911, Val Acc: 0.8527\n","Epoch [96/500], Loss: 0.4397442042827606, Train Acc: 0.8560, Val Loss: 0.5001986026763916, Val Acc: 0.8621\n","Epoch [97/500], Loss: 0.4377681016921997, Train Acc: 0.8592, Val Loss: 0.4943012595176697, Val Acc: 0.8568\n","Epoch [98/500], Loss: 0.43511754274368286, Train Acc: 0.8599, Val Loss: 0.4955219626426697, Val Acc: 0.8568\n","Epoch [99/500], Loss: 0.4325523376464844, Train Acc: 0.8586, Val Loss: 0.5032450556755066, Val Acc: 0.8581\n","Epoch [100/500], Loss: 0.42989230155944824, Train Acc: 0.8604, Val Loss: 0.5009124279022217, Val Acc: 0.8594\n","Epoch [101/500], Loss: 0.4273582398891449, Train Acc: 0.8607, Val Loss: 0.4968665838241577, Val Acc: 0.8581\n","Epoch [102/500], Loss: 0.42442798614501953, Train Acc: 0.8623, Val Loss: 0.49636369943618774, Val Acc: 0.8588\n","Epoch [103/500], Loss: 0.4223916530609131, Train Acc: 0.8643, Val Loss: 0.4911450147628784, Val Acc: 0.8594\n","Epoch [104/500], Loss: 0.4202905595302582, Train Acc: 0.8629, Val Loss: 0.48885613679885864, Val Acc: 0.8588\n","Epoch [105/500], Loss: 0.41683369874954224, Train Acc: 0.8653, Val Loss: 0.49088314175605774, Val Acc: 0.8614\n","Epoch [106/500], Loss: 0.41458240151405334, Train Acc: 0.8662, Val Loss: 0.4901767671108246, Val Acc: 0.8554\n","Epoch [107/500], Loss: 0.4126133620738983, Train Acc: 0.8638, Val Loss: 0.4911287724971771, Val Acc: 0.8608\n","Epoch [108/500], Loss: 0.40938830375671387, Train Acc: 0.8682, Val Loss: 0.488800048828125, Val Acc: 0.8594\n","Epoch [109/500], Loss: 0.40739861130714417, Train Acc: 0.8675, Val Loss: 0.48789265751838684, Val Acc: 0.8594\n","Epoch [110/500], Loss: 0.4052688181400299, Train Acc: 0.8669, Val Loss: 0.48823267221450806, Val Acc: 0.8594\n","Epoch [111/500], Loss: 0.40256235003471375, Train Acc: 0.8704, Val Loss: 0.4876517355442047, Val Acc: 0.8588\n","Epoch [112/500], Loss: 0.399995356798172, Train Acc: 0.8701, Val Loss: 0.4871722459793091, Val Acc: 0.8608\n","Epoch [113/500], Loss: 0.3978753685951233, Train Acc: 0.8699, Val Loss: 0.4874347746372223, Val Acc: 0.8574\n","Epoch [114/500], Loss: 0.3961661756038666, Train Acc: 0.8711, Val Loss: 0.4865233302116394, Val Acc: 0.8628\n","Epoch [115/500], Loss: 0.39374232292175293, Train Acc: 0.8715, Val Loss: 0.4856792986392975, Val Acc: 0.8608\n","Epoch [116/500], Loss: 0.3921785354614258, Train Acc: 0.8714, Val Loss: 0.4887787997722626, Val Acc: 0.8614\n","Epoch [117/500], Loss: 0.3938809335231781, Train Acc: 0.8719, Val Loss: 0.48706287145614624, Val Acc: 0.8561\n","Epoch [118/500], Loss: 0.3917883038520813, Train Acc: 0.8692, Val Loss: 0.48876523971557617, Val Acc: 0.8614\n","Epoch [119/500], Loss: 0.38932931423187256, Train Acc: 0.8734, Val Loss: 0.48379987478256226, Val Acc: 0.8628\n","Epoch [120/500], Loss: 0.3857884705066681, Train Acc: 0.8728, Val Loss: 0.48214051127433777, Val Acc: 0.8641\n","Epoch [121/500], Loss: 0.3841940462589264, Train Acc: 0.8721, Val Loss: 0.479904443025589, Val Acc: 0.8628\n","Epoch [122/500], Loss: 0.3803013861179352, Train Acc: 0.8758, Val Loss: 0.48153871297836304, Val Acc: 0.8648\n","Epoch [123/500], Loss: 0.3790726363658905, Train Acc: 0.8747, Val Loss: 0.4829581677913666, Val Acc: 0.8621\n","Epoch [124/500], Loss: 0.376260370016098, Train Acc: 0.8773, Val Loss: 0.4807947874069214, Val Acc: 0.8655\n","Epoch [125/500], Loss: 0.37518107891082764, Train Acc: 0.8760, Val Loss: 0.4797191321849823, Val Acc: 0.8641\n","Epoch [126/500], Loss: 0.3736953139305115, Train Acc: 0.8747, Val Loss: 0.48258644342422485, Val Acc: 0.8641\n","Epoch [127/500], Loss: 0.37381091713905334, Train Acc: 0.8769, Val Loss: 0.4783153533935547, Val Acc: 0.8655\n","Epoch [128/500], Loss: 0.37269702553749084, Train Acc: 0.8761, Val Loss: 0.4806697964668274, Val Acc: 0.8635\n","Epoch [129/500], Loss: 0.3721887171268463, Train Acc: 0.8759, Val Loss: 0.5270401239395142, Val Acc: 0.8601\n","Epoch [130/500], Loss: 0.372509241104126, Train Acc: 0.8780, Val Loss: 0.5207511782646179, Val Acc: 0.8594\n","Epoch [131/500], Loss: 0.37038785219192505, Train Acc: 0.8767, Val Loss: 0.5051372647285461, Val Acc: 0.8655\n","Epoch [132/500], Loss: 0.36531195044517517, Train Acc: 0.8796, Val Loss: 0.4875243902206421, Val Acc: 0.8648\n","Epoch [133/500], Loss: 0.36149531602859497, Train Acc: 0.8798, Val Loss: 0.4768984913825989, Val Acc: 0.8635\n","Epoch [134/500], Loss: 0.3605155646800995, Train Acc: 0.8806, Val Loss: 0.4954938590526581, Val Acc: 0.8635\n","Epoch [135/500], Loss: 0.36097481846809387, Train Acc: 0.8823, Val Loss: 0.506495475769043, Val Acc: 0.8661\n","Epoch [136/500], Loss: 0.35989242792129517, Train Acc: 0.8797, Val Loss: 0.5055919289588928, Val Acc: 0.8621\n","Epoch [137/500], Loss: 0.35526272654533386, Train Acc: 0.8832, Val Loss: 0.48230621218681335, Val Acc: 0.8648\n","Epoch [138/500], Loss: 0.35254886746406555, Train Acc: 0.8844, Val Loss: 0.48418113589286804, Val Acc: 0.8668\n","Epoch [139/500], Loss: 0.35437706112861633, Train Acc: 0.8826, Val Loss: 0.476440966129303, Val Acc: 0.8635\n","Epoch [140/500], Loss: 0.35002368688583374, Train Acc: 0.8856, Val Loss: 0.4952486455440521, Val Acc: 0.8628\n","Epoch [141/500], Loss: 0.34947699308395386, Train Acc: 0.8831, Val Loss: 0.4895561635494232, Val Acc: 0.8655\n","Epoch [142/500], Loss: 0.3459731936454773, Train Acc: 0.8863, Val Loss: 0.48364540934562683, Val Acc: 0.8655\n","Epoch [143/500], Loss: 0.34482425451278687, Train Acc: 0.8868, Val Loss: 0.47988229990005493, Val Acc: 0.8701\n","Epoch [144/500], Loss: 0.3435550928115845, Train Acc: 0.8856, Val Loss: 0.4875378906726837, Val Acc: 0.8621\n","Epoch [145/500], Loss: 0.34273508191108704, Train Acc: 0.8890, Val Loss: 0.4828740060329437, Val Acc: 0.8648\n","Epoch [146/500], Loss: 0.3469838500022888, Train Acc: 0.8849, Val Loss: 0.48606666922569275, Val Acc: 0.8628\n","Epoch [147/500], Loss: 0.3480338454246521, Train Acc: 0.8869, Val Loss: 0.47839832305908203, Val Acc: 0.8614\n","Epoch [148/500], Loss: 0.3440806269645691, Train Acc: 0.8861, Val Loss: 0.4844883382320404, Val Acc: 0.8655\n","Early stopping at epoch 148\n","Test Loss: 0.5463305115699768, Test Accuracy: 0.8632707774798928\n","Precision: 0.8574, Recall: 0.8633, F1-score: 0.8590\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]}]}