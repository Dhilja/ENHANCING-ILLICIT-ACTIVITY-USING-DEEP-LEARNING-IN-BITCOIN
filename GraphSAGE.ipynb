{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORFqFZRhekwMNgdmsAVmGQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-3XgC5JGt9b","executionInfo":{"status":"ok","timestamp":1714460012911,"user_tz":-330,"elapsed":3739,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"a39d2d8f-a348-473e-9504-8da16e19b38f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install torch-geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H-N-haoIGxiW","executionInfo":{"status":"ok","timestamp":1714455607436,"user_tz":-330,"elapsed":14494,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"d96d0bd0-aa4b-4c35-eac5-4e428ee2a19f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.5.3\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import SAGEConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","# Define the GraphSAGE model\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super(GraphSAGE, self).__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=-1)\n","\n","# Initialize the GraphSAGE model\n","model = GraphSAGE(in_channels=X_train.shape[1], hidden_channels=128, out_channels=13)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Convert data to appropriate format\n","edge_index_train = edge_index_train.to(device)\n","edge_index_val = edge_index_val.to(device)\n","edge_index_test = edge_index_test.to(device)\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/train_f1_scores_GS.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/train_loss_GS.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/epochs_GS.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GS.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hxCY4Hx9GkNp","executionInfo":{"status":"ok","timestamp":1714460457397,"user_tz":-330,"elapsed":39963,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"213b61e8-871f-4d7a-b834-c6e211b4fc34"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.571683645248413, Train Acc: 0.0835,train F1-score:0.1311 Val Loss: 1.9960720539093018, Val Acc: 0.6412\n","Epoch [2/500], Loss: 1.956561803817749, Train Acc: 0.6018,train F1-score:0.5563 Val Loss: 1.5173274278640747, Val Acc: 0.6613\n","Epoch [3/500], Loss: 1.5179853439331055, Train Acc: 0.6326,train F1-score:0.5715 Val Loss: 1.1520590782165527, Val Acc: 0.6908\n","Epoch [4/500], Loss: 1.2453346252441406, Train Acc: 0.6464,train F1-score:0.5632 Val Loss: 0.9770436882972717, Val Acc: 0.7021\n","Epoch [5/500], Loss: 1.1177735328674316, Train Acc: 0.6563,train F1-score:0.5581 Val Loss: 0.9197558760643005, Val Acc: 0.7041\n","Epoch [6/500], Loss: 1.0572932958602905, Train Acc: 0.6632,train F1-score:0.5699 Val Loss: 0.892148494720459, Val Acc: 0.7296\n","Epoch [7/500], Loss: 1.0082656145095825, Train Acc: 0.6751,train F1-score:0.6062 Val Loss: 0.8791957497596741, Val Acc: 0.7229\n","Epoch [8/500], Loss: 0.9709280729293823, Train Acc: 0.6784,train F1-score:0.6361 Val Loss: 0.8701502680778503, Val Acc: 0.7135\n","Epoch [9/500], Loss: 0.9628139138221741, Train Acc: 0.6674,train F1-score:0.6389 Val Loss: 0.8467140793800354, Val Acc: 0.7376\n","Epoch [10/500], Loss: 0.9325307607650757, Train Acc: 0.6831,train F1-score:0.6517 Val Loss: 0.8273459076881409, Val Acc: 0.7443\n","Epoch [11/500], Loss: 0.9124378561973572, Train Acc: 0.6923,train F1-score:0.6531 Val Loss: 0.8076603412628174, Val Acc: 0.7443\n","Epoch [12/500], Loss: 0.8972839713096619, Train Acc: 0.7006,train F1-score:0.6546 Val Loss: 0.7847391366958618, Val Acc: 0.7523\n","Epoch [13/500], Loss: 0.8651031851768494, Train Acc: 0.7122,train F1-score:0.6655 Val Loss: 0.7623358964920044, Val Acc: 0.7651\n","Epoch [14/500], Loss: 0.8379641175270081, Train Acc: 0.7194,train F1-score:0.6768 Val Loss: 0.7480291724205017, Val Acc: 0.7704\n","Epoch [15/500], Loss: 0.830310046672821, Train Acc: 0.7210,train F1-score:0.6858 Val Loss: 0.7370930910110474, Val Acc: 0.7738\n","Epoch [16/500], Loss: 0.8103741407394409, Train Acc: 0.7254,train F1-score:0.6930 Val Loss: 0.7218809127807617, Val Acc: 0.7798\n","Epoch [17/500], Loss: 0.7893762588500977, Train Acc: 0.7319,train F1-score:0.6951 Val Loss: 0.7095513343811035, Val Acc: 0.7818\n","Epoch [18/500], Loss: 0.7769498229026794, Train Acc: 0.7392,train F1-score:0.7010 Val Loss: 0.7020751237869263, Val Acc: 0.7784\n","Epoch [19/500], Loss: 0.7614147663116455, Train Acc: 0.7406,train F1-score:0.6999 Val Loss: 0.6968377828598022, Val Acc: 0.7811\n","Epoch [20/500], Loss: 0.7525069117546082, Train Acc: 0.7484,train F1-score:0.7134 Val Loss: 0.6916368007659912, Val Acc: 0.7938\n","Epoch [21/500], Loss: 0.7317464351654053, Train Acc: 0.7541,train F1-score:0.7242 Val Loss: 0.6856125593185425, Val Acc: 0.7985\n","Epoch [22/500], Loss: 0.7219879031181335, Train Acc: 0.7600,train F1-score:0.7367 Val Loss: 0.6750901937484741, Val Acc: 0.8012\n","Epoch [23/500], Loss: 0.7148076891899109, Train Acc: 0.7631,train F1-score:0.7396 Val Loss: 0.6603609323501587, Val Acc: 0.8025\n","Epoch [24/500], Loss: 0.6930382251739502, Train Acc: 0.7670,train F1-score:0.7425 Val Loss: 0.6448102593421936, Val Acc: 0.8059\n","Epoch [25/500], Loss: 0.6817822456359863, Train Acc: 0.7696,train F1-score:0.7431 Val Loss: 0.6371815800666809, Val Acc: 0.8079\n","Epoch [26/500], Loss: 0.6773951053619385, Train Acc: 0.7707,train F1-score:0.7448 Val Loss: 0.6290268898010254, Val Acc: 0.8086\n","Epoch [27/500], Loss: 0.666731595993042, Train Acc: 0.7775,train F1-score:0.7553 Val Loss: 0.6204121708869934, Val Acc: 0.8166\n","Epoch [28/500], Loss: 0.6664274334907532, Train Acc: 0.7792,train F1-score:0.7594 Val Loss: 0.6105234622955322, Val Acc: 0.8213\n","Epoch [29/500], Loss: 0.6451261043548584, Train Acc: 0.7839,train F1-score:0.7674 Val Loss: 0.6016719341278076, Val Acc: 0.8233\n","Epoch [30/500], Loss: 0.6324595808982849, Train Acc: 0.7860,train F1-score:0.7696 Val Loss: 0.5940991640090942, Val Acc: 0.8226\n","Epoch [31/500], Loss: 0.6272690892219543, Train Acc: 0.7891,train F1-score:0.7714 Val Loss: 0.5913617610931396, Val Acc: 0.8240\n","Epoch [32/500], Loss: 0.6235882639884949, Train Acc: 0.7923,train F1-score:0.7743 Val Loss: 0.5857265591621399, Val Acc: 0.8280\n","Epoch [33/500], Loss: 0.6142054200172424, Train Acc: 0.7982,train F1-score:0.7809 Val Loss: 0.5784686207771301, Val Acc: 0.8293\n","Epoch [34/500], Loss: 0.6001303195953369, Train Acc: 0.7994,train F1-score:0.7846 Val Loss: 0.5703898072242737, Val Acc: 0.8320\n","Epoch [35/500], Loss: 0.6075524091720581, Train Acc: 0.8005,train F1-score:0.7874 Val Loss: 0.5628097057342529, Val Acc: 0.8293\n","Epoch [36/500], Loss: 0.587973952293396, Train Acc: 0.8017,train F1-score:0.7892 Val Loss: 0.557881772518158, Val Acc: 0.8333\n","Epoch [37/500], Loss: 0.5744718909263611, Train Acc: 0.8092,train F1-score:0.7964 Val Loss: 0.5533236265182495, Val Acc: 0.8387\n","Epoch [38/500], Loss: 0.5707806944847107, Train Acc: 0.8064,train F1-score:0.7920 Val Loss: 0.5474856495857239, Val Acc: 0.8373\n","Epoch [39/500], Loss: 0.5750333666801453, Train Acc: 0.8090,train F1-score:0.7944 Val Loss: 0.5404103994369507, Val Acc: 0.8367\n","Epoch [40/500], Loss: 0.5633032321929932, Train Acc: 0.8142,train F1-score:0.8014 Val Loss: 0.5375972390174866, Val Acc: 0.8380\n","Epoch [41/500], Loss: 0.5603591799736023, Train Acc: 0.8128,train F1-score:0.8016 Val Loss: 0.5351678133010864, Val Acc: 0.8394\n","Epoch [42/500], Loss: 0.5595277547836304, Train Acc: 0.8107,train F1-score:0.7999 Val Loss: 0.5297124981880188, Val Acc: 0.8394\n","Epoch [43/500], Loss: 0.5507959723472595, Train Acc: 0.8147,train F1-score:0.8023 Val Loss: 0.5273891091346741, Val Acc: 0.8394\n","Epoch [44/500], Loss: 0.5463725924491882, Train Acc: 0.8140,train F1-score:0.8020 Val Loss: 0.5286546349525452, Val Acc: 0.8394\n","Epoch [45/500], Loss: 0.5373110175132751, Train Acc: 0.8197,train F1-score:0.8082 Val Loss: 0.5283312797546387, Val Acc: 0.8420\n","Epoch [46/500], Loss: 0.5352667570114136, Train Acc: 0.8216,train F1-score:0.8103 Val Loss: 0.5287831425666809, Val Acc: 0.8400\n","Epoch [47/500], Loss: 0.528919517993927, Train Acc: 0.8242,train F1-score:0.8140 Val Loss: 0.5258382558822632, Val Acc: 0.8407\n","Epoch [48/500], Loss: 0.5196925401687622, Train Acc: 0.8217,train F1-score:0.8120 Val Loss: 0.5231772065162659, Val Acc: 0.8400\n","Epoch [49/500], Loss: 0.5180391073226929, Train Acc: 0.8250,train F1-score:0.8153 Val Loss: 0.5188372731208801, Val Acc: 0.8407\n","Epoch [50/500], Loss: 0.5150476098060608, Train Acc: 0.8253,train F1-score:0.8154 Val Loss: 0.5135959386825562, Val Acc: 0.8420\n","Epoch [51/500], Loss: 0.5110529065132141, Train Acc: 0.8259,train F1-score:0.8153 Val Loss: 0.5085163116455078, Val Acc: 0.8407\n","Epoch [52/500], Loss: 0.5159643888473511, Train Acc: 0.8211,train F1-score:0.8105 Val Loss: 0.5054110288619995, Val Acc: 0.8407\n","Epoch [53/500], Loss: 0.5049669146537781, Train Acc: 0.8301,train F1-score:0.8208 Val Loss: 0.50252765417099, Val Acc: 0.8440\n","Epoch [54/500], Loss: 0.49842050671577454, Train Acc: 0.8313,train F1-score:0.8223 Val Loss: 0.4993818998336792, Val Acc: 0.8440\n","Epoch [55/500], Loss: 0.49574142694473267, Train Acc: 0.8339,train F1-score:0.8249 Val Loss: 0.49555766582489014, Val Acc: 0.8454\n","Epoch [56/500], Loss: 0.4963143467903137, Train Acc: 0.8300,train F1-score:0.8207 Val Loss: 0.49125349521636963, Val Acc: 0.8454\n","Epoch [57/500], Loss: 0.4918193519115448, Train Acc: 0.8341,train F1-score:0.8244 Val Loss: 0.4872685670852661, Val Acc: 0.8487\n","Epoch [58/500], Loss: 0.4864407181739807, Train Acc: 0.8346,train F1-score:0.8251 Val Loss: 0.4861066937446594, Val Acc: 0.8474\n","Epoch [59/500], Loss: 0.48166781663894653, Train Acc: 0.8362,train F1-score:0.8276 Val Loss: 0.4837370812892914, Val Acc: 0.8461\n","Epoch [60/500], Loss: 0.4816560745239258, Train Acc: 0.8384,train F1-score:0.8304 Val Loss: 0.48160916566848755, Val Acc: 0.8474\n","Epoch [61/500], Loss: 0.4805503487586975, Train Acc: 0.8346,train F1-score:0.8261 Val Loss: 0.48057612776756287, Val Acc: 0.8494\n","Epoch [62/500], Loss: 0.47508159279823303, Train Acc: 0.8370,train F1-score:0.8278 Val Loss: 0.4789636731147766, Val Acc: 0.8501\n","Epoch [63/500], Loss: 0.46801188588142395, Train Acc: 0.8434,train F1-score:0.8343 Val Loss: 0.4779542088508606, Val Acc: 0.8527\n","Epoch [64/500], Loss: 0.47143489122390747, Train Acc: 0.8372,train F1-score:0.8282 Val Loss: 0.47676828503608704, Val Acc: 0.8548\n","Epoch [65/500], Loss: 0.4639987647533417, Train Acc: 0.8365,train F1-score:0.8284 Val Loss: 0.4742932915687561, Val Acc: 0.8541\n","Epoch [66/500], Loss: 0.4600353538990021, Train Acc: 0.8422,train F1-score:0.8346 Val Loss: 0.472577840089798, Val Acc: 0.8574\n","Epoch [67/500], Loss: 0.4589356780052185, Train Acc: 0.8418,train F1-score:0.8336 Val Loss: 0.4711703658103943, Val Acc: 0.8581\n","Epoch [68/500], Loss: 0.45737969875335693, Train Acc: 0.8424,train F1-score:0.8337 Val Loss: 0.47014689445495605, Val Acc: 0.8581\n","Epoch [69/500], Loss: 0.45524007081985474, Train Acc: 0.8437,train F1-score:0.8361 Val Loss: 0.4703841805458069, Val Acc: 0.8588\n","Epoch [70/500], Loss: 0.45376643538475037, Train Acc: 0.8453,train F1-score:0.8379 Val Loss: 0.4669739603996277, Val Acc: 0.8588\n","Epoch [71/500], Loss: 0.4541812241077423, Train Acc: 0.8440,train F1-score:0.8370 Val Loss: 0.46486997604370117, Val Acc: 0.8588\n","Epoch [72/500], Loss: 0.4462100565433502, Train Acc: 0.8458,train F1-score:0.8384 Val Loss: 0.46772703528404236, Val Acc: 0.8588\n","Epoch [73/500], Loss: 0.4401603043079376, Train Acc: 0.8468,train F1-score:0.8390 Val Loss: 0.4673539102077484, Val Acc: 0.8594\n","Epoch [74/500], Loss: 0.4392527639865875, Train Acc: 0.8472,train F1-score:0.8394 Val Loss: 0.46253904700279236, Val Acc: 0.8614\n","Epoch [75/500], Loss: 0.44757217168807983, Train Acc: 0.8467,train F1-score:0.8383 Val Loss: 0.4622972011566162, Val Acc: 0.8601\n","Epoch [76/500], Loss: 0.4381452202796936, Train Acc: 0.8510,train F1-score:0.8441 Val Loss: 0.4635387659072876, Val Acc: 0.8635\n","Epoch [77/500], Loss: 0.43386006355285645, Train Acc: 0.8531,train F1-score:0.8466 Val Loss: 0.4631710946559906, Val Acc: 0.8614\n","Epoch [78/500], Loss: 0.43404659628868103, Train Acc: 0.8488,train F1-score:0.8420 Val Loss: 0.45926809310913086, Val Acc: 0.8621\n","Epoch [79/500], Loss: 0.43391308188438416, Train Acc: 0.8526,train F1-score:0.8456 Val Loss: 0.45496901869773865, Val Acc: 0.8621\n","Epoch [80/500], Loss: 0.4310460686683655, Train Acc: 0.8499,train F1-score:0.8424 Val Loss: 0.45312437415122986, Val Acc: 0.8641\n","Epoch [81/500], Loss: 0.41801756620407104, Train Acc: 0.8549,train F1-score:0.8481 Val Loss: 0.45124733448028564, Val Acc: 0.8675\n","Epoch [82/500], Loss: 0.4268338680267334, Train Acc: 0.8532,train F1-score:0.8470 Val Loss: 0.4492413103580475, Val Acc: 0.8695\n","Epoch [83/500], Loss: 0.42307305335998535, Train Acc: 0.8530,train F1-score:0.8469 Val Loss: 0.4471966624259949, Val Acc: 0.8715\n","Epoch [84/500], Loss: 0.42060017585754395, Train Acc: 0.8552,train F1-score:0.8491 Val Loss: 0.4426376521587372, Val Acc: 0.8722\n","Epoch [85/500], Loss: 0.4213525652885437, Train Acc: 0.8554,train F1-score:0.8489 Val Loss: 0.4419509768486023, Val Acc: 0.8701\n","Epoch [86/500], Loss: 0.4193626642227173, Train Acc: 0.8561,train F1-score:0.8497 Val Loss: 0.4426891803741455, Val Acc: 0.8701\n","Epoch [87/500], Loss: 0.41831788420677185, Train Acc: 0.8563,train F1-score:0.8496 Val Loss: 0.44188374280929565, Val Acc: 0.8695\n","Epoch [88/500], Loss: 0.4165405333042145, Train Acc: 0.8564,train F1-score:0.8500 Val Loss: 0.4467141330242157, Val Acc: 0.8688\n","Epoch [89/500], Loss: 0.4124557077884674, Train Acc: 0.8591,train F1-score:0.8533 Val Loss: 0.4466160237789154, Val Acc: 0.8688\n","Epoch [90/500], Loss: 0.40798622369766235, Train Acc: 0.8587,train F1-score:0.8523 Val Loss: 0.44376105070114136, Val Acc: 0.8701\n","Epoch [91/500], Loss: 0.4093915820121765, Train Acc: 0.8571,train F1-score:0.8509 Val Loss: 0.4372214674949646, Val Acc: 0.8688\n","Epoch [92/500], Loss: 0.40747034549713135, Train Acc: 0.8614,train F1-score:0.8551 Val Loss: 0.43685469031333923, Val Acc: 0.8708\n","Epoch [93/500], Loss: 0.39573290944099426, Train Acc: 0.8635,train F1-score:0.8569 Val Loss: 0.43697014451026917, Val Acc: 0.8722\n","Epoch [94/500], Loss: 0.3948826491832733, Train Acc: 0.8653,train F1-score:0.8596 Val Loss: 0.43667545914649963, Val Acc: 0.8735\n","Epoch [95/500], Loss: 0.3942992389202118, Train Acc: 0.8643,train F1-score:0.8592 Val Loss: 0.4319511950016022, Val Acc: 0.8722\n","Epoch [96/500], Loss: 0.4030226767063141, Train Acc: 0.8628,train F1-score:0.8573 Val Loss: 0.4369533360004425, Val Acc: 0.8708\n","Epoch [97/500], Loss: 0.3944627642631531, Train Acc: 0.8635,train F1-score:0.8578 Val Loss: 0.44007694721221924, Val Acc: 0.8715\n","Epoch [98/500], Loss: 0.40439680218696594, Train Acc: 0.8627,train F1-score:0.8564 Val Loss: 0.43782728910446167, Val Acc: 0.8695\n","Epoch [99/500], Loss: 0.3979572355747223, Train Acc: 0.8620,train F1-score:0.8559 Val Loss: 0.4309767484664917, Val Acc: 0.8688\n","Epoch [100/500], Loss: 0.3899382948875427, Train Acc: 0.8626,train F1-score:0.8569 Val Loss: 0.4239979386329651, Val Acc: 0.8708\n","Epoch [101/500], Loss: 0.3925378918647766, Train Acc: 0.8661,train F1-score:0.8608 Val Loss: 0.425622820854187, Val Acc: 0.8728\n","Epoch [102/500], Loss: 0.3892776370048523, Train Acc: 0.8671,train F1-score:0.8615 Val Loss: 0.4247579574584961, Val Acc: 0.8728\n","Epoch [103/500], Loss: 0.3914938271045685, Train Acc: 0.8646,train F1-score:0.8595 Val Loss: 0.42290279269218445, Val Acc: 0.8735\n","Epoch [104/500], Loss: 0.3864800035953522, Train Acc: 0.8666,train F1-score:0.8614 Val Loss: 0.42386701703071594, Val Acc: 0.8715\n","Epoch [105/500], Loss: 0.38633865118026733, Train Acc: 0.8664,train F1-score:0.8608 Val Loss: 0.4252861440181732, Val Acc: 0.8735\n","Epoch [106/500], Loss: 0.38729944825172424, Train Acc: 0.8678,train F1-score:0.8622 Val Loss: 0.424349308013916, Val Acc: 0.8748\n","Epoch [107/500], Loss: 0.3815763294696808, Train Acc: 0.8679,train F1-score:0.8626 Val Loss: 0.42101213335990906, Val Acc: 0.8742\n","Epoch [108/500], Loss: 0.3841204047203064, Train Acc: 0.8696,train F1-score:0.8644 Val Loss: 0.41573092341423035, Val Acc: 0.8748\n","Epoch [109/500], Loss: 0.380825936794281, Train Acc: 0.8664,train F1-score:0.8612 Val Loss: 0.4124128818511963, Val Acc: 0.8735\n","Epoch [110/500], Loss: 0.38261598348617554, Train Acc: 0.8679,train F1-score:0.8626 Val Loss: 0.4114406406879425, Val Acc: 0.8762\n","Epoch [111/500], Loss: 0.38377469778060913, Train Acc: 0.8669,train F1-score:0.8616 Val Loss: 0.4121561050415039, Val Acc: 0.8775\n","Epoch [112/500], Loss: 0.37466177344322205, Train Acc: 0.8725,train F1-score:0.8674 Val Loss: 0.41507431864738464, Val Acc: 0.8795\n","Epoch [113/500], Loss: 0.37561944127082825, Train Acc: 0.8695,train F1-score:0.8642 Val Loss: 0.4126606583595276, Val Acc: 0.8775\n","Epoch [114/500], Loss: 0.3692891597747803, Train Acc: 0.8720,train F1-score:0.8670 Val Loss: 0.4156564772129059, Val Acc: 0.8775\n","Epoch [115/500], Loss: 0.3686501383781433, Train Acc: 0.8737,train F1-score:0.8685 Val Loss: 0.4189762771129608, Val Acc: 0.8735\n","Epoch [116/500], Loss: 0.37000471353530884, Train Acc: 0.8741,train F1-score:0.8694 Val Loss: 0.42641234397888184, Val Acc: 0.8735\n","Epoch [117/500], Loss: 0.3729668855667114, Train Acc: 0.8731,train F1-score:0.8678 Val Loss: 0.425558477640152, Val Acc: 0.8742\n","Epoch [118/500], Loss: 0.37212294340133667, Train Acc: 0.8724,train F1-score:0.8676 Val Loss: 0.4182676374912262, Val Acc: 0.8742\n","Epoch [119/500], Loss: 0.37417072057724, Train Acc: 0.8728,train F1-score:0.8682 Val Loss: 0.40787726640701294, Val Acc: 0.8762\n","Epoch [120/500], Loss: 0.3640559911727905, Train Acc: 0.8751,train F1-score:0.8703 Val Loss: 0.40252164006233215, Val Acc: 0.8775\n","Epoch [121/500], Loss: 0.3581714630126953, Train Acc: 0.8758,train F1-score:0.8708 Val Loss: 0.401047945022583, Val Acc: 0.8795\n","Epoch [122/500], Loss: 0.3637641668319702, Train Acc: 0.8788,train F1-score:0.8738 Val Loss: 0.3983398973941803, Val Acc: 0.8788\n","Epoch [123/500], Loss: 0.3577553629875183, Train Acc: 0.8762,train F1-score:0.8719 Val Loss: 0.39653751254081726, Val Acc: 0.8802\n","Epoch [124/500], Loss: 0.3630439043045044, Train Acc: 0.8771,train F1-score:0.8731 Val Loss: 0.3931003212928772, Val Acc: 0.8809\n","Epoch [125/500], Loss: 0.3573918342590332, Train Acc: 0.8780,train F1-score:0.8733 Val Loss: 0.3995702862739563, Val Acc: 0.8768\n","Epoch [126/500], Loss: 0.3582916855812073, Train Acc: 0.8777,train F1-score:0.8724 Val Loss: 0.4008287489414215, Val Acc: 0.8822\n","Epoch [127/500], Loss: 0.3621494770050049, Train Acc: 0.8767,train F1-score:0.8718 Val Loss: 0.4000650942325592, Val Acc: 0.8802\n","Epoch [128/500], Loss: 0.3599530756473541, Train Acc: 0.8743,train F1-score:0.8700 Val Loss: 0.39772921800613403, Val Acc: 0.8809\n","Epoch [129/500], Loss: 0.3534854054450989, Train Acc: 0.8776,train F1-score:0.8732 Val Loss: 0.3962081968784332, Val Acc: 0.8822\n","Epoch [130/500], Loss: 0.35342714190483093, Train Acc: 0.8804,train F1-score:0.8759 Val Loss: 0.39761143922805786, Val Acc: 0.8822\n","Epoch [131/500], Loss: 0.35343557596206665, Train Acc: 0.8807,train F1-score:0.8762 Val Loss: 0.39535051584243774, Val Acc: 0.8829\n","Epoch [132/500], Loss: 0.35359400510787964, Train Acc: 0.8774,train F1-score:0.8729 Val Loss: 0.39425942301750183, Val Acc: 0.8829\n","Epoch [133/500], Loss: 0.3523575961589813, Train Acc: 0.8801,train F1-score:0.8758 Val Loss: 0.39322689175605774, Val Acc: 0.8842\n","Epoch [134/500], Loss: 0.34928786754608154, Train Acc: 0.8787,train F1-score:0.8745 Val Loss: 0.3986915946006775, Val Acc: 0.8802\n","Epoch [135/500], Loss: 0.3448053002357483, Train Acc: 0.8826,train F1-score:0.8784 Val Loss: 0.40050357580184937, Val Acc: 0.8822\n","Epoch [136/500], Loss: 0.35126280784606934, Train Acc: 0.8798,train F1-score:0.8757 Val Loss: 0.39511287212371826, Val Acc: 0.8835\n","Epoch [137/500], Loss: 0.34754249453544617, Train Acc: 0.8785,train F1-score:0.8740 Val Loss: 0.3945061266422272, Val Acc: 0.8849\n","Epoch [138/500], Loss: 0.347368061542511, Train Acc: 0.8787,train F1-score:0.8747 Val Loss: 0.39401766657829285, Val Acc: 0.8869\n","Epoch [139/500], Loss: 0.34015926718711853, Train Acc: 0.8819,train F1-score:0.8779 Val Loss: 0.39428699016571045, Val Acc: 0.8849\n","Epoch [140/500], Loss: 0.34350255131721497, Train Acc: 0.8792,train F1-score:0.8750 Val Loss: 0.392892986536026, Val Acc: 0.8842\n","Epoch [141/500], Loss: 0.3423571288585663, Train Acc: 0.8829,train F1-score:0.8792 Val Loss: 0.38987404108047485, Val Acc: 0.8849\n","Epoch [142/500], Loss: 0.34699738025665283, Train Acc: 0.8802,train F1-score:0.8766 Val Loss: 0.3856883943080902, Val Acc: 0.8842\n","Epoch [143/500], Loss: 0.33821916580200195, Train Acc: 0.8849,train F1-score:0.8811 Val Loss: 0.39026889204978943, Val Acc: 0.8822\n","Epoch [144/500], Loss: 0.34430092573165894, Train Acc: 0.8804,train F1-score:0.8758 Val Loss: 0.40070009231567383, Val Acc: 0.8802\n","Epoch [145/500], Loss: 0.3482714891433716, Train Acc: 0.8780,train F1-score:0.8733 Val Loss: 0.4062870144844055, Val Acc: 0.8815\n","Epoch [146/500], Loss: 0.3370727300643921, Train Acc: 0.8830,train F1-score:0.8788 Val Loss: 0.40482813119888306, Val Acc: 0.8795\n","Epoch [147/500], Loss: 0.3423933982849121, Train Acc: 0.8792,train F1-score:0.8749 Val Loss: 0.39685073494911194, Val Acc: 0.8788\n","Epoch [148/500], Loss: 0.35070985555648804, Train Acc: 0.8834,train F1-score:0.8795 Val Loss: 0.38984182476997375, Val Acc: 0.8829\n","Epoch [149/500], Loss: 0.33914288878440857, Train Acc: 0.8820,train F1-score:0.8780 Val Loss: 0.39536356925964355, Val Acc: 0.8842\n","Epoch [150/500], Loss: 0.33961349725723267, Train Acc: 0.8835,train F1-score:0.8797 Val Loss: 0.39716678857803345, Val Acc: 0.8862\n","Epoch [151/500], Loss: 0.33577778935432434, Train Acc: 0.8824,train F1-score:0.8794 Val Loss: 0.4020240306854248, Val Acc: 0.8869\n","Epoch [152/500], Loss: 0.3377012610435486, Train Acc: 0.8842,train F1-score:0.8802 Val Loss: 0.404776006937027, Val Acc: 0.8835\n","Epoch [153/500], Loss: 0.34049832820892334, Train Acc: 0.8825,train F1-score:0.8787 Val Loss: 0.4125794768333435, Val Acc: 0.8835\n","Epoch [154/500], Loss: 0.3313748836517334, Train Acc: 0.8815,train F1-score:0.8776 Val Loss: 0.41326239705085754, Val Acc: 0.8842\n","Epoch [155/500], Loss: 0.3325846493244171, Train Acc: 0.8875,train F1-score:0.8837 Val Loss: 0.4078422784805298, Val Acc: 0.8842\n","Epoch [156/500], Loss: 0.33069443702697754, Train Acc: 0.8869,train F1-score:0.8833 Val Loss: 0.3997204601764679, Val Acc: 0.8829\n","Epoch [157/500], Loss: 0.3283514082431793, Train Acc: 0.8854,train F1-score:0.8818 Val Loss: 0.3934946656227112, Val Acc: 0.8809\n","Epoch [158/500], Loss: 0.3276602029800415, Train Acc: 0.8870,train F1-score:0.8831 Val Loss: 0.38762325048446655, Val Acc: 0.8822\n","Epoch [159/500], Loss: 0.3383641541004181, Train Acc: 0.8857,train F1-score:0.8813 Val Loss: 0.38635969161987305, Val Acc: 0.8829\n","Epoch [160/500], Loss: 0.3262070119380951, Train Acc: 0.8890,train F1-score:0.8849 Val Loss: 0.3897624611854553, Val Acc: 0.8862\n","Epoch [161/500], Loss: 0.3241879343986511, Train Acc: 0.8890,train F1-score:0.8854 Val Loss: 0.3969295620918274, Val Acc: 0.8855\n","Epoch [162/500], Loss: 0.3267838954925537, Train Acc: 0.8852,train F1-score:0.8814 Val Loss: 0.40384340286254883, Val Acc: 0.8862\n","Epoch [163/500], Loss: 0.32567402720451355, Train Acc: 0.8839,train F1-score:0.8799 Val Loss: 0.4087517559528351, Val Acc: 0.8862\n","Epoch [164/500], Loss: 0.32536786794662476, Train Acc: 0.8857,train F1-score:0.8816 Val Loss: 0.4095679223537445, Val Acc: 0.8876\n","Epoch [165/500], Loss: 0.32424259185791016, Train Acc: 0.8859,train F1-score:0.8821 Val Loss: 0.404377281665802, Val Acc: 0.8882\n","Epoch [166/500], Loss: 0.3278193175792694, Train Acc: 0.8849,train F1-score:0.8813 Val Loss: 0.39623814821243286, Val Acc: 0.8889\n","Epoch [167/500], Loss: 0.3244399130344391, Train Acc: 0.8895,train F1-score:0.8859 Val Loss: 0.39158615469932556, Val Acc: 0.8876\n","Epoch [168/500], Loss: 0.328867107629776, Train Acc: 0.8884,train F1-score:0.8847 Val Loss: 0.3951769769191742, Val Acc: 0.8876\n","Epoch [169/500], Loss: 0.3302554488182068, Train Acc: 0.8857,train F1-score:0.8826 Val Loss: 0.39890187978744507, Val Acc: 0.8862\n","Epoch [170/500], Loss: 0.31873607635498047, Train Acc: 0.8898,train F1-score:0.8870 Val Loss: 0.4012729823589325, Val Acc: 0.8835\n","Epoch [171/500], Loss: 0.3217522203922272, Train Acc: 0.8878,train F1-score:0.8841 Val Loss: 0.39995718002319336, Val Acc: 0.8862\n","Epoch [172/500], Loss: 0.3225782513618469, Train Acc: 0.8878,train F1-score:0.8844 Val Loss: 0.39891672134399414, Val Acc: 0.8889\n","Epoch [173/500], Loss: 0.3255910277366638, Train Acc: 0.8878,train F1-score:0.8849 Val Loss: 0.3989005386829376, Val Acc: 0.8882\n","Epoch [174/500], Loss: 0.32131409645080566, Train Acc: 0.8878,train F1-score:0.8845 Val Loss: 0.400313138961792, Val Acc: 0.8869\n","Epoch [175/500], Loss: 0.3185763657093048, Train Acc: 0.8921,train F1-score:0.8888 Val Loss: 0.39989152550697327, Val Acc: 0.8896\n","Epoch [176/500], Loss: 0.3229564130306244, Train Acc: 0.8875,train F1-score:0.8837 Val Loss: 0.3997196555137634, Val Acc: 0.8916\n","Epoch [177/500], Loss: 0.31569141149520874, Train Acc: 0.8903,train F1-score:0.8872 Val Loss: 0.40159469842910767, Val Acc: 0.8916\n","Epoch [178/500], Loss: 0.3161202669143677, Train Acc: 0.8903,train F1-score:0.8871 Val Loss: 0.4072004556655884, Val Acc: 0.8922\n","Epoch [179/500], Loss: 0.3162994980812073, Train Acc: 0.8915,train F1-score:0.8878 Val Loss: 0.406625896692276, Val Acc: 0.8882\n","Epoch [180/500], Loss: 0.31930580735206604, Train Acc: 0.8885,train F1-score:0.8856 Val Loss: 0.40406563878059387, Val Acc: 0.8889\n","Epoch [181/500], Loss: 0.31591615080833435, Train Acc: 0.8913,train F1-score:0.8878 Val Loss: 0.40074101090431213, Val Acc: 0.8909\n","Epoch [182/500], Loss: 0.31032493710517883, Train Acc: 0.8889,train F1-score:0.8847 Val Loss: 0.3971761167049408, Val Acc: 0.8929\n","Epoch [183/500], Loss: 0.3089257478713989, Train Acc: 0.8932,train F1-score:0.8897 Val Loss: 0.39725461602211, Val Acc: 0.8949\n","Epoch [184/500], Loss: 0.31880074739456177, Train Acc: 0.8916,train F1-score:0.8885 Val Loss: 0.3971285820007324, Val Acc: 0.8929\n","Epoch [185/500], Loss: 0.3119221329689026, Train Acc: 0.8916,train F1-score:0.8884 Val Loss: 0.3978552222251892, Val Acc: 0.8929\n","Epoch [186/500], Loss: 0.304619699716568, Train Acc: 0.8945,train F1-score:0.8915 Val Loss: 0.3988684117794037, Val Acc: 0.8909\n","Epoch [187/500], Loss: 0.30478090047836304, Train Acc: 0.8929,train F1-score:0.8893 Val Loss: 0.3953906297683716, Val Acc: 0.8902\n","Epoch [188/500], Loss: 0.30837321281433105, Train Acc: 0.8900,train F1-score:0.8865 Val Loss: 0.3879351317882538, Val Acc: 0.8882\n","Epoch [189/500], Loss: 0.311987042427063, Train Acc: 0.8940,train F1-score:0.8909 Val Loss: 0.3836471438407898, Val Acc: 0.8869\n","Epoch [190/500], Loss: 0.31112977862358093, Train Acc: 0.8937,train F1-score:0.8909 Val Loss: 0.38379982113838196, Val Acc: 0.8916\n","Epoch [191/500], Loss: 0.30535927414894104, Train Acc: 0.8948,train F1-score:0.8917 Val Loss: 0.38515132665634155, Val Acc: 0.8896\n","Epoch [192/500], Loss: 0.3100239634513855, Train Acc: 0.8920,train F1-score:0.8884 Val Loss: 0.38911187648773193, Val Acc: 0.8876\n","Epoch [193/500], Loss: 0.3050904870033264, Train Acc: 0.8934,train F1-score:0.8901 Val Loss: 0.3890456557273865, Val Acc: 0.8869\n","Epoch [194/500], Loss: 0.3037673532962799, Train Acc: 0.8912,train F1-score:0.8880 Val Loss: 0.3879852592945099, Val Acc: 0.8882\n","Epoch [195/500], Loss: 0.3048648238182068, Train Acc: 0.8942,train F1-score:0.8910 Val Loss: 0.38661155104637146, Val Acc: 0.8889\n","Epoch [196/500], Loss: 0.30313658714294434, Train Acc: 0.8921,train F1-score:0.8891 Val Loss: 0.38534367084503174, Val Acc: 0.8896\n","Epoch [197/500], Loss: 0.3004532754421234, Train Acc: 0.8972,train F1-score:0.8941 Val Loss: 0.38423073291778564, Val Acc: 0.8909\n","Epoch [198/500], Loss: 0.30036669969558716, Train Acc: 0.8942,train F1-score:0.8910 Val Loss: 0.38257065415382385, Val Acc: 0.8916\n","Epoch [199/500], Loss: 0.29930350184440613, Train Acc: 0.8949,train F1-score:0.8920 Val Loss: 0.38159728050231934, Val Acc: 0.8942\n","Epoch [200/500], Loss: 0.30290916562080383, Train Acc: 0.8937,train F1-score:0.8907 Val Loss: 0.38569018244743347, Val Acc: 0.8902\n","Epoch [201/500], Loss: 0.30308008193969727, Train Acc: 0.8954,train F1-score:0.8922 Val Loss: 0.38371533155441284, Val Acc: 0.8909\n","Epoch [202/500], Loss: 0.2983044683933258, Train Acc: 0.8962,train F1-score:0.8932 Val Loss: 0.3823831379413605, Val Acc: 0.8922\n","Epoch [203/500], Loss: 0.30857613682746887, Train Acc: 0.8955,train F1-score:0.8925 Val Loss: 0.38905128836631775, Val Acc: 0.8922\n","Epoch [204/500], Loss: 0.29672810435295105, Train Acc: 0.8945,train F1-score:0.8915 Val Loss: 0.39045029878616333, Val Acc: 0.8949\n","Epoch [205/500], Loss: 0.3040483593940735, Train Acc: 0.8933,train F1-score:0.8900 Val Loss: 0.38643550872802734, Val Acc: 0.8949\n","Epoch [206/500], Loss: 0.30459922552108765, Train Acc: 0.8956,train F1-score:0.8924 Val Loss: 0.3790462613105774, Val Acc: 0.8929\n","Epoch [207/500], Loss: 0.2987827956676483, Train Acc: 0.8965,train F1-score:0.8933 Val Loss: 0.38106569647789, Val Acc: 0.8956\n","Epoch [208/500], Loss: 0.3022432029247284, Train Acc: 0.8960,train F1-score:0.8929 Val Loss: 0.3853974938392639, Val Acc: 0.8963\n","Epoch [209/500], Loss: 0.2923855185508728, Train Acc: 0.8984,train F1-score:0.8958 Val Loss: 0.3867347538471222, Val Acc: 0.8942\n","Epoch [210/500], Loss: 0.298159122467041, Train Acc: 0.8966,train F1-score:0.8936 Val Loss: 0.38889408111572266, Val Acc: 0.8936\n","Epoch [211/500], Loss: 0.3036823868751526, Train Acc: 0.8957,train F1-score:0.8932 Val Loss: 0.38900524377822876, Val Acc: 0.8922\n","Epoch [212/500], Loss: 0.3027088940143585, Train Acc: 0.8942,train F1-score:0.8914 Val Loss: 0.3864637017250061, Val Acc: 0.8922\n","Epoch [213/500], Loss: 0.29365628957748413, Train Acc: 0.9004,train F1-score:0.8973 Val Loss: 0.3911105990409851, Val Acc: 0.8929\n","Epoch [214/500], Loss: 0.2936558127403259, Train Acc: 0.9003,train F1-score:0.8976 Val Loss: 0.39503443241119385, Val Acc: 0.8916\n","Epoch [215/500], Loss: 0.30210375785827637, Train Acc: 0.8945,train F1-score:0.8918 Val Loss: 0.3976174592971802, Val Acc: 0.8882\n","Epoch [216/500], Loss: 0.2935565114021301, Train Acc: 0.8938,train F1-score:0.8909 Val Loss: 0.39578795433044434, Val Acc: 0.8909\n","Epoch [217/500], Loss: 0.2891311049461365, Train Acc: 0.8965,train F1-score:0.8932 Val Loss: 0.3911649286746979, Val Acc: 0.8963\n","Epoch [218/500], Loss: 0.29850494861602783, Train Acc: 0.8950,train F1-score:0.8921 Val Loss: 0.3876575231552124, Val Acc: 0.8976\n","Epoch [219/500], Loss: 0.2978217899799347, Train Acc: 0.8966,train F1-score:0.8936 Val Loss: 0.38526302576065063, Val Acc: 0.8969\n","Epoch [220/500], Loss: 0.2936495840549469, Train Acc: 0.8985,train F1-score:0.8957 Val Loss: 0.38322675228118896, Val Acc: 0.8949\n","Epoch [221/500], Loss: 0.28995510935783386, Train Acc: 0.9003,train F1-score:0.8976 Val Loss: 0.3867008686065674, Val Acc: 0.8963\n","Epoch [222/500], Loss: 0.2934209406375885, Train Acc: 0.9014,train F1-score:0.8991 Val Loss: 0.3896688222885132, Val Acc: 0.8949\n","Epoch [223/500], Loss: 0.29187774658203125, Train Acc: 0.8967,train F1-score:0.8937 Val Loss: 0.3944600522518158, Val Acc: 0.8936\n","Epoch [224/500], Loss: 0.2927253246307373, Train Acc: 0.8969,train F1-score:0.8931 Val Loss: 0.39054784178733826, Val Acc: 0.8889\n","Epoch [225/500], Loss: 0.28853318095207214, Train Acc: 0.8972,train F1-score:0.8942 Val Loss: 0.38684552907943726, Val Acc: 0.8896\n","Epoch [226/500], Loss: 0.29407376050949097, Train Acc: 0.8974,train F1-score:0.8949 Val Loss: 0.3909543752670288, Val Acc: 0.8936\n","Epoch [227/500], Loss: 0.2875135838985443, Train Acc: 0.9010,train F1-score:0.8985 Val Loss: 0.394449383020401, Val Acc: 0.8963\n","Epoch [228/500], Loss: 0.28950244188308716, Train Acc: 0.9020,train F1-score:0.8996 Val Loss: 0.3960265815258026, Val Acc: 0.8936\n","Epoch [229/500], Loss: 0.2844794690608978, Train Acc: 0.9002,train F1-score:0.8976 Val Loss: 0.39661285281181335, Val Acc: 0.8942\n","Epoch [230/500], Loss: 0.2822771668434143, Train Acc: 0.8998,train F1-score:0.8967 Val Loss: 0.4026274085044861, Val Acc: 0.8942\n","Epoch [231/500], Loss: 0.28359392285346985, Train Acc: 0.8994,train F1-score:0.8964 Val Loss: 0.40255066752433777, Val Acc: 0.8942\n","Epoch [232/500], Loss: 0.2875140309333801, Train Acc: 0.8999,train F1-score:0.8976 Val Loss: 0.4041304886341095, Val Acc: 0.8942\n","Epoch [233/500], Loss: 0.28726449608802795, Train Acc: 0.9011,train F1-score:0.8986 Val Loss: 0.40394285321235657, Val Acc: 0.8949\n","Epoch [234/500], Loss: 0.28422462940216064, Train Acc: 0.9020,train F1-score:0.8992 Val Loss: 0.40481311082839966, Val Acc: 0.8942\n","Epoch [235/500], Loss: 0.2832305133342743, Train Acc: 0.9001,train F1-score:0.8972 Val Loss: 0.40331918001174927, Val Acc: 0.8942\n","Epoch [236/500], Loss: 0.2791840434074402, Train Acc: 0.9019,train F1-score:0.8996 Val Loss: 0.40084362030029297, Val Acc: 0.8949\n","Epoch [237/500], Loss: 0.2827528715133667, Train Acc: 0.9032,train F1-score:0.9009 Val Loss: 0.397964209318161, Val Acc: 0.8949\n","Epoch [238/500], Loss: 0.27962204813957214, Train Acc: 0.9026,train F1-score:0.9000 Val Loss: 0.3928096890449524, Val Acc: 0.8949\n","Epoch [239/500], Loss: 0.2827055752277374, Train Acc: 0.9038,train F1-score:0.9010 Val Loss: 0.3901643753051758, Val Acc: 0.8956\n","Epoch [240/500], Loss: 0.27655211091041565, Train Acc: 0.9041,train F1-score:0.9015 Val Loss: 0.3929998278617859, Val Acc: 0.8956\n","Epoch [241/500], Loss: 0.2801687717437744, Train Acc: 0.9033,train F1-score:0.9004 Val Loss: 0.39804109930992126, Val Acc: 0.8983\n","Epoch [242/500], Loss: 0.27984923124313354, Train Acc: 0.9014,train F1-score:0.8992 Val Loss: 0.39907559752464294, Val Acc: 0.8989\n","Epoch [243/500], Loss: 0.28294801712036133, Train Acc: 0.9029,train F1-score:0.9008 Val Loss: 0.39529550075531006, Val Acc: 0.8976\n","Epoch [244/500], Loss: 0.2754290997982025, Train Acc: 0.9035,train F1-score:0.9011 Val Loss: 0.3909764587879181, Val Acc: 0.8989\n","Epoch [245/500], Loss: 0.27908071875572205, Train Acc: 0.9012,train F1-score:0.8982 Val Loss: 0.3908192217350006, Val Acc: 0.8936\n","Epoch [246/500], Loss: 0.28225165605545044, Train Acc: 0.9003,train F1-score:0.8976 Val Loss: 0.39502307772636414, Val Acc: 0.8942\n","Epoch [247/500], Loss: 0.28835830092430115, Train Acc: 0.9003,train F1-score:0.8978 Val Loss: 0.39601022005081177, Val Acc: 0.8936\n","Epoch [248/500], Loss: 0.2831711769104004, Train Acc: 0.9019,train F1-score:0.8991 Val Loss: 0.3937249183654785, Val Acc: 0.8956\n","Epoch [249/500], Loss: 0.2761582136154175, Train Acc: 0.9017,train F1-score:0.8992 Val Loss: 0.3913663625717163, Val Acc: 0.8976\n","Epoch [250/500], Loss: 0.27945882081985474, Train Acc: 0.9000,train F1-score:0.8980 Val Loss: 0.3835446238517761, Val Acc: 0.8969\n","Epoch [251/500], Loss: 0.2826403081417084, Train Acc: 0.9016,train F1-score:0.8996 Val Loss: 0.3798815906047821, Val Acc: 0.8963\n","Epoch [252/500], Loss: 0.2753477096557617, Train Acc: 0.9032,train F1-score:0.9003 Val Loss: 0.37780019640922546, Val Acc: 0.8969\n","Epoch [253/500], Loss: 0.27981558442115784, Train Acc: 0.8980,train F1-score:0.8948 Val Loss: 0.37641260027885437, Val Acc: 0.8989\n","Epoch [254/500], Loss: 0.27626273036003113, Train Acc: 0.9051,train F1-score:0.9021 Val Loss: 0.3847027122974396, Val Acc: 0.8983\n","Epoch [255/500], Loss: 0.27779483795166016, Train Acc: 0.9027,train F1-score:0.9003 Val Loss: 0.396714448928833, Val Acc: 0.8989\n","Epoch [256/500], Loss: 0.28463634848594666, Train Acc: 0.9021,train F1-score:0.8996 Val Loss: 0.40324780344963074, Val Acc: 0.9003\n","Epoch [257/500], Loss: 0.2756892740726471, Train Acc: 0.9048,train F1-score:0.9029 Val Loss: 0.4070129692554474, Val Acc: 0.8949\n","Epoch [258/500], Loss: 0.27987903356552124, Train Acc: 0.9004,train F1-score:0.8981 Val Loss: 0.40621691942214966, Val Acc: 0.8936\n","Epoch [259/500], Loss: 0.27316686511039734, Train Acc: 0.9003,train F1-score:0.8973 Val Loss: 0.3994251787662506, Val Acc: 0.8956\n","Epoch [260/500], Loss: 0.2712539732456207, Train Acc: 0.9029,train F1-score:0.8999 Val Loss: 0.38965117931365967, Val Acc: 0.8956\n","Epoch [261/500], Loss: 0.28412848711013794, Train Acc: 0.9033,train F1-score:0.9013 Val Loss: 0.3902742266654968, Val Acc: 0.8942\n","Epoch [262/500], Loss: 0.27050501108169556, Train Acc: 0.9040,train F1-score:0.9021 Val Loss: 0.395723432302475, Val Acc: 0.8942\n","Epoch [263/500], Loss: 0.27530431747436523, Train Acc: 0.9031,train F1-score:0.9007 Val Loss: 0.39524805545806885, Val Acc: 0.8969\n","Epoch [264/500], Loss: 0.27449551224708557, Train Acc: 0.9024,train F1-score:0.8996 Val Loss: 0.38513433933258057, Val Acc: 0.8969\n","Epoch [265/500], Loss: 0.2746551036834717, Train Acc: 0.9073,train F1-score:0.9053 Val Loss: 0.3840361535549164, Val Acc: 0.8983\n","Epoch [266/500], Loss: 0.27537184953689575, Train Acc: 0.9069,train F1-score:0.9045 Val Loss: 0.38164180517196655, Val Acc: 0.8963\n","Epoch [267/500], Loss: 0.2772885262966156, Train Acc: 0.9050,train F1-score:0.9023 Val Loss: 0.37509334087371826, Val Acc: 0.8976\n","Epoch [268/500], Loss: 0.2703566253185272, Train Acc: 0.9067,train F1-score:0.9042 Val Loss: 0.3760650157928467, Val Acc: 0.8976\n","Epoch [269/500], Loss: 0.2726631760597229, Train Acc: 0.9048,train F1-score:0.9021 Val Loss: 0.3804367482662201, Val Acc: 0.8996\n","Epoch [270/500], Loss: 0.27453121542930603, Train Acc: 0.9062,train F1-score:0.9038 Val Loss: 0.3819545805454254, Val Acc: 0.9016\n","Epoch [271/500], Loss: 0.2707924544811249, Train Acc: 0.9080,train F1-score:0.9058 Val Loss: 0.391886830329895, Val Acc: 0.9023\n","Epoch [272/500], Loss: 0.27519291639328003, Train Acc: 0.9044,train F1-score:0.9021 Val Loss: 0.3920135796070099, Val Acc: 0.8976\n","Epoch [273/500], Loss: 0.2701605260372162, Train Acc: 0.9049,train F1-score:0.9023 Val Loss: 0.38958457112312317, Val Acc: 0.8989\n","Epoch [274/500], Loss: 0.2718147933483124, Train Acc: 0.9034,train F1-score:0.9011 Val Loss: 0.3829260766506195, Val Acc: 0.8983\n","Epoch [275/500], Loss: 0.26859191060066223, Train Acc: 0.9074,train F1-score:0.9053 Val Loss: 0.3820919692516327, Val Acc: 0.8996\n","Epoch [276/500], Loss: 0.26853147149086, Train Acc: 0.9088,train F1-score:0.9066 Val Loss: 0.38225868344306946, Val Acc: 0.8976\n","Epoch [277/500], Loss: 0.26713505387306213, Train Acc: 0.9080,train F1-score:0.9054 Val Loss: 0.38666877150535583, Val Acc: 0.8983\n","Epoch [278/500], Loss: 0.2725692391395569, Train Acc: 0.9062,train F1-score:0.9034 Val Loss: 0.3905192017555237, Val Acc: 0.8969\n","Epoch [279/500], Loss: 0.2670848071575165, Train Acc: 0.9044,train F1-score:0.9022 Val Loss: 0.39398568868637085, Val Acc: 0.9009\n","Epoch [280/500], Loss: 0.2704820930957794, Train Acc: 0.9063,train F1-score:0.9046 Val Loss: 0.3949088454246521, Val Acc: 0.8996\n","Epoch [281/500], Loss: 0.26850685477256775, Train Acc: 0.9057,train F1-score:0.9039 Val Loss: 0.39833492040634155, Val Acc: 0.8989\n","Epoch [282/500], Loss: 0.26625534892082214, Train Acc: 0.9077,train F1-score:0.9056 Val Loss: 0.40386730432510376, Val Acc: 0.8976\n","Epoch [283/500], Loss: 0.2720121741294861, Train Acc: 0.9069,train F1-score:0.9043 Val Loss: 0.3993178606033325, Val Acc: 0.8989\n","Epoch [284/500], Loss: 0.27422377467155457, Train Acc: 0.9051,train F1-score:0.9024 Val Loss: 0.3928529918193817, Val Acc: 0.8989\n","Epoch [285/500], Loss: 0.2635417580604553, Train Acc: 0.9084,train F1-score:0.9061 Val Loss: 0.39157480001449585, Val Acc: 0.8996\n","Epoch [286/500], Loss: 0.26249197125434875, Train Acc: 0.9091,train F1-score:0.9069 Val Loss: 0.39372798800468445, Val Acc: 0.8996\n","Epoch [287/500], Loss: 0.2680007517337799, Train Acc: 0.9064,train F1-score:0.9036 Val Loss: 0.39710289239883423, Val Acc: 0.8969\n","Epoch [288/500], Loss: 0.2659505009651184, Train Acc: 0.9072,train F1-score:0.9048 Val Loss: 0.3955806493759155, Val Acc: 0.9003\n","Epoch [289/500], Loss: 0.26901474595069885, Train Acc: 0.9062,train F1-score:0.9037 Val Loss: 0.3933517634868622, Val Acc: 0.8976\n","Epoch [290/500], Loss: 0.2714442312717438, Train Acc: 0.9039,train F1-score:0.9012 Val Loss: 0.39360159635543823, Val Acc: 0.8976\n","Epoch [291/500], Loss: 0.2698494791984558, Train Acc: 0.9073,train F1-score:0.9046 Val Loss: 0.39191192388534546, Val Acc: 0.8989\n","Epoch [292/500], Loss: 0.2638675570487976, Train Acc: 0.9073,train F1-score:0.9047 Val Loss: 0.3907310962677002, Val Acc: 0.9029\n","Epoch [293/500], Loss: 0.2640068829059601, Train Acc: 0.9083,train F1-score:0.9061 Val Loss: 0.39123401045799255, Val Acc: 0.9023\n","Epoch [294/500], Loss: 0.25880008935928345, Train Acc: 0.9075,train F1-score:0.9054 Val Loss: 0.39555928111076355, Val Acc: 0.8996\n","Epoch [295/500], Loss: 0.2584266662597656, Train Acc: 0.9086,train F1-score:0.9059 Val Loss: 0.3975154459476471, Val Acc: 0.8983\n","Epoch [296/500], Loss: 0.26991716027259827, Train Acc: 0.9043,train F1-score:0.9019 Val Loss: 0.3997252881526947, Val Acc: 0.8989\n","Epoch [297/500], Loss: 0.2579822838306427, Train Acc: 0.9143,train F1-score:0.9128 Val Loss: 0.4009177088737488, Val Acc: 0.8996\n","Epoch [298/500], Loss: 0.270168274641037, Train Acc: 0.9096,train F1-score:0.9080 Val Loss: 0.4007033109664917, Val Acc: 0.9003\n","Epoch [299/500], Loss: 0.26172584295272827, Train Acc: 0.9091,train F1-score:0.9071 Val Loss: 0.4033047556877136, Val Acc: 0.8996\n","Epoch [300/500], Loss: 0.26747581362724304, Train Acc: 0.9059,train F1-score:0.9034 Val Loss: 0.3988178074359894, Val Acc: 0.8996\n","Epoch [301/500], Loss: 0.2673206925392151, Train Acc: 0.9040,train F1-score:0.9013 Val Loss: 0.39368972182273865, Val Acc: 0.8989\n","Epoch [302/500], Loss: 0.2620367407798767, Train Acc: 0.9062,train F1-score:0.9038 Val Loss: 0.38950854539871216, Val Acc: 0.8989\n","Epoch [303/500], Loss: 0.25955045223236084, Train Acc: 0.9094,train F1-score:0.9071 Val Loss: 0.3891524076461792, Val Acc: 0.8989\n","Epoch [304/500], Loss: 0.2580547332763672, Train Acc: 0.9086,train F1-score:0.9060 Val Loss: 0.3914474546909332, Val Acc: 0.9029\n","Epoch [305/500], Loss: 0.2600577175617218, Train Acc: 0.9075,train F1-score:0.9051 Val Loss: 0.3955167531967163, Val Acc: 0.9050\n","Epoch [306/500], Loss: 0.26021966338157654, Train Acc: 0.9094,train F1-score:0.9072 Val Loss: 0.3989412188529968, Val Acc: 0.9029\n","Epoch [307/500], Loss: 0.26001667976379395, Train Acc: 0.9089,train F1-score:0.9069 Val Loss: 0.39991310238838196, Val Acc: 0.9023\n","Epoch [308/500], Loss: 0.25931018590927124, Train Acc: 0.9132,train F1-score:0.9110 Val Loss: 0.40080198645591736, Val Acc: 0.9009\n","Epoch [309/500], Loss: 0.26241323351860046, Train Acc: 0.9067,train F1-score:0.9042 Val Loss: 0.40186670422554016, Val Acc: 0.9016\n","Epoch [310/500], Loss: 0.26154807209968567, Train Acc: 0.9122,train F1-score:0.9097 Val Loss: 0.4021228551864624, Val Acc: 0.9003\n","Epoch [311/500], Loss: 0.265205055475235, Train Acc: 0.9074,train F1-score:0.9057 Val Loss: 0.40267258882522583, Val Acc: 0.9003\n","Epoch [312/500], Loss: 0.25973987579345703, Train Acc: 0.9109,train F1-score:0.9090 Val Loss: 0.39973732829093933, Val Acc: 0.8989\n","Epoch [313/500], Loss: 0.2550024390220642, Train Acc: 0.9116,train F1-score:0.9095 Val Loss: 0.39502522349357605, Val Acc: 0.8983\n","Epoch [314/500], Loss: 0.25827470421791077, Train Acc: 0.9091,train F1-score:0.9070 Val Loss: 0.3921853303909302, Val Acc: 0.8989\n","Epoch [315/500], Loss: 0.25840991735458374, Train Acc: 0.9097,train F1-score:0.9077 Val Loss: 0.3966660797595978, Val Acc: 0.8983\n","Epoch [316/500], Loss: 0.25630486011505127, Train Acc: 0.9143,train F1-score:0.9122 Val Loss: 0.4055240750312805, Val Acc: 0.8976\n","Epoch [317/500], Loss: 0.25891780853271484, Train Acc: 0.9084,train F1-score:0.9061 Val Loss: 0.4096061587333679, Val Acc: 0.8989\n","Epoch [318/500], Loss: 0.27180150151252747, Train Acc: 0.9093,train F1-score:0.9068 Val Loss: 0.41173067688941956, Val Acc: 0.9009\n","Epoch [319/500], Loss: 0.2616219222545624, Train Acc: 0.9117,train F1-score:0.9097 Val Loss: 0.418760746717453, Val Acc: 0.9009\n","Epoch [320/500], Loss: 0.259249210357666, Train Acc: 0.9095,train F1-score:0.9073 Val Loss: 0.4221639931201935, Val Acc: 0.9023\n","Epoch [321/500], Loss: 0.2588847279548645, Train Acc: 0.9091,train F1-score:0.9075 Val Loss: 0.4234750270843506, Val Acc: 0.9016\n","Epoch [322/500], Loss: 0.25124481320381165, Train Acc: 0.9113,train F1-score:0.9094 Val Loss: 0.4261249601840973, Val Acc: 0.9009\n","Epoch [323/500], Loss: 0.25856515765190125, Train Acc: 0.9067,train F1-score:0.9047 Val Loss: 0.4333827495574951, Val Acc: 0.8996\n","Epoch [324/500], Loss: 0.2556295096874237, Train Acc: 0.9130,train F1-score:0.9109 Val Loss: 0.43380439281463623, Val Acc: 0.8969\n","Epoch [325/500], Loss: 0.25007393956184387, Train Acc: 0.9107,train F1-score:0.9085 Val Loss: 0.4273064434528351, Val Acc: 0.9016\n","Epoch [326/500], Loss: 0.26288604736328125, Train Acc: 0.9101,train F1-score:0.9076 Val Loss: 0.41941308975219727, Val Acc: 0.9009\n","Epoch [327/500], Loss: 0.26021644473075867, Train Acc: 0.9072,train F1-score:0.9050 Val Loss: 0.41109707951545715, Val Acc: 0.8989\n","Epoch [328/500], Loss: 0.26007023453712463, Train Acc: 0.9079,train F1-score:0.9056 Val Loss: 0.4055210053920746, Val Acc: 0.8969\n","Epoch [329/500], Loss: 0.2537079155445099, Train Acc: 0.9114,train F1-score:0.9093 Val Loss: 0.4079414904117584, Val Acc: 0.8976\n","Epoch [330/500], Loss: 0.26114654541015625, Train Acc: 0.9065,train F1-score:0.9044 Val Loss: 0.41216641664505005, Val Acc: 0.8996\n","Epoch [331/500], Loss: 0.2583863139152527, Train Acc: 0.9124,train F1-score:0.9103 Val Loss: 0.41969966888427734, Val Acc: 0.9003\n","Epoch [332/500], Loss: 0.25380751490592957, Train Acc: 0.9112,train F1-score:0.9090 Val Loss: 0.43063321709632874, Val Acc: 0.8996\n","Epoch [333/500], Loss: 0.25156110525131226, Train Acc: 0.9105,train F1-score:0.9081 Val Loss: 0.43146368861198425, Val Acc: 0.9009\n","Epoch [334/500], Loss: 0.25671979784965515, Train Acc: 0.9092,train F1-score:0.9074 Val Loss: 0.42575082182884216, Val Acc: 0.9003\n","Epoch [335/500], Loss: 0.25543472170829773, Train Acc: 0.9101,train F1-score:0.9082 Val Loss: 0.42546844482421875, Val Acc: 0.9016\n","Epoch [336/500], Loss: 0.25511878728866577, Train Acc: 0.9107,train F1-score:0.9091 Val Loss: 0.429054856300354, Val Acc: 0.9016\n","Epoch [337/500], Loss: 0.2570911645889282, Train Acc: 0.9104,train F1-score:0.9081 Val Loss: 0.4316907227039337, Val Acc: 0.9016\n","Epoch [338/500], Loss: 0.25742626190185547, Train Acc: 0.9099,train F1-score:0.9077 Val Loss: 0.43101704120635986, Val Acc: 0.9003\n","Epoch [339/500], Loss: 0.2508799433708191, Train Acc: 0.9124,train F1-score:0.9105 Val Loss: 0.42259758710861206, Val Acc: 0.8989\n","Epoch [340/500], Loss: 0.25029510259628296, Train Acc: 0.9118,train F1-score:0.9100 Val Loss: 0.41283994913101196, Val Acc: 0.8963\n","Epoch [341/500], Loss: 0.2549910545349121, Train Acc: 0.9097,train F1-score:0.9080 Val Loss: 0.40841731429100037, Val Acc: 0.8989\n","Epoch [342/500], Loss: 0.25374266505241394, Train Acc: 0.9117,train F1-score:0.9096 Val Loss: 0.4053628146648407, Val Acc: 0.9009\n","Epoch [343/500], Loss: 0.2521359622478485, Train Acc: 0.9121,train F1-score:0.9099 Val Loss: 0.4040165841579437, Val Acc: 0.8989\n","Epoch [344/500], Loss: 0.2557623088359833, Train Acc: 0.9104,train F1-score:0.9083 Val Loss: 0.4050404131412506, Val Acc: 0.8983\n","Epoch [345/500], Loss: 0.24648067355155945, Train Acc: 0.9117,train F1-score:0.9098 Val Loss: 0.40723109245300293, Val Acc: 0.9009\n","Epoch [346/500], Loss: 0.25028228759765625, Train Acc: 0.9103,train F1-score:0.9079 Val Loss: 0.4120301902294159, Val Acc: 0.9009\n","Epoch [347/500], Loss: 0.25075462460517883, Train Acc: 0.9132,train F1-score:0.9115 Val Loss: 0.41635823249816895, Val Acc: 0.9009\n","Epoch [348/500], Loss: 0.2545008659362793, Train Acc: 0.9127,train F1-score:0.9107 Val Loss: 0.41703447699546814, Val Acc: 0.9029\n","Epoch [349/500], Loss: 0.2573986053466797, Train Acc: 0.9137,train F1-score:0.9118 Val Loss: 0.4172665476799011, Val Acc: 0.9023\n","Epoch [350/500], Loss: 0.24904052913188934, Train Acc: 0.9122,train F1-score:0.9103 Val Loss: 0.41480526328086853, Val Acc: 0.9036\n","Epoch [351/500], Loss: 0.24587389826774597, Train Acc: 0.9136,train F1-score:0.9116 Val Loss: 0.41704261302948, Val Acc: 0.9023\n","Epoch [352/500], Loss: 0.24805912375450134, Train Acc: 0.9130,train F1-score:0.9112 Val Loss: 0.42152854800224304, Val Acc: 0.9016\n","Epoch [353/500], Loss: 0.25180569291114807, Train Acc: 0.9101,train F1-score:0.9079 Val Loss: 0.4266717731952667, Val Acc: 0.8996\n","Epoch [354/500], Loss: 0.24719321727752686, Train Acc: 0.9152,train F1-score:0.9130 Val Loss: 0.4284271001815796, Val Acc: 0.8989\n","Epoch [355/500], Loss: 0.25345954298973083, Train Acc: 0.9131,train F1-score:0.9113 Val Loss: 0.4232204556465149, Val Acc: 0.8976\n","Epoch [356/500], Loss: 0.25311174988746643, Train Acc: 0.9094,train F1-score:0.9078 Val Loss: 0.41511353850364685, Val Acc: 0.9016\n","Epoch [357/500], Loss: 0.24806056916713715, Train Acc: 0.9106,train F1-score:0.9082 Val Loss: 0.41130751371383667, Val Acc: 0.9003\n","Epoch [358/500], Loss: 0.252882182598114, Train Acc: 0.9137,train F1-score:0.9112 Val Loss: 0.40723085403442383, Val Acc: 0.9016\n","Epoch [359/500], Loss: 0.24173305928707123, Train Acc: 0.9158,train F1-score:0.9138 Val Loss: 0.4097921550273895, Val Acc: 0.9029\n","Epoch [360/500], Loss: 0.24565616250038147, Train Acc: 0.9127,train F1-score:0.9112 Val Loss: 0.41778066754341125, Val Acc: 0.9036\n","Epoch [361/500], Loss: 0.24778541922569275, Train Acc: 0.9138,train F1-score:0.9120 Val Loss: 0.4280737340450287, Val Acc: 0.9036\n","Epoch [362/500], Loss: 0.2541698217391968, Train Acc: 0.9117,train F1-score:0.9097 Val Loss: 0.43023526668548584, Val Acc: 0.9023\n","Epoch [363/500], Loss: 0.24877703189849854, Train Acc: 0.9132,train F1-score:0.9112 Val Loss: 0.43025436997413635, Val Acc: 0.9043\n","Epoch [364/500], Loss: 0.2454216033220291, Train Acc: 0.9126,train F1-score:0.9111 Val Loss: 0.43241432309150696, Val Acc: 0.9050\n","Epoch [365/500], Loss: 0.2485775500535965, Train Acc: 0.9122,train F1-score:0.9107 Val Loss: 0.4389018416404724, Val Acc: 0.9056\n","Epoch [366/500], Loss: 0.25189724564552307, Train Acc: 0.9088,train F1-score:0.9069 Val Loss: 0.44350841641426086, Val Acc: 0.9023\n","Early stopping at epoch 366\n","Test Loss: 0.4719918966293335, Test Accuracy: 0.9081769436997319\n","Precision: 0.9044, Recall: 0.9082, F1-score: 0.9028\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_index_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_index_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_index_test).cpu().numpy()\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=8)\n","decision_tree.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(train_outputs)\n","val_pred = decision_tree.predict(val_outputs)\n","test_pred = decision_tree.predict(test_outputs)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GS_DT.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kamvOEQkIiRH","executionInfo":{"status":"ok","timestamp":1714460767121,"user_tz":-330,"elapsed":557,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"edf146aa-e22c-406a-a5fd-565a89d90e31"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.957126109529392\n","Validation Accuracy (Decision Tree): 0.8982597054886211\n","Test Accuracy (Decision Tree): 0.9054959785522788\n","Precision: 0.9002, Recall: 0.9055, F1-score: 0.9013\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_index_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_index_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_index_test).cpu().numpy()\n","\n","# Concatenate the GIN model output with the original feature matrices\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train a decision tree classifier\n","decision_tree = DecisionTreeClassifier(max_depth=10)\n","decision_tree.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the decision tree\n","train_pred = decision_tree.predict(X_train_combined)\n","val_pred = decision_tree.predict(X_val_combined)\n","test_pred = decision_tree.predict(X_test_combined)\n","\n","# Evaluate decision tree performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (Decision Tree): {train_acc}')\n","print(f'Validation Accuracy (Decision Tree): {val_acc}')\n","print(f'Test Accuracy (Decision Tree): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GS_DT_COMB.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"diA2Fy6KKTYg","executionInfo":{"status":"ok","timestamp":1714460771710,"user_tz":-330,"elapsed":1344,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"a13215eb-27c5-4421-a946-dca82efaffa3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (Decision Tree): 0.9789817451013231\n","Validation Accuracy (Decision Tree): 0.9089692101740294\n","Test Accuracy (Decision Tree): 0.9175603217158177\n","Precision: 0.9162, Recall: 0.9176, F1-score: 0.9144\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_index_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_index_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_index_test).cpu().numpy()\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(train_outputs, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(train_outputs)\n","val_pred = svm_classifier.predict(val_outputs)\n","test_pred = svm_classifier.predict(test_outputs)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(),test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GS_SVM.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzzc4-XaJ9Bs","executionInfo":{"status":"ok","timestamp":1714460784044,"user_tz":-330,"elapsed":10198,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"03a0488b-f3e7-4e1d-fb3f-a690c51695a1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9395411153910568\n","Validation Accuracy (SVM): 0.8975903614457831\n","Test Accuracy (SVM): 0.9054959785522788\n","Precision: 0.9021, Recall: 0.9055, F1-score: 0.9026\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the best GIN model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Get the output of the GIN model for the training, validation, and test sets\n","model.eval()\n","with torch.no_grad():\n","    train_outputs = model(X_train, edge_index_train).cpu().numpy()\n","    val_outputs = model(X_val, edge_index_val).cpu().numpy()\n","    test_outputs = model(X_test, edge_index_test).cpu().numpy()\n","\n","# Combine the output of the GIN model with the original features\n","X_train_combined = np.concatenate((X_train.cpu().numpy(), train_outputs), axis=1)\n","X_val_combined = np.concatenate((X_val.cpu().numpy(), val_outputs), axis=1)\n","X_test_combined = np.concatenate((X_test.cpu().numpy(), test_outputs), axis=1)\n","\n","# Train an SVM classifier\n","svm_classifier = SVC(kernel='linear')  # You can specify different kernel functions (e.g., 'linear', 'poly', 'rbf', etc.)\n","svm_classifier.fit(X_train_combined, y_train.cpu().numpy())\n","\n","# Predict labels using the SVM classifier\n","train_pred = svm_classifier.predict(X_train_combined)\n","val_pred = svm_classifier.predict(X_val_combined)\n","test_pred = svm_classifier.predict(X_test_combined)\n","\n","# Evaluate SVM performance\n","train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n","val_acc = accuracy_score(y_val.cpu().numpy(), val_pred)\n","test_acc = accuracy_score(y_true.cpu().numpy(), test_pred)\n","\n","print(f'Training Accuracy (SVM): {train_acc}')\n","print(f'Validation Accuracy (SVM): {val_acc}')\n","print(f'Test Accuracy (SVM): {test_acc}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), test_pred, average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","with open('/content/drive/MyDrive/PROJECT/f1_scores_GS_SVM_COMB.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkcFhZAKMnJV","executionInfo":{"status":"ok","timestamp":1714460798335,"user_tz":-330,"elapsed":14297,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"09368408-33bb-462a-b93c-558adc367bf8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy (SVM): 0.9510132306146374\n","Validation Accuracy (SVM): 0.8995983935742972\n","Test Accuracy (SVM): 0.9121983914209115\n","Precision: 0.9080, Recall: 0.9122, F1-score: 0.9091\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import SAGEConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n","        super(GraphSAGE, self).__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.convs = torch.nn.ModuleList([SAGEConv(hidden_channels, hidden_channels) for _ in range(num_layers - 2)])\n","        self.conv2 = SAGEConv(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        for conv in self.convs:\n","            x = F.relu(conv(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=-1)\n","# Initialize the GraphSAGE model\n","model = GraphSAGE(in_channels=X_train.shape[1], hidden_channels=128, out_channels=13,num_layers=1)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Convert data to appropriate format\n","edge_index_train = edge_index_train.to(device)\n","edge_index_val = edge_index_val.to(device)\n","edge_index_test = edge_index_test.to(device)\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model1.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer1/train_f1_scores_GS.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer1/train_loss_GS.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer1/epochs_GS.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model1.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/layer1/f1_score_GS.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTq1kD7fdtdK","executionInfo":{"status":"ok","timestamp":1714456195715,"user_tz":-330,"elapsed":38488,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"e31b5ff2-9e10-40eb-98cf-866e9f8c60c5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.630472183227539, Train Acc: 0.0605,train F1-score:0.0938 Val Loss: 2.056767702102661, Val Acc: 0.6975\n","Epoch [2/500], Loss: 2.0232532024383545, Train Acc: 0.6161,train F1-score:0.5896 Val Loss: 1.5753557682037354, Val Acc: 0.6941\n","Epoch [3/500], Loss: 1.5751813650131226, Train Acc: 0.6563,train F1-score:0.6057 Val Loss: 1.1893659830093384, Val Acc: 0.7021\n","Epoch [4/500], Loss: 1.257482647895813, Train Acc: 0.6661,train F1-score:0.6030 Val Loss: 0.9811868667602539, Val Acc: 0.7122\n","Epoch [5/500], Loss: 1.0920981168746948, Train Acc: 0.6717,train F1-score:0.6012 Val Loss: 0.9069438576698303, Val Acc: 0.7209\n","Epoch [6/500], Loss: 1.0436480045318604, Train Acc: 0.6764,train F1-score:0.6089 Val Loss: 0.878046452999115, Val Acc: 0.7396\n","Epoch [7/500], Loss: 1.033139944076538, Train Acc: 0.6818,train F1-score:0.6254 Val Loss: 0.8619298338890076, Val Acc: 0.7356\n","Epoch [8/500], Loss: 0.9932178854942322, Train Acc: 0.6800,train F1-score:0.6381 Val Loss: 0.8448105454444885, Val Acc: 0.7450\n","Epoch [9/500], Loss: 0.9705811142921448, Train Acc: 0.6882,train F1-score:0.6497 Val Loss: 0.8193026781082153, Val Acc: 0.7570\n","Epoch [10/500], Loss: 0.9243867993354797, Train Acc: 0.6980,train F1-score:0.6538 Val Loss: 0.8023679256439209, Val Acc: 0.7544\n","Epoch [11/500], Loss: 0.8899045586585999, Train Acc: 0.6999,train F1-score:0.6538 Val Loss: 0.7938104867935181, Val Acc: 0.7564\n","Epoch [12/500], Loss: 0.87797611951828, Train Acc: 0.7067,train F1-score:0.6622 Val Loss: 0.7837217450141907, Val Acc: 0.7677\n","Epoch [13/500], Loss: 0.867065966129303, Train Acc: 0.7086,train F1-score:0.6719 Val Loss: 0.7698395252227783, Val Acc: 0.7744\n","Epoch [14/500], Loss: 0.8411489129066467, Train Acc: 0.7189,train F1-score:0.6891 Val Loss: 0.7533140182495117, Val Acc: 0.7771\n","Epoch [15/500], Loss: 0.8185137510299683, Train Acc: 0.7303,train F1-score:0.6993 Val Loss: 0.7358161211013794, Val Acc: 0.7778\n","Epoch [16/500], Loss: 0.800335168838501, Train Acc: 0.7334,train F1-score:0.7000 Val Loss: 0.7221419215202332, Val Acc: 0.7758\n","Epoch [17/500], Loss: 0.783351480960846, Train Acc: 0.7384,train F1-score:0.6997 Val Loss: 0.7126847505569458, Val Acc: 0.7744\n","Epoch [18/500], Loss: 0.7599289417266846, Train Acc: 0.7438,train F1-score:0.7048 Val Loss: 0.7063164710998535, Val Acc: 0.7805\n","Epoch [19/500], Loss: 0.7599128484725952, Train Acc: 0.7449,train F1-score:0.7093 Val Loss: 0.7007644772529602, Val Acc: 0.7892\n","Epoch [20/500], Loss: 0.7533405423164368, Train Acc: 0.7436,train F1-score:0.7121 Val Loss: 0.688798725605011, Val Acc: 0.7885\n","Epoch [21/500], Loss: 0.7346951365470886, Train Acc: 0.7521,train F1-score:0.7232 Val Loss: 0.6719943881034851, Val Acc: 0.7885\n","Epoch [22/500], Loss: 0.7216130495071411, Train Acc: 0.7552,train F1-score:0.7275 Val Loss: 0.6592087745666504, Val Acc: 0.7878\n","Epoch [23/500], Loss: 0.7063050866127014, Train Acc: 0.7642,train F1-score:0.7361 Val Loss: 0.6517124176025391, Val Acc: 0.7918\n","Epoch [24/500], Loss: 0.6879168748855591, Train Acc: 0.7648,train F1-score:0.7386 Val Loss: 0.6435922384262085, Val Acc: 0.7979\n","Epoch [25/500], Loss: 0.6857947111129761, Train Acc: 0.7688,train F1-score:0.7457 Val Loss: 0.635489821434021, Val Acc: 0.7992\n","Epoch [26/500], Loss: 0.6709566116333008, Train Acc: 0.7712,train F1-score:0.7493 Val Loss: 0.6273019909858704, Val Acc: 0.8025\n","Epoch [27/500], Loss: 0.6587235927581787, Train Acc: 0.7758,train F1-score:0.7564 Val Loss: 0.6209656000137329, Val Acc: 0.8039\n","Epoch [28/500], Loss: 0.6539517641067505, Train Acc: 0.7804,train F1-score:0.7619 Val Loss: 0.6140672564506531, Val Acc: 0.8112\n","Epoch [29/500], Loss: 0.6363464593887329, Train Acc: 0.7853,train F1-score:0.7684 Val Loss: 0.6073431968688965, Val Acc: 0.8112\n","Epoch [30/500], Loss: 0.6350647807121277, Train Acc: 0.7851,train F1-score:0.7689 Val Loss: 0.5971404910087585, Val Acc: 0.8112\n","Epoch [31/500], Loss: 0.6196783781051636, Train Acc: 0.7887,train F1-score:0.7717 Val Loss: 0.5884907841682434, Val Acc: 0.8146\n","Epoch [32/500], Loss: 0.6206331253051758, Train Acc: 0.7959,train F1-score:0.7793 Val Loss: 0.5799897909164429, Val Acc: 0.8146\n","Epoch [33/500], Loss: 0.6096784472465515, Train Acc: 0.7938,train F1-score:0.7778 Val Loss: 0.5755969882011414, Val Acc: 0.8199\n","Epoch [34/500], Loss: 0.6031855344772339, Train Acc: 0.7958,train F1-score:0.7828 Val Loss: 0.5706260800361633, Val Acc: 0.8253\n","Epoch [35/500], Loss: 0.5907852649688721, Train Acc: 0.7994,train F1-score:0.7863 Val Loss: 0.5652721524238586, Val Acc: 0.8246\n","Epoch [36/500], Loss: 0.5942368507385254, Train Acc: 0.7979,train F1-score:0.7847 Val Loss: 0.5577647686004639, Val Acc: 0.8253\n","Epoch [37/500], Loss: 0.5780595541000366, Train Acc: 0.8069,train F1-score:0.7939 Val Loss: 0.549948513507843, Val Acc: 0.8280\n","Epoch [38/500], Loss: 0.5783828496932983, Train Acc: 0.8043,train F1-score:0.7902 Val Loss: 0.5420623421669006, Val Acc: 0.8286\n","Epoch [39/500], Loss: 0.571694016456604, Train Acc: 0.8047,train F1-score:0.7907 Val Loss: 0.5366568565368652, Val Acc: 0.8280\n","Epoch [40/500], Loss: 0.5621622204780579, Train Acc: 0.8069,train F1-score:0.7941 Val Loss: 0.5333597660064697, Val Acc: 0.8300\n","Epoch [41/500], Loss: 0.5600714087486267, Train Acc: 0.8083,train F1-score:0.7972 Val Loss: 0.5308662056922913, Val Acc: 0.8313\n","Epoch [42/500], Loss: 0.5538827180862427, Train Acc: 0.8139,train F1-score:0.8032 Val Loss: 0.5258287191390991, Val Acc: 0.8353\n","Epoch [43/500], Loss: 0.5469062924385071, Train Acc: 0.8190,train F1-score:0.8079 Val Loss: 0.5211779475212097, Val Acc: 0.8367\n","Epoch [44/500], Loss: 0.5401313304901123, Train Acc: 0.8164,train F1-score:0.8047 Val Loss: 0.5170609951019287, Val Acc: 0.8380\n","Epoch [45/500], Loss: 0.5363948941230774, Train Acc: 0.8185,train F1-score:0.8070 Val Loss: 0.5142893195152283, Val Acc: 0.8394\n","Epoch [46/500], Loss: 0.5322980880737305, Train Acc: 0.8200,train F1-score:0.8087 Val Loss: 0.5085251927375793, Val Acc: 0.8394\n","Epoch [47/500], Loss: 0.5285298824310303, Train Acc: 0.8197,train F1-score:0.8084 Val Loss: 0.5065439343452454, Val Acc: 0.8400\n","Epoch [48/500], Loss: 0.5266535878181458, Train Acc: 0.8196,train F1-score:0.8092 Val Loss: 0.5058522820472717, Val Acc: 0.8414\n","Epoch [49/500], Loss: 0.5215464234352112, Train Acc: 0.8248,train F1-score:0.8144 Val Loss: 0.5049408674240112, Val Acc: 0.8420\n","Epoch [50/500], Loss: 0.5102816820144653, Train Acc: 0.8282,train F1-score:0.8186 Val Loss: 0.5022290349006653, Val Acc: 0.8427\n","Epoch [51/500], Loss: 0.5133189558982849, Train Acc: 0.8272,train F1-score:0.8176 Val Loss: 0.5002414584159851, Val Acc: 0.8420\n","Epoch [52/500], Loss: 0.5067989230155945, Train Acc: 0.8261,train F1-score:0.8167 Val Loss: 0.4990798532962799, Val Acc: 0.8434\n","Epoch [53/500], Loss: 0.5052105784416199, Train Acc: 0.8263,train F1-score:0.8165 Val Loss: 0.49745485186576843, Val Acc: 0.8414\n","Epoch [54/500], Loss: 0.5013430714607239, Train Acc: 0.8284,train F1-score:0.8189 Val Loss: 0.4936128258705139, Val Acc: 0.8434\n","Epoch [55/500], Loss: 0.4978167712688446, Train Acc: 0.8324,train F1-score:0.8235 Val Loss: 0.4906666874885559, Val Acc: 0.8454\n","Epoch [56/500], Loss: 0.49353134632110596, Train Acc: 0.8330,train F1-score:0.8247 Val Loss: 0.4879510998725891, Val Acc: 0.8454\n","Epoch [57/500], Loss: 0.4850890636444092, Train Acc: 0.8323,train F1-score:0.8233 Val Loss: 0.48456206917762756, Val Acc: 0.8447\n","Epoch [58/500], Loss: 0.4895213842391968, Train Acc: 0.8311,train F1-score:0.8221 Val Loss: 0.48103195428848267, Val Acc: 0.8461\n","Epoch [59/500], Loss: 0.482067346572876, Train Acc: 0.8339,train F1-score:0.8257 Val Loss: 0.48196154832839966, Val Acc: 0.8481\n","Epoch [60/500], Loss: 0.4813782572746277, Train Acc: 0.8346,train F1-score:0.8262 Val Loss: 0.4811860918998718, Val Acc: 0.8494\n","Epoch [61/500], Loss: 0.47120681405067444, Train Acc: 0.8377,train F1-score:0.8296 Val Loss: 0.4805855453014374, Val Acc: 0.8501\n","Epoch [62/500], Loss: 0.4726547300815582, Train Acc: 0.8401,train F1-score:0.8320 Val Loss: 0.4779964089393616, Val Acc: 0.8514\n","Epoch [63/500], Loss: 0.47125044465065, Train Acc: 0.8370,train F1-score:0.8293 Val Loss: 0.47519007325172424, Val Acc: 0.8541\n","Epoch [64/500], Loss: 0.47094103693962097, Train Acc: 0.8368,train F1-score:0.8292 Val Loss: 0.47346240282058716, Val Acc: 0.8534\n","Epoch [65/500], Loss: 0.47216135263442993, Train Acc: 0.8351,train F1-score:0.8279 Val Loss: 0.4759584665298462, Val Acc: 0.8534\n","Epoch [66/500], Loss: 0.4634002149105072, Train Acc: 0.8382,train F1-score:0.8296 Val Loss: 0.47637683153152466, Val Acc: 0.8521\n","Epoch [67/500], Loss: 0.45342037081718445, Train Acc: 0.8423,train F1-score:0.8339 Val Loss: 0.4782940447330475, Val Acc: 0.8527\n","Epoch [68/500], Loss: 0.45359092950820923, Train Acc: 0.8438,train F1-score:0.8355 Val Loss: 0.47950640320777893, Val Acc: 0.8527\n","Epoch [69/500], Loss: 0.44848546385765076, Train Acc: 0.8439,train F1-score:0.8371 Val Loss: 0.4782921075820923, Val Acc: 0.8534\n","Epoch [70/500], Loss: 0.45268553495407104, Train Acc: 0.8422,train F1-score:0.8357 Val Loss: 0.47171035408973694, Val Acc: 0.8534\n","Epoch [71/500], Loss: 0.4505767226219177, Train Acc: 0.8398,train F1-score:0.8312 Val Loss: 0.4679891765117645, Val Acc: 0.8527\n","Epoch [72/500], Loss: 0.44632020592689514, Train Acc: 0.8442,train F1-score:0.8363 Val Loss: 0.4611555337905884, Val Acc: 0.8554\n","Epoch [73/500], Loss: 0.44565945863723755, Train Acc: 0.8437,train F1-score:0.8365 Val Loss: 0.4562656879425049, Val Acc: 0.8568\n","Epoch [74/500], Loss: 0.4365794062614441, Train Acc: 0.8489,train F1-score:0.8424 Val Loss: 0.45293956995010376, Val Acc: 0.8581\n","Epoch [75/500], Loss: 0.441116601228714, Train Acc: 0.8458,train F1-score:0.8386 Val Loss: 0.45161566138267517, Val Acc: 0.8581\n","Epoch [76/500], Loss: 0.4441569745540619, Train Acc: 0.8477,train F1-score:0.8404 Val Loss: 0.4535999596118927, Val Acc: 0.8541\n","Epoch [77/500], Loss: 0.435057133436203, Train Acc: 0.8480,train F1-score:0.8419 Val Loss: 0.459089994430542, Val Acc: 0.8541\n","Epoch [78/500], Loss: 0.4387359917163849, Train Acc: 0.8485,train F1-score:0.8424 Val Loss: 0.4632903039455414, Val Acc: 0.8548\n","Epoch [79/500], Loss: 0.4334923028945923, Train Acc: 0.8506,train F1-score:0.8443 Val Loss: 0.4656830132007599, Val Acc: 0.8521\n","Epoch [80/500], Loss: 0.4279264509677887, Train Acc: 0.8509,train F1-score:0.8432 Val Loss: 0.46430841088294983, Val Acc: 0.8541\n","Epoch [81/500], Loss: 0.4267868399620056, Train Acc: 0.8489,train F1-score:0.8421 Val Loss: 0.4606339633464813, Val Acc: 0.8548\n","Epoch [82/500], Loss: 0.42869850993156433, Train Acc: 0.8509,train F1-score:0.8447 Val Loss: 0.45421138405799866, Val Acc: 0.8588\n","Epoch [83/500], Loss: 0.4239537715911865, Train Acc: 0.8509,train F1-score:0.8446 Val Loss: 0.4521334171295166, Val Acc: 0.8581\n","Epoch [84/500], Loss: 0.4225989282131195, Train Acc: 0.8564,train F1-score:0.8500 Val Loss: 0.4524935185909271, Val Acc: 0.8601\n","Epoch [85/500], Loss: 0.4254986047744751, Train Acc: 0.8545,train F1-score:0.8483 Val Loss: 0.4502568244934082, Val Acc: 0.8621\n","Epoch [86/500], Loss: 0.4146274924278259, Train Acc: 0.8556,train F1-score:0.8484 Val Loss: 0.45402929186820984, Val Acc: 0.8608\n","Epoch [87/500], Loss: 0.41933587193489075, Train Acc: 0.8556,train F1-score:0.8490 Val Loss: 0.45727965235710144, Val Acc: 0.8588\n","Epoch [88/500], Loss: 0.41835612058639526, Train Acc: 0.8566,train F1-score:0.8507 Val Loss: 0.4552761912345886, Val Acc: 0.8614\n","Epoch [89/500], Loss: 0.40812793374061584, Train Acc: 0.8576,train F1-score:0.8521 Val Loss: 0.45101121068000793, Val Acc: 0.8601\n","Epoch [90/500], Loss: 0.41704487800598145, Train Acc: 0.8528,train F1-score:0.8469 Val Loss: 0.4514811336994171, Val Acc: 0.8601\n","Epoch [91/500], Loss: 0.4056583642959595, Train Acc: 0.8581,train F1-score:0.8520 Val Loss: 0.4503781795501709, Val Acc: 0.8608\n","Epoch [92/500], Loss: 0.4124852418899536, Train Acc: 0.8555,train F1-score:0.8489 Val Loss: 0.44582080841064453, Val Acc: 0.8601\n","Epoch [93/500], Loss: 0.4026464819908142, Train Acc: 0.8594,train F1-score:0.8531 Val Loss: 0.44216299057006836, Val Acc: 0.8614\n","Epoch [94/500], Loss: 0.40081989765167236, Train Acc: 0.8592,train F1-score:0.8525 Val Loss: 0.44125473499298096, Val Acc: 0.8628\n","Epoch [95/500], Loss: 0.4045467674732208, Train Acc: 0.8576,train F1-score:0.8519 Val Loss: 0.4379338324069977, Val Acc: 0.8661\n","Epoch [96/500], Loss: 0.3992922902107239, Train Acc: 0.8602,train F1-score:0.8542 Val Loss: 0.4369737207889557, Val Acc: 0.8668\n","Epoch [97/500], Loss: 0.3974092900753021, Train Acc: 0.8603,train F1-score:0.8544 Val Loss: 0.43726062774658203, Val Acc: 0.8648\n","Epoch [98/500], Loss: 0.3977857232093811, Train Acc: 0.8612,train F1-score:0.8550 Val Loss: 0.43982771039009094, Val Acc: 0.8635\n","Epoch [99/500], Loss: 0.3947550356388092, Train Acc: 0.8625,train F1-score:0.8563 Val Loss: 0.442183256149292, Val Acc: 0.8688\n","Epoch [100/500], Loss: 0.38978999853134155, Train Acc: 0.8633,train F1-score:0.8577 Val Loss: 0.44269800186157227, Val Acc: 0.8735\n","Epoch [101/500], Loss: 0.3947044909000397, Train Acc: 0.8648,train F1-score:0.8600 Val Loss: 0.4398152232170105, Val Acc: 0.8728\n","Epoch [102/500], Loss: 0.38955986499786377, Train Acc: 0.8619,train F1-score:0.8567 Val Loss: 0.43992629647254944, Val Acc: 0.8695\n","Epoch [103/500], Loss: 0.3860463798046112, Train Acc: 0.8661,train F1-score:0.8601 Val Loss: 0.4374847114086151, Val Acc: 0.8681\n","Epoch [104/500], Loss: 0.38854923844337463, Train Acc: 0.8617,train F1-score:0.8557 Val Loss: 0.43550679087638855, Val Acc: 0.8675\n","Epoch [105/500], Loss: 0.3823715150356293, Train Acc: 0.8696,train F1-score:0.8644 Val Loss: 0.4375859498977661, Val Acc: 0.8681\n","Epoch [106/500], Loss: 0.38485389947891235, Train Acc: 0.8682,train F1-score:0.8636 Val Loss: 0.4412168562412262, Val Acc: 0.8688\n","Epoch [107/500], Loss: 0.3860234022140503, Train Acc: 0.8678,train F1-score:0.8625 Val Loss: 0.43871554732322693, Val Acc: 0.8695\n","Epoch [108/500], Loss: 0.3820643424987793, Train Acc: 0.8652,train F1-score:0.8601 Val Loss: 0.4340521991252899, Val Acc: 0.8688\n","Epoch [109/500], Loss: 0.37630701065063477, Train Acc: 0.8705,train F1-score:0.8656 Val Loss: 0.4283093810081482, Val Acc: 0.8701\n","Epoch [110/500], Loss: 0.381770521402359, Train Acc: 0.8665,train F1-score:0.8617 Val Loss: 0.42825934290885925, Val Acc: 0.8695\n","Epoch [111/500], Loss: 0.3803172707557678, Train Acc: 0.8683,train F1-score:0.8630 Val Loss: 0.429940789937973, Val Acc: 0.8701\n","Epoch [112/500], Loss: 0.3752095103263855, Train Acc: 0.8687,train F1-score:0.8635 Val Loss: 0.43221601843833923, Val Acc: 0.8708\n","Epoch [113/500], Loss: 0.3806988596916199, Train Acc: 0.8669,train F1-score:0.8617 Val Loss: 0.43189820647239685, Val Acc: 0.8742\n","Epoch [114/500], Loss: 0.3760620653629303, Train Acc: 0.8674,train F1-score:0.8629 Val Loss: 0.4292914569377899, Val Acc: 0.8735\n","Epoch [115/500], Loss: 0.3731141686439514, Train Acc: 0.8682,train F1-score:0.8633 Val Loss: 0.432422399520874, Val Acc: 0.8762\n","Epoch [116/500], Loss: 0.37437567114830017, Train Acc: 0.8695,train F1-score:0.8646 Val Loss: 0.43034908175468445, Val Acc: 0.8768\n","Epoch [117/500], Loss: 0.3738950788974762, Train Acc: 0.8703,train F1-score:0.8655 Val Loss: 0.4267655611038208, Val Acc: 0.8715\n","Epoch [118/500], Loss: 0.3726135790348053, Train Acc: 0.8671,train F1-score:0.8619 Val Loss: 0.42091110348701477, Val Acc: 0.8715\n","Epoch [119/500], Loss: 0.3717218339443207, Train Acc: 0.8698,train F1-score:0.8648 Val Loss: 0.42502668499946594, Val Acc: 0.8695\n","Epoch [120/500], Loss: 0.3664165735244751, Train Acc: 0.8729,train F1-score:0.8688 Val Loss: 0.4272250235080719, Val Acc: 0.8715\n","Epoch [121/500], Loss: 0.3660966455936432, Train Acc: 0.8710,train F1-score:0.8664 Val Loss: 0.4289255738258362, Val Acc: 0.8681\n","Epoch [122/500], Loss: 0.3633526861667633, Train Acc: 0.8709,train F1-score:0.8656 Val Loss: 0.4265497326850891, Val Acc: 0.8675\n","Epoch [123/500], Loss: 0.3659827709197998, Train Acc: 0.8746,train F1-score:0.8700 Val Loss: 0.42895394563674927, Val Acc: 0.8681\n","Epoch [124/500], Loss: 0.367392361164093, Train Acc: 0.8746,train F1-score:0.8702 Val Loss: 0.4279427230358124, Val Acc: 0.8695\n","Epoch [125/500], Loss: 0.36304405331611633, Train Acc: 0.8742,train F1-score:0.8702 Val Loss: 0.4240829348564148, Val Acc: 0.8695\n","Epoch [126/500], Loss: 0.363585501909256, Train Acc: 0.8735,train F1-score:0.8686 Val Loss: 0.42137381434440613, Val Acc: 0.8715\n","Epoch [127/500], Loss: 0.3568572998046875, Train Acc: 0.8759,train F1-score:0.8710 Val Loss: 0.424978107213974, Val Acc: 0.8701\n","Epoch [128/500], Loss: 0.35802847146987915, Train Acc: 0.8727,train F1-score:0.8681 Val Loss: 0.42419400811195374, Val Acc: 0.8722\n","Epoch [129/500], Loss: 0.3572169542312622, Train Acc: 0.8751,train F1-score:0.8713 Val Loss: 0.4250473976135254, Val Acc: 0.8735\n","Epoch [130/500], Loss: 0.3580579161643982, Train Acc: 0.8756,train F1-score:0.8710 Val Loss: 0.42200496792793274, Val Acc: 0.8775\n","Epoch [131/500], Loss: 0.3587739169597626, Train Acc: 0.8774,train F1-score:0.8728 Val Loss: 0.4168430268764496, Val Acc: 0.8735\n","Epoch [132/500], Loss: 0.3546566069126129, Train Acc: 0.8767,train F1-score:0.8720 Val Loss: 0.4139985740184784, Val Acc: 0.8748\n","Epoch [133/500], Loss: 0.35372886061668396, Train Acc: 0.8771,train F1-score:0.8731 Val Loss: 0.41459792852401733, Val Acc: 0.8822\n","Epoch [134/500], Loss: 0.3493613004684448, Train Acc: 0.8813,train F1-score:0.8773 Val Loss: 0.415395587682724, Val Acc: 0.8788\n","Epoch [135/500], Loss: 0.362602174282074, Train Acc: 0.8764,train F1-score:0.8721 Val Loss: 0.41234150528907776, Val Acc: 0.8775\n","Epoch [136/500], Loss: 0.358497679233551, Train Acc: 0.8781,train F1-score:0.8743 Val Loss: 0.40941566228866577, Val Acc: 0.8775\n","Epoch [137/500], Loss: 0.35751044750213623, Train Acc: 0.8742,train F1-score:0.8698 Val Loss: 0.41512882709503174, Val Acc: 0.8755\n","Epoch [138/500], Loss: 0.34420713782310486, Train Acc: 0.8830,train F1-score:0.8789 Val Loss: 0.4194422960281372, Val Acc: 0.8742\n","Epoch [139/500], Loss: 0.3588573634624481, Train Acc: 0.8794,train F1-score:0.8750 Val Loss: 0.4214266836643219, Val Acc: 0.8748\n","Epoch [140/500], Loss: 0.3509334325790405, Train Acc: 0.8765,train F1-score:0.8719 Val Loss: 0.41971081495285034, Val Acc: 0.8748\n","Epoch [141/500], Loss: 0.35568296909332275, Train Acc: 0.8761,train F1-score:0.8717 Val Loss: 0.41654932498931885, Val Acc: 0.8768\n","Epoch [142/500], Loss: 0.3469950556755066, Train Acc: 0.8785,train F1-score:0.8741 Val Loss: 0.41237398982048035, Val Acc: 0.8748\n","Epoch [143/500], Loss: 0.3513246476650238, Train Acc: 0.8813,train F1-score:0.8777 Val Loss: 0.40940964221954346, Val Acc: 0.8768\n","Epoch [144/500], Loss: 0.34653571248054504, Train Acc: 0.8798,train F1-score:0.8758 Val Loss: 0.40907517075538635, Val Acc: 0.8782\n","Epoch [145/500], Loss: 0.34843024611473083, Train Acc: 0.8784,train F1-score:0.8744 Val Loss: 0.40991267561912537, Val Acc: 0.8802\n","Epoch [146/500], Loss: 0.3380069136619568, Train Acc: 0.8835,train F1-score:0.8800 Val Loss: 0.40802550315856934, Val Acc: 0.8782\n","Epoch [147/500], Loss: 0.3492237627506256, Train Acc: 0.8769,train F1-score:0.8729 Val Loss: 0.4127116799354553, Val Acc: 0.8748\n","Epoch [148/500], Loss: 0.3433374762535095, Train Acc: 0.8799,train F1-score:0.8749 Val Loss: 0.42401379346847534, Val Acc: 0.8768\n","Epoch [149/500], Loss: 0.3397316634654999, Train Acc: 0.8809,train F1-score:0.8767 Val Loss: 0.43012598156929016, Val Acc: 0.8782\n","Epoch [150/500], Loss: 0.3358289897441864, Train Acc: 0.8838,train F1-score:0.8802 Val Loss: 0.4337522089481354, Val Acc: 0.8768\n","Epoch [151/500], Loss: 0.339050829410553, Train Acc: 0.8838,train F1-score:0.8803 Val Loss: 0.4361675977706909, Val Acc: 0.8762\n","Epoch [152/500], Loss: 0.3378453254699707, Train Acc: 0.8854,train F1-score:0.8815 Val Loss: 0.4344892203807831, Val Acc: 0.8735\n","Epoch [153/500], Loss: 0.34448516368865967, Train Acc: 0.8801,train F1-score:0.8766 Val Loss: 0.4276842772960663, Val Acc: 0.8722\n","Epoch [154/500], Loss: 0.3332139551639557, Train Acc: 0.8818,train F1-score:0.8783 Val Loss: 0.4200725555419922, Val Acc: 0.8748\n","Epoch [155/500], Loss: 0.33480605483055115, Train Acc: 0.8813,train F1-score:0.8775 Val Loss: 0.41459333896636963, Val Acc: 0.8768\n","Epoch [156/500], Loss: 0.3355351984500885, Train Acc: 0.8806,train F1-score:0.8761 Val Loss: 0.4162036180496216, Val Acc: 0.8802\n","Epoch [157/500], Loss: 0.3301420211791992, Train Acc: 0.8847,train F1-score:0.8807 Val Loss: 0.4160133898258209, Val Acc: 0.8815\n","Epoch [158/500], Loss: 0.33714333176612854, Train Acc: 0.8833,train F1-score:0.8791 Val Loss: 0.4166263937950134, Val Acc: 0.8829\n","Epoch [159/500], Loss: 0.33050674200057983, Train Acc: 0.8850,train F1-score:0.8813 Val Loss: 0.4157906770706177, Val Acc: 0.8809\n","Epoch [160/500], Loss: 0.3291531503200531, Train Acc: 0.8847,train F1-score:0.8811 Val Loss: 0.41585540771484375, Val Acc: 0.8815\n","Epoch [161/500], Loss: 0.3328762650489807, Train Acc: 0.8816,train F1-score:0.8782 Val Loss: 0.4138365685939789, Val Acc: 0.8802\n","Epoch [162/500], Loss: 0.3308525085449219, Train Acc: 0.8852,train F1-score:0.8815 Val Loss: 0.41405677795410156, Val Acc: 0.8809\n","Epoch [163/500], Loss: 0.33121055364608765, Train Acc: 0.8859,train F1-score:0.8819 Val Loss: 0.413804292678833, Val Acc: 0.8829\n","Epoch [164/500], Loss: 0.3316907584667206, Train Acc: 0.8805,train F1-score:0.8768 Val Loss: 0.4145931899547577, Val Acc: 0.8855\n","Epoch [165/500], Loss: 0.3219638466835022, Train Acc: 0.8868,train F1-score:0.8834 Val Loss: 0.4171690046787262, Val Acc: 0.8809\n","Epoch [166/500], Loss: 0.3270489573478699, Train Acc: 0.8854,train F1-score:0.8822 Val Loss: 0.41989025473594666, Val Acc: 0.8835\n","Epoch [167/500], Loss: 0.33011430501937866, Train Acc: 0.8836,train F1-score:0.8799 Val Loss: 0.4202793836593628, Val Acc: 0.8835\n","Epoch [168/500], Loss: 0.32730042934417725, Train Acc: 0.8879,train F1-score:0.8841 Val Loss: 0.42047640681266785, Val Acc: 0.8842\n","Epoch [169/500], Loss: 0.32495763897895813, Train Acc: 0.8829,train F1-score:0.8788 Val Loss: 0.4196089506149292, Val Acc: 0.8849\n","Epoch [170/500], Loss: 0.32696157693862915, Train Acc: 0.8869,train F1-score:0.8829 Val Loss: 0.41523948311805725, Val Acc: 0.8869\n","Epoch [171/500], Loss: 0.32588446140289307, Train Acc: 0.8884,train F1-score:0.8841 Val Loss: 0.41127195954322815, Val Acc: 0.8855\n","Epoch [172/500], Loss: 0.3242446184158325, Train Acc: 0.8859,train F1-score:0.8820 Val Loss: 0.4086016118526459, Val Acc: 0.8835\n","Epoch [173/500], Loss: 0.3220815360546112, Train Acc: 0.8885,train F1-score:0.8849 Val Loss: 0.41270551085472107, Val Acc: 0.8855\n","Epoch [174/500], Loss: 0.32364821434020996, Train Acc: 0.8856,train F1-score:0.8822 Val Loss: 0.4141498804092407, Val Acc: 0.8862\n","Epoch [175/500], Loss: 0.326431542634964, Train Acc: 0.8859,train F1-score:0.8820 Val Loss: 0.4137237071990967, Val Acc: 0.8835\n","Epoch [176/500], Loss: 0.316047340631485, Train Acc: 0.8890,train F1-score:0.8855 Val Loss: 0.4164856970310211, Val Acc: 0.8862\n","Epoch [177/500], Loss: 0.31798142194747925, Train Acc: 0.8893,train F1-score:0.8855 Val Loss: 0.42095890641212463, Val Acc: 0.8842\n","Epoch [178/500], Loss: 0.31807997822761536, Train Acc: 0.8875,train F1-score:0.8839 Val Loss: 0.4228319525718689, Val Acc: 0.8842\n","Epoch [179/500], Loss: 0.3157211244106293, Train Acc: 0.8902,train F1-score:0.8872 Val Loss: 0.4260035753250122, Val Acc: 0.8829\n","Epoch [180/500], Loss: 0.3192473351955414, Train Acc: 0.8881,train F1-score:0.8849 Val Loss: 0.42556267976760864, Val Acc: 0.8842\n","Epoch [181/500], Loss: 0.3186374008655548, Train Acc: 0.8890,train F1-score:0.8854 Val Loss: 0.4251318871974945, Val Acc: 0.8869\n","Epoch [182/500], Loss: 0.3106623888015747, Train Acc: 0.8935,train F1-score:0.8904 Val Loss: 0.42414331436157227, Val Acc: 0.8855\n","Epoch [183/500], Loss: 0.3172135353088379, Train Acc: 0.8890,train F1-score:0.8856 Val Loss: 0.4181751310825348, Val Acc: 0.8822\n","Epoch [184/500], Loss: 0.31382983922958374, Train Acc: 0.8904,train F1-score:0.8873 Val Loss: 0.41558367013931274, Val Acc: 0.8842\n","Epoch [185/500], Loss: 0.3087323009967804, Train Acc: 0.8911,train F1-score:0.8881 Val Loss: 0.41541749238967896, Val Acc: 0.8862\n","Epoch [186/500], Loss: 0.315056711435318, Train Acc: 0.8911,train F1-score:0.8875 Val Loss: 0.41541966795921326, Val Acc: 0.8835\n","Epoch [187/500], Loss: 0.31516122817993164, Train Acc: 0.8926,train F1-score:0.8888 Val Loss: 0.4144658148288727, Val Acc: 0.8855\n","Epoch [188/500], Loss: 0.307041734457016, Train Acc: 0.8906,train F1-score:0.8865 Val Loss: 0.41237834095954895, Val Acc: 0.8876\n","Epoch [189/500], Loss: 0.3019844591617584, Train Acc: 0.8968,train F1-score:0.8941 Val Loss: 0.4109906554222107, Val Acc: 0.8909\n","Epoch [190/500], Loss: 0.3073333203792572, Train Acc: 0.8942,train F1-score:0.8914 Val Loss: 0.4065309166908264, Val Acc: 0.8902\n","Epoch [191/500], Loss: 0.30990901589393616, Train Acc: 0.8895,train F1-score:0.8860 Val Loss: 0.4079906940460205, Val Acc: 0.8876\n","Epoch [192/500], Loss: 0.3096455931663513, Train Acc: 0.8924,train F1-score:0.8892 Val Loss: 0.4158765971660614, Val Acc: 0.8882\n","Epoch [193/500], Loss: 0.30449017882347107, Train Acc: 0.8946,train F1-score:0.8916 Val Loss: 0.42341089248657227, Val Acc: 0.8855\n","Epoch [194/500], Loss: 0.3078218996524811, Train Acc: 0.8921,train F1-score:0.8893 Val Loss: 0.42566797137260437, Val Acc: 0.8855\n","Epoch [195/500], Loss: 0.31166574358940125, Train Acc: 0.8913,train F1-score:0.8881 Val Loss: 0.42123618721961975, Val Acc: 0.8829\n","Epoch [196/500], Loss: 0.3055320084095001, Train Acc: 0.8936,train F1-score:0.8904 Val Loss: 0.41496115922927856, Val Acc: 0.8842\n","Epoch [197/500], Loss: 0.3023374676704407, Train Acc: 0.8942,train F1-score:0.8917 Val Loss: 0.4093877673149109, Val Acc: 0.8889\n","Epoch [198/500], Loss: 0.3069877624511719, Train Acc: 0.8915,train F1-score:0.8879 Val Loss: 0.403722882270813, Val Acc: 0.8902\n","Epoch [199/500], Loss: 0.30577796697616577, Train Acc: 0.8948,train F1-score:0.8917 Val Loss: 0.40610435605049133, Val Acc: 0.8876\n","Epoch [200/500], Loss: 0.3058677911758423, Train Acc: 0.8899,train F1-score:0.8868 Val Loss: 0.4086027443408966, Val Acc: 0.8916\n","Epoch [201/500], Loss: 0.30165424942970276, Train Acc: 0.8927,train F1-score:0.8901 Val Loss: 0.40851086378097534, Val Acc: 0.8909\n","Epoch [202/500], Loss: 0.30056625604629517, Train Acc: 0.8956,train F1-score:0.8924 Val Loss: 0.40639644861221313, Val Acc: 0.8876\n","Epoch [203/500], Loss: 0.3122403919696808, Train Acc: 0.8972,train F1-score:0.8940 Val Loss: 0.40943267941474915, Val Acc: 0.8882\n","Epoch [204/500], Loss: 0.3059622049331665, Train Acc: 0.8915,train F1-score:0.8880 Val Loss: 0.40514281392097473, Val Acc: 0.8902\n","Epoch [205/500], Loss: 0.30472880601882935, Train Acc: 0.8966,train F1-score:0.8940 Val Loss: 0.39879345893859863, Val Acc: 0.8896\n","Epoch [206/500], Loss: 0.298390656709671, Train Acc: 0.8961,train F1-score:0.8937 Val Loss: 0.4010761082172394, Val Acc: 0.8889\n","Epoch [207/500], Loss: 0.2966054379940033, Train Acc: 0.8947,train F1-score:0.8919 Val Loss: 0.4096416234970093, Val Acc: 0.8889\n","Epoch [208/500], Loss: 0.29719915986061096, Train Acc: 0.8939,train F1-score:0.8904 Val Loss: 0.4252115786075592, Val Acc: 0.8896\n","Epoch [209/500], Loss: 0.3046089708805084, Train Acc: 0.8937,train F1-score:0.8901 Val Loss: 0.4299373924732208, Val Acc: 0.8889\n","Epoch [210/500], Loss: 0.2945493161678314, Train Acc: 0.8947,train F1-score:0.8918 Val Loss: 0.42511022090911865, Val Acc: 0.8902\n","Epoch [211/500], Loss: 0.2983315587043762, Train Acc: 0.8963,train F1-score:0.8934 Val Loss: 0.42710036039352417, Val Acc: 0.8896\n","Epoch [212/500], Loss: 0.292293518781662, Train Acc: 0.8975,train F1-score:0.8943 Val Loss: 0.4235615134239197, Val Acc: 0.8942\n","Epoch [213/500], Loss: 0.2951631247997284, Train Acc: 0.8932,train F1-score:0.8905 Val Loss: 0.4178078770637512, Val Acc: 0.8909\n","Epoch [214/500], Loss: 0.30156567692756653, Train Acc: 0.8954,train F1-score:0.8926 Val Loss: 0.41168007254600525, Val Acc: 0.8916\n","Epoch [215/500], Loss: 0.29678502678871155, Train Acc: 0.8971,train F1-score:0.8939 Val Loss: 0.41253527998924255, Val Acc: 0.8902\n","Epoch [216/500], Loss: 0.296904981136322, Train Acc: 0.8954,train F1-score:0.8924 Val Loss: 0.415616512298584, Val Acc: 0.8956\n","Epoch [217/500], Loss: 0.29697516560554504, Train Acc: 0.8983,train F1-score:0.8954 Val Loss: 0.40962082147598267, Val Acc: 0.8983\n","Epoch [218/500], Loss: 0.29382777214050293, Train Acc: 0.8982,train F1-score:0.8956 Val Loss: 0.40802842378616333, Val Acc: 0.8996\n","Epoch [219/500], Loss: 0.29257410764694214, Train Acc: 0.8959,train F1-score:0.8934 Val Loss: 0.40848636627197266, Val Acc: 0.8969\n","Epoch [220/500], Loss: 0.2867663502693176, Train Acc: 0.8988,train F1-score:0.8961 Val Loss: 0.41378307342529297, Val Acc: 0.8949\n","Epoch [221/500], Loss: 0.2946583330631256, Train Acc: 0.8991,train F1-score:0.8961 Val Loss: 0.4183861017227173, Val Acc: 0.8929\n","Epoch [222/500], Loss: 0.29410627484321594, Train Acc: 0.8983,train F1-score:0.8959 Val Loss: 0.42839521169662476, Val Acc: 0.8922\n","Epoch [223/500], Loss: 0.28874528408050537, Train Acc: 0.9020,train F1-score:0.8998 Val Loss: 0.4348616302013397, Val Acc: 0.8896\n","Epoch [224/500], Loss: 0.29779887199401855, Train Acc: 0.8931,train F1-score:0.8904 Val Loss: 0.43864861130714417, Val Acc: 0.8916\n","Epoch [225/500], Loss: 0.2884276509284973, Train Acc: 0.8995,train F1-score:0.8968 Val Loss: 0.4331192672252655, Val Acc: 0.8936\n","Epoch [226/500], Loss: 0.2909739911556244, Train Acc: 0.9004,train F1-score:0.8978 Val Loss: 0.4307311177253723, Val Acc: 0.8942\n","Epoch [227/500], Loss: 0.30646592378616333, Train Acc: 0.8961,train F1-score:0.8925 Val Loss: 0.4326092302799225, Val Acc: 0.8956\n","Epoch [228/500], Loss: 0.29353201389312744, Train Acc: 0.8950,train F1-score:0.8916 Val Loss: 0.4397762715816498, Val Acc: 0.8936\n","Epoch [229/500], Loss: 0.28821638226509094, Train Acc: 0.8981,train F1-score:0.8947 Val Loss: 0.432730108499527, Val Acc: 0.8922\n","Epoch [230/500], Loss: 0.28702831268310547, Train Acc: 0.9001,train F1-score:0.8973 Val Loss: 0.42277082800865173, Val Acc: 0.8956\n","Epoch [231/500], Loss: 0.29020941257476807, Train Acc: 0.9012,train F1-score:0.8991 Val Loss: 0.4131433367729187, Val Acc: 0.8963\n","Epoch [232/500], Loss: 0.29508018493652344, Train Acc: 0.8966,train F1-score:0.8938 Val Loss: 0.4084162712097168, Val Acc: 0.8942\n","Epoch [233/500], Loss: 0.28969404101371765, Train Acc: 0.9018,train F1-score:0.8990 Val Loss: 0.41525599360466003, Val Acc: 0.8949\n","Epoch [234/500], Loss: 0.2827484607696533, Train Acc: 0.8985,train F1-score:0.8959 Val Loss: 0.4190407991409302, Val Acc: 0.8942\n","Epoch [235/500], Loss: 0.2871825098991394, Train Acc: 0.8978,train F1-score:0.8950 Val Loss: 0.41803720593452454, Val Acc: 0.9003\n","Epoch [236/500], Loss: 0.2870092988014221, Train Acc: 0.9004,train F1-score:0.8972 Val Loss: 0.41557085514068604, Val Acc: 0.9003\n","Epoch [237/500], Loss: 0.287809818983078, Train Acc: 0.9001,train F1-score:0.8973 Val Loss: 0.4163125455379486, Val Acc: 0.8996\n","Epoch [238/500], Loss: 0.28511449694633484, Train Acc: 0.9027,train F1-score:0.9000 Val Loss: 0.4228927791118622, Val Acc: 0.8963\n","Epoch [239/500], Loss: 0.2889859676361084, Train Acc: 0.8988,train F1-score:0.8961 Val Loss: 0.42788806557655334, Val Acc: 0.8976\n","Epoch [240/500], Loss: 0.2810583710670471, Train Acc: 0.8998,train F1-score:0.8976 Val Loss: 0.42511171102523804, Val Acc: 0.8989\n","Epoch [241/500], Loss: 0.28494149446487427, Train Acc: 0.9025,train F1-score:0.8999 Val Loss: 0.4215930998325348, Val Acc: 0.9003\n","Epoch [242/500], Loss: 0.28378212451934814, Train Acc: 0.9016,train F1-score:0.8989 Val Loss: 0.4163067936897278, Val Acc: 0.8949\n","Epoch [243/500], Loss: 0.28060269355773926, Train Acc: 0.9025,train F1-score:0.8998 Val Loss: 0.41190460324287415, Val Acc: 0.8936\n","Epoch [244/500], Loss: 0.27953895926475525, Train Acc: 0.9017,train F1-score:0.8993 Val Loss: 0.4181181788444519, Val Acc: 0.8916\n","Epoch [245/500], Loss: 0.2857480049133301, Train Acc: 0.9011,train F1-score:0.8984 Val Loss: 0.4200673997402191, Val Acc: 0.8969\n","Epoch [246/500], Loss: 0.2884921729564667, Train Acc: 0.9017,train F1-score:0.8991 Val Loss: 0.4195736050605774, Val Acc: 0.8983\n","Epoch [247/500], Loss: 0.279315322637558, Train Acc: 0.9039,train F1-score:0.9014 Val Loss: 0.42148804664611816, Val Acc: 0.8983\n","Epoch [248/500], Loss: 0.281230092048645, Train Acc: 0.9017,train F1-score:0.8991 Val Loss: 0.4188149571418762, Val Acc: 0.8936\n","Epoch [249/500], Loss: 0.2795242369174957, Train Acc: 0.9039,train F1-score:0.9013 Val Loss: 0.41447749733924866, Val Acc: 0.8949\n","Epoch [250/500], Loss: 0.28650400042533875, Train Acc: 0.9019,train F1-score:0.8994 Val Loss: 0.41059282422065735, Val Acc: 0.8976\n","Epoch [251/500], Loss: 0.2832866609096527, Train Acc: 0.9031,train F1-score:0.9005 Val Loss: 0.4112392067909241, Val Acc: 0.8976\n","Epoch [252/500], Loss: 0.2761971950531006, Train Acc: 0.9035,train F1-score:0.9009 Val Loss: 0.4130578637123108, Val Acc: 0.8949\n","Epoch [253/500], Loss: 0.2755288779735565, Train Acc: 0.9014,train F1-score:0.8989 Val Loss: 0.4146386981010437, Val Acc: 0.8976\n","Epoch [254/500], Loss: 0.2775953412055969, Train Acc: 0.9022,train F1-score:0.8998 Val Loss: 0.4144781827926636, Val Acc: 0.8963\n","Epoch [255/500], Loss: 0.2728724181652069, Train Acc: 0.9061,train F1-score:0.9037 Val Loss: 0.4178240895271301, Val Acc: 0.8963\n","Epoch [256/500], Loss: 0.27619031071662903, Train Acc: 0.9070,train F1-score:0.9043 Val Loss: 0.41659465432167053, Val Acc: 0.8949\n","Epoch [257/500], Loss: 0.2746036946773529, Train Acc: 0.9032,train F1-score:0.9005 Val Loss: 0.4150700271129608, Val Acc: 0.8922\n","Epoch [258/500], Loss: 0.2769226133823395, Train Acc: 0.9025,train F1-score:0.9001 Val Loss: 0.41090288758277893, Val Acc: 0.8956\n","Epoch [259/500], Loss: 0.27739787101745605, Train Acc: 0.9005,train F1-score:0.8977 Val Loss: 0.40298283100128174, Val Acc: 0.8956\n","Epoch [260/500], Loss: 0.277446448802948, Train Acc: 0.9048,train F1-score:0.9021 Val Loss: 0.396345853805542, Val Acc: 0.8963\n","Epoch [261/500], Loss: 0.27410849928855896, Train Acc: 0.9029,train F1-score:0.9002 Val Loss: 0.39927637577056885, Val Acc: 0.8949\n","Epoch [262/500], Loss: 0.27370068430900574, Train Acc: 0.9057,train F1-score:0.9030 Val Loss: 0.40520158410072327, Val Acc: 0.8942\n","Epoch [263/500], Loss: 0.28039953112602234, Train Acc: 0.9010,train F1-score:0.8984 Val Loss: 0.4094759225845337, Val Acc: 0.8976\n","Epoch [264/500], Loss: 0.2783206105232239, Train Acc: 0.9024,train F1-score:0.9000 Val Loss: 0.41181299090385437, Val Acc: 0.8996\n","Epoch [265/500], Loss: 0.27325496077537537, Train Acc: 0.9076,train F1-score:0.9053 Val Loss: 0.4118703305721283, Val Acc: 0.8976\n","Epoch [266/500], Loss: 0.2722429931163788, Train Acc: 0.9047,train F1-score:0.9023 Val Loss: 0.40998485684394836, Val Acc: 0.8949\n","Epoch [267/500], Loss: 0.27008703351020813, Train Acc: 0.9058,train F1-score:0.9028 Val Loss: 0.4041675627231598, Val Acc: 0.8942\n","Epoch [268/500], Loss: 0.2766222357749939, Train Acc: 0.9045,train F1-score:0.9020 Val Loss: 0.40169164538383484, Val Acc: 0.8956\n","Epoch [269/500], Loss: 0.26874038577079773, Train Acc: 0.9053,train F1-score:0.9029 Val Loss: 0.39937031269073486, Val Acc: 0.8996\n","Epoch [270/500], Loss: 0.27531349658966064, Train Acc: 0.9014,train F1-score:0.8986 Val Loss: 0.40212440490722656, Val Acc: 0.8989\n","Epoch [271/500], Loss: 0.2717846632003784, Train Acc: 0.9055,train F1-score:0.9029 Val Loss: 0.4096393287181854, Val Acc: 0.8969\n","Epoch [272/500], Loss: 0.27742519974708557, Train Acc: 0.9045,train F1-score:0.9021 Val Loss: 0.41790473461151123, Val Acc: 0.8916\n","Epoch [273/500], Loss: 0.2784963846206665, Train Acc: 0.9068,train F1-score:0.9047 Val Loss: 0.4159155786037445, Val Acc: 0.8956\n","Epoch [274/500], Loss: 0.2734762728214264, Train Acc: 0.9065,train F1-score:0.9038 Val Loss: 0.4134739637374878, Val Acc: 0.8956\n","Epoch [275/500], Loss: 0.26686573028564453, Train Acc: 0.9053,train F1-score:0.9030 Val Loss: 0.41550707817077637, Val Acc: 0.8963\n","Epoch [276/500], Loss: 0.272471159696579, Train Acc: 0.9049,train F1-score:0.9023 Val Loss: 0.4135899841785431, Val Acc: 0.8942\n","Epoch [277/500], Loss: 0.26673993468284607, Train Acc: 0.9069,train F1-score:0.9040 Val Loss: 0.409160315990448, Val Acc: 0.8936\n","Epoch [278/500], Loss: 0.27193188667297363, Train Acc: 0.9088,train F1-score:0.9061 Val Loss: 0.4089101254940033, Val Acc: 0.8963\n","Epoch [279/500], Loss: 0.27028244733810425, Train Acc: 0.9058,train F1-score:0.9029 Val Loss: 0.40682873129844666, Val Acc: 0.8969\n","Epoch [280/500], Loss: 0.26440638303756714, Train Acc: 0.9086,train F1-score:0.9064 Val Loss: 0.4058874249458313, Val Acc: 0.8956\n","Epoch [281/500], Loss: 0.26357486844062805, Train Acc: 0.9083,train F1-score:0.9060 Val Loss: 0.41247716546058655, Val Acc: 0.8969\n","Epoch [282/500], Loss: 0.2698920965194702, Train Acc: 0.9056,train F1-score:0.9034 Val Loss: 0.41517117619514465, Val Acc: 0.8976\n","Epoch [283/500], Loss: 0.2668974995613098, Train Acc: 0.9093,train F1-score:0.9072 Val Loss: 0.41846317052841187, Val Acc: 0.8936\n","Epoch [284/500], Loss: 0.27023574709892273, Train Acc: 0.9076,train F1-score:0.9049 Val Loss: 0.42263615131378174, Val Acc: 0.8916\n","Epoch [285/500], Loss: 0.26597580313682556, Train Acc: 0.9035,train F1-score:0.9006 Val Loss: 0.4347965717315674, Val Acc: 0.8942\n","Epoch [286/500], Loss: 0.2567874491214752, Train Acc: 0.9088,train F1-score:0.9070 Val Loss: 0.44589829444885254, Val Acc: 0.8922\n","Epoch [287/500], Loss: 0.2687174677848816, Train Acc: 0.9040,train F1-score:0.9014 Val Loss: 0.4508247971534729, Val Acc: 0.8936\n","Epoch [288/500], Loss: 0.2673793435096741, Train Acc: 0.9087,train F1-score:0.9060 Val Loss: 0.4496537148952484, Val Acc: 0.8949\n","Epoch [289/500], Loss: 0.2617780268192291, Train Acc: 0.9065,train F1-score:0.9040 Val Loss: 0.44154539704322815, Val Acc: 0.8956\n","Epoch [290/500], Loss: 0.2641488313674927, Train Acc: 0.9075,train F1-score:0.9053 Val Loss: 0.4313964545726776, Val Acc: 0.8976\n","Epoch [291/500], Loss: 0.2631379961967468, Train Acc: 0.9074,train F1-score:0.9051 Val Loss: 0.42614248394966125, Val Acc: 0.8956\n","Epoch [292/500], Loss: 0.26152777671813965, Train Acc: 0.9091,train F1-score:0.9067 Val Loss: 0.428474098443985, Val Acc: 0.8942\n","Epoch [293/500], Loss: 0.2640807628631592, Train Acc: 0.9107,train F1-score:0.9083 Val Loss: 0.4202878475189209, Val Acc: 0.8949\n","Epoch [294/500], Loss: 0.2641073763370514, Train Acc: 0.9061,train F1-score:0.9034 Val Loss: 0.4214228689670563, Val Acc: 0.8983\n","Epoch [295/500], Loss: 0.26920726895332336, Train Acc: 0.9055,train F1-score:0.9033 Val Loss: 0.4294673800468445, Val Acc: 0.8989\n","Epoch [296/500], Loss: 0.26034268736839294, Train Acc: 0.9091,train F1-score:0.9073 Val Loss: 0.4462401270866394, Val Acc: 0.8963\n","Epoch [297/500], Loss: 0.26346081495285034, Train Acc: 0.9093,train F1-score:0.9070 Val Loss: 0.4488506615161896, Val Acc: 0.8956\n","Epoch [298/500], Loss: 0.26209115982055664, Train Acc: 0.9107,train F1-score:0.9081 Val Loss: 0.440193772315979, Val Acc: 0.8963\n","Epoch [299/500], Loss: 0.27195966243743896, Train Acc: 0.9094,train F1-score:0.9073 Val Loss: 0.4257822632789612, Val Acc: 0.8996\n","Epoch [300/500], Loss: 0.2613285183906555, Train Acc: 0.9095,train F1-score:0.9074 Val Loss: 0.42422693967819214, Val Acc: 0.8976\n","Epoch [301/500], Loss: 0.2706751525402069, Train Acc: 0.9070,train F1-score:0.9049 Val Loss: 0.4205935597419739, Val Acc: 0.8976\n","Epoch [302/500], Loss: 0.261388897895813, Train Acc: 0.9081,train F1-score:0.9056 Val Loss: 0.4166443645954132, Val Acc: 0.9016\n","Epoch [303/500], Loss: 0.26196449995040894, Train Acc: 0.9060,train F1-score:0.9041 Val Loss: 0.4136151671409607, Val Acc: 0.8996\n","Epoch [304/500], Loss: 0.25608396530151367, Train Acc: 0.9081,train F1-score:0.9064 Val Loss: 0.4131377339363098, Val Acc: 0.8949\n","Epoch [305/500], Loss: 0.2610263228416443, Train Acc: 0.9087,train F1-score:0.9066 Val Loss: 0.41883113980293274, Val Acc: 0.8963\n","Epoch [306/500], Loss: 0.26163220405578613, Train Acc: 0.9065,train F1-score:0.9033 Val Loss: 0.4191931188106537, Val Acc: 0.8989\n","Epoch [307/500], Loss: 0.2625783681869507, Train Acc: 0.9084,train F1-score:0.9057 Val Loss: 0.422741174697876, Val Acc: 0.9009\n","Epoch [308/500], Loss: 0.2637367248535156, Train Acc: 0.9072,train F1-score:0.9051 Val Loss: 0.42178860306739807, Val Acc: 0.9003\n","Epoch [309/500], Loss: 0.2613460421562195, Train Acc: 0.9109,train F1-score:0.9090 Val Loss: 0.42224276065826416, Val Acc: 0.8976\n","Epoch [310/500], Loss: 0.259391188621521, Train Acc: 0.9106,train F1-score:0.9084 Val Loss: 0.42149868607521057, Val Acc: 0.9016\n","Epoch [311/500], Loss: 0.2602732479572296, Train Acc: 0.9076,train F1-score:0.9051 Val Loss: 0.42635226249694824, Val Acc: 0.9023\n","Epoch [312/500], Loss: 0.2613677382469177, Train Acc: 0.9092,train F1-score:0.9071 Val Loss: 0.43481558561325073, Val Acc: 0.9003\n","Epoch [313/500], Loss: 0.2538323700428009, Train Acc: 0.9114,train F1-score:0.9090 Val Loss: 0.4396982192993164, Val Acc: 0.8989\n","Epoch [314/500], Loss: 0.25725433230400085, Train Acc: 0.9078,train F1-score:0.9054 Val Loss: 0.4402470290660858, Val Acc: 0.8983\n","Epoch [315/500], Loss: 0.2644478976726532, Train Acc: 0.9057,train F1-score:0.9034 Val Loss: 0.43310073018074036, Val Acc: 0.9036\n","Epoch [316/500], Loss: 0.25938352942466736, Train Acc: 0.9091,train F1-score:0.9073 Val Loss: 0.43204984068870544, Val Acc: 0.9036\n","Epoch [317/500], Loss: 0.2579713463783264, Train Acc: 0.9129,train F1-score:0.9109 Val Loss: 0.43098941445350647, Val Acc: 0.9003\n","Epoch [318/500], Loss: 0.25567975640296936, Train Acc: 0.9095,train F1-score:0.9070 Val Loss: 0.42533472180366516, Val Acc: 0.8969\n","Epoch [319/500], Loss: 0.25525596737861633, Train Acc: 0.9091,train F1-score:0.9068 Val Loss: 0.4215851426124573, Val Acc: 0.8929\n","Epoch [320/500], Loss: 0.2553962767124176, Train Acc: 0.9122,train F1-score:0.9100 Val Loss: 0.42198508977890015, Val Acc: 0.8983\n","Epoch [321/500], Loss: 0.24863627552986145, Train Acc: 0.9143,train F1-score:0.9125 Val Loss: 0.42625775933265686, Val Acc: 0.9009\n","Epoch [322/500], Loss: 0.25477710366249084, Train Acc: 0.9132,train F1-score:0.9115 Val Loss: 0.4279279410839081, Val Acc: 0.9009\n","Epoch [323/500], Loss: 0.25179922580718994, Train Acc: 0.9134,train F1-score:0.9113 Val Loss: 0.420980304479599, Val Acc: 0.8983\n","Epoch [324/500], Loss: 0.24954630434513092, Train Acc: 0.9091,train F1-score:0.9073 Val Loss: 0.4262669086456299, Val Acc: 0.8969\n","Epoch [325/500], Loss: 0.2526940405368805, Train Acc: 0.9091,train F1-score:0.9069 Val Loss: 0.43153074383735657, Val Acc: 0.8976\n","Epoch [326/500], Loss: 0.25893133878707886, Train Acc: 0.9083,train F1-score:0.9059 Val Loss: 0.43594345450401306, Val Acc: 0.8989\n","Epoch [327/500], Loss: 0.253659725189209, Train Acc: 0.9124,train F1-score:0.9103 Val Loss: 0.4338516294956207, Val Acc: 0.8976\n","Epoch [328/500], Loss: 0.24940647184848785, Train Acc: 0.9099,train F1-score:0.9080 Val Loss: 0.4339538812637329, Val Acc: 0.8956\n","Epoch [329/500], Loss: 0.25277549028396606, Train Acc: 0.9127,train F1-score:0.9109 Val Loss: 0.4358222782611847, Val Acc: 0.8963\n","Epoch [330/500], Loss: 0.249384805560112, Train Acc: 0.9132,train F1-score:0.9111 Val Loss: 0.4396708607673645, Val Acc: 0.8976\n","Epoch [331/500], Loss: 0.2549520432949066, Train Acc: 0.9059,train F1-score:0.9035 Val Loss: 0.4367334246635437, Val Acc: 0.9009\n","Epoch [332/500], Loss: 0.25803428888320923, Train Acc: 0.9109,train F1-score:0.9091 Val Loss: 0.43196994066238403, Val Acc: 0.9003\n","Epoch [333/500], Loss: 0.2514476180076599, Train Acc: 0.9115,train F1-score:0.9095 Val Loss: 0.4357779026031494, Val Acc: 0.8989\n","Epoch [334/500], Loss: 0.24880194664001465, Train Acc: 0.9153,train F1-score:0.9132 Val Loss: 0.4444178342819214, Val Acc: 0.8949\n","Epoch [335/500], Loss: 0.2532443702220917, Train Acc: 0.9105,train F1-score:0.9078 Val Loss: 0.44837284088134766, Val Acc: 0.8949\n","Epoch [336/500], Loss: 0.25044333934783936, Train Acc: 0.9105,train F1-score:0.9079 Val Loss: 0.4418541193008423, Val Acc: 0.8949\n","Epoch [337/500], Loss: 0.2560350000858307, Train Acc: 0.9082,train F1-score:0.9058 Val Loss: 0.42667147517204285, Val Acc: 0.8956\n","Epoch [338/500], Loss: 0.25029242038726807, Train Acc: 0.9125,train F1-score:0.9106 Val Loss: 0.42074376344680786, Val Acc: 0.8969\n","Epoch [339/500], Loss: 0.25093552470207214, Train Acc: 0.9127,train F1-score:0.9110 Val Loss: 0.4163110852241516, Val Acc: 0.8956\n","Epoch [340/500], Loss: 0.2467348724603653, Train Acc: 0.9132,train F1-score:0.9111 Val Loss: 0.417366087436676, Val Acc: 0.8963\n","Epoch [341/500], Loss: 0.2473030388355255, Train Acc: 0.9147,train F1-score:0.9128 Val Loss: 0.4265885055065155, Val Acc: 0.8949\n","Epoch [342/500], Loss: 0.24483655393123627, Train Acc: 0.9160,train F1-score:0.9141 Val Loss: 0.4332371950149536, Val Acc: 0.8949\n","Epoch [343/500], Loss: 0.2501573860645294, Train Acc: 0.9131,train F1-score:0.9113 Val Loss: 0.43908753991127014, Val Acc: 0.8956\n","Epoch [344/500], Loss: 0.24600712954998016, Train Acc: 0.9151,train F1-score:0.9131 Val Loss: 0.44833073019981384, Val Acc: 0.8976\n","Epoch [345/500], Loss: 0.2386385202407837, Train Acc: 0.9153,train F1-score:0.9135 Val Loss: 0.4437304735183716, Val Acc: 0.8996\n","Epoch [346/500], Loss: 0.24424335360527039, Train Acc: 0.9150,train F1-score:0.9129 Val Loss: 0.4387247562408447, Val Acc: 0.8976\n","Epoch [347/500], Loss: 0.2447655349969864, Train Acc: 0.9137,train F1-score:0.9117 Val Loss: 0.4301689565181732, Val Acc: 0.8969\n","Epoch [348/500], Loss: 0.24411852657794952, Train Acc: 0.9137,train F1-score:0.9121 Val Loss: 0.4296816289424896, Val Acc: 0.8969\n","Epoch [349/500], Loss: 0.24730437994003296, Train Acc: 0.9137,train F1-score:0.9116 Val Loss: 0.42991161346435547, Val Acc: 0.8949\n","Epoch [350/500], Loss: 0.2431870847940445, Train Acc: 0.9162,train F1-score:0.9141 Val Loss: 0.4346177875995636, Val Acc: 0.8956\n","Epoch [351/500], Loss: 0.244126096367836, Train Acc: 0.9166,train F1-score:0.9146 Val Loss: 0.44073686003685, Val Acc: 0.8976\n","Epoch [352/500], Loss: 0.24438484013080597, Train Acc: 0.9149,train F1-score:0.9130 Val Loss: 0.44448336958885193, Val Acc: 0.9016\n","Epoch [353/500], Loss: 0.24239180982112885, Train Acc: 0.9128,train F1-score:0.9111 Val Loss: 0.44209787249565125, Val Acc: 0.9029\n","Epoch [354/500], Loss: 0.24231694638729095, Train Acc: 0.9193,train F1-score:0.9173 Val Loss: 0.44631969928741455, Val Acc: 0.9016\n","Epoch [355/500], Loss: 0.2374913990497589, Train Acc: 0.9153,train F1-score:0.9129 Val Loss: 0.4433889091014862, Val Acc: 0.9036\n","Epoch [356/500], Loss: 0.24751906096935272, Train Acc: 0.9138,train F1-score:0.9121 Val Loss: 0.44250497221946716, Val Acc: 0.9056\n","Epoch [357/500], Loss: 0.2366148680448532, Train Acc: 0.9122,train F1-score:0.9105 Val Loss: 0.44330570101737976, Val Acc: 0.8983\n","Epoch [358/500], Loss: 0.24866019189357758, Train Acc: 0.9113,train F1-score:0.9095 Val Loss: 0.441518098115921, Val Acc: 0.8989\n","Epoch [359/500], Loss: 0.24957157671451569, Train Acc: 0.9119,train F1-score:0.9098 Val Loss: 0.4360411763191223, Val Acc: 0.8989\n","Early stopping at epoch 359\n","Test Loss: 0.4448968768119812, Test Accuracy: 0.896112600536193\n","Precision: 0.8918, Recall: 0.8961, F1-score: 0.8904\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import SAGEConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n","        super(GraphSAGE, self).__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.convs = torch.nn.ModuleList([SAGEConv(hidden_channels, hidden_channels) for _ in range(num_layers - 2)])\n","        self.conv2 = SAGEConv(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        for conv in self.convs:\n","            x = F.relu(conv(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=-1)\n","# Initialize the GraphSAGE model\n","model = GraphSAGE(in_channels=X_train.shape[1], hidden_channels=128, out_channels=13,num_layers=3)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Convert data to appropriate format\n","edge_index_train = edge_index_train.to(device)\n","edge_index_val = edge_index_val.to(device)\n","edge_index_test = edge_index_test.to(device)\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model3.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer3/train_f1_scores_GS.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer3/train_loss_GS.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer3/epochs_GS.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model3.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/layer3/f1_score_GS.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2fu1YXwgmHJ","executionInfo":{"status":"ok","timestamp":1714455822475,"user_tz":-330,"elapsed":29737,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"c08133f9-c4ea-4363-feb1-3d988c7a6b10"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.62652325630188, Train Acc: 0.0355,train F1-score:0.0289 Val Loss: 2.151719331741333, Val Acc: 0.6700\n","Epoch [2/500], Loss: 2.122615337371826, Train Acc: 0.6061,train F1-score:0.5161 Val Loss: 1.5809277296066284, Val Acc: 0.6787\n","Epoch [3/500], Loss: 1.6516495943069458, Train Acc: 0.6429,train F1-score:0.5086 Val Loss: 1.182791829109192, Val Acc: 0.6787\n","Epoch [4/500], Loss: 1.381618618965149, Train Acc: 0.6439,train F1-score:0.5080 Val Loss: 1.059310793876648, Val Acc: 0.6760\n","Epoch [5/500], Loss: 1.2540559768676758, Train Acc: 0.6197,train F1-score:0.5310 Val Loss: 1.0099374055862427, Val Acc: 0.6934\n","Epoch [6/500], Loss: 1.1960561275482178, Train Acc: 0.6014,train F1-score:0.5458 Val Loss: 0.9757529497146606, Val Acc: 0.6948\n","Epoch [7/500], Loss: 1.1229907274246216, Train Acc: 0.6476,train F1-score:0.5687 Val Loss: 0.9263303279876709, Val Acc: 0.7182\n","Epoch [8/500], Loss: 1.0631680488586426, Train Acc: 0.6635,train F1-score:0.5872 Val Loss: 0.8725459575653076, Val Acc: 0.7336\n","Epoch [9/500], Loss: 0.9924464225769043, Train Acc: 0.6613,train F1-score:0.6188 Val Loss: 0.8846814632415771, Val Acc: 0.7182\n","Epoch [10/500], Loss: 0.9833457469940186, Train Acc: 0.6516,train F1-score:0.6271 Val Loss: 0.8474426865577698, Val Acc: 0.7329\n","Epoch [11/500], Loss: 0.9431874752044678, Train Acc: 0.6828,train F1-score:0.6365 Val Loss: 0.8124158382415771, Val Acc: 0.7403\n","Epoch [12/500], Loss: 0.9083820581436157, Train Acc: 0.6917,train F1-score:0.6278 Val Loss: 0.7816012501716614, Val Acc: 0.7523\n","Epoch [13/500], Loss: 0.8723062872886658, Train Acc: 0.7064,train F1-score:0.6409 Val Loss: 0.7588505148887634, Val Acc: 0.7517\n","Epoch [14/500], Loss: 0.8501299023628235, Train Acc: 0.7083,train F1-score:0.6486 Val Loss: 0.7452660799026489, Val Acc: 0.7564\n","Epoch [15/500], Loss: 0.8434738516807556, Train Acc: 0.7066,train F1-score:0.6529 Val Loss: 0.7263306379318237, Val Acc: 0.7597\n","Epoch [16/500], Loss: 0.820248007774353, Train Acc: 0.7163,train F1-score:0.6600 Val Loss: 0.7137736082077026, Val Acc: 0.7604\n","Epoch [17/500], Loss: 0.7977069616317749, Train Acc: 0.7256,train F1-score:0.6689 Val Loss: 0.702570378780365, Val Acc: 0.7644\n","Epoch [18/500], Loss: 0.7818077206611633, Train Acc: 0.7273,train F1-score:0.6720 Val Loss: 0.6928368806838989, Val Acc: 0.7738\n","Epoch [19/500], Loss: 0.7547289729118347, Train Acc: 0.7407,train F1-score:0.6933 Val Loss: 0.6909112930297852, Val Acc: 0.7751\n","Epoch [20/500], Loss: 0.740721583366394, Train Acc: 0.7444,train F1-score:0.7020 Val Loss: 0.6793908476829529, Val Acc: 0.7805\n","Epoch [21/500], Loss: 0.7252718210220337, Train Acc: 0.7523,train F1-score:0.7134 Val Loss: 0.6633298397064209, Val Acc: 0.7871\n","Epoch [22/500], Loss: 0.7013179063796997, Train Acc: 0.7553,train F1-score:0.7157 Val Loss: 0.6510701179504395, Val Acc: 0.7878\n","Epoch [23/500], Loss: 0.6898103356361389, Train Acc: 0.7590,train F1-score:0.7199 Val Loss: 0.6376520395278931, Val Acc: 0.7871\n","Epoch [24/500], Loss: 0.6704978346824646, Train Acc: 0.7585,train F1-score:0.7214 Val Loss: 0.6227116584777832, Val Acc: 0.7932\n","Epoch [25/500], Loss: 0.6592283248901367, Train Acc: 0.7641,train F1-score:0.7319 Val Loss: 0.6109992861747742, Val Acc: 0.7959\n","Epoch [26/500], Loss: 0.6413401365280151, Train Acc: 0.7677,train F1-score:0.7372 Val Loss: 0.6023848056793213, Val Acc: 0.8005\n","Epoch [27/500], Loss: 0.6230540871620178, Train Acc: 0.7745,train F1-score:0.7468 Val Loss: 0.5957242250442505, Val Acc: 0.8005\n","Epoch [28/500], Loss: 0.6132901906967163, Train Acc: 0.7784,train F1-score:0.7539 Val Loss: 0.5872435569763184, Val Acc: 0.8059\n","Epoch [29/500], Loss: 0.5986347198486328, Train Acc: 0.7842,train F1-score:0.7626 Val Loss: 0.5830702185630798, Val Acc: 0.8072\n","Epoch [30/500], Loss: 0.5914821028709412, Train Acc: 0.7889,train F1-score:0.7697 Val Loss: 0.5754190683364868, Val Acc: 0.8126\n","Epoch [31/500], Loss: 0.5794126987457275, Train Acc: 0.7942,train F1-score:0.7775 Val Loss: 0.5683590769767761, Val Acc: 0.8139\n","Epoch [32/500], Loss: 0.5675064325332642, Train Acc: 0.8005,train F1-score:0.7852 Val Loss: 0.561786949634552, Val Acc: 0.8199\n","Epoch [33/500], Loss: 0.5569151639938354, Train Acc: 0.8094,train F1-score:0.7961 Val Loss: 0.5543320178985596, Val Acc: 0.8246\n","Epoch [34/500], Loss: 0.5453892350196838, Train Acc: 0.8112,train F1-score:0.7985 Val Loss: 0.55076664686203, Val Acc: 0.8307\n","Epoch [35/500], Loss: 0.5296596884727478, Train Acc: 0.8185,train F1-score:0.8060 Val Loss: 0.5500696301460266, Val Acc: 0.8313\n","Epoch [36/500], Loss: 0.5238714218139648, Train Acc: 0.8245,train F1-score:0.8137 Val Loss: 0.5425487160682678, Val Acc: 0.8387\n","Epoch [37/500], Loss: 0.5134273767471313, Train Acc: 0.8274,train F1-score:0.8172 Val Loss: 0.5328285098075867, Val Acc: 0.8327\n","Epoch [38/500], Loss: 0.4996674656867981, Train Acc: 0.8310,train F1-score:0.8211 Val Loss: 0.5234002470970154, Val Acc: 0.8387\n","Epoch [39/500], Loss: 0.49194443225860596, Train Acc: 0.8335,train F1-score:0.8237 Val Loss: 0.5182744264602661, Val Acc: 0.8380\n","Epoch [40/500], Loss: 0.48218584060668945, Train Acc: 0.8386,train F1-score:0.8302 Val Loss: 0.5145002007484436, Val Acc: 0.8353\n","Epoch [41/500], Loss: 0.47798439860343933, Train Acc: 0.8390,train F1-score:0.8312 Val Loss: 0.5100809931755066, Val Acc: 0.8387\n","Epoch [42/500], Loss: 0.4652585983276367, Train Acc: 0.8422,train F1-score:0.8343 Val Loss: 0.5060358047485352, Val Acc: 0.8394\n","Epoch [43/500], Loss: 0.4579976797103882, Train Acc: 0.8465,train F1-score:0.8386 Val Loss: 0.5005351305007935, Val Acc: 0.8427\n","Epoch [44/500], Loss: 0.4545036554336548, Train Acc: 0.8482,train F1-score:0.8407 Val Loss: 0.4955664575099945, Val Acc: 0.8474\n","Epoch [45/500], Loss: 0.4420636296272278, Train Acc: 0.8516,train F1-score:0.8444 Val Loss: 0.49405384063720703, Val Acc: 0.8481\n","Epoch [46/500], Loss: 0.4345107078552246, Train Acc: 0.8545,train F1-score:0.8470 Val Loss: 0.4943309724330902, Val Acc: 0.8514\n","Epoch [47/500], Loss: 0.4252817928791046, Train Acc: 0.8576,train F1-score:0.8503 Val Loss: 0.49225813150405884, Val Acc: 0.8507\n","Epoch [48/500], Loss: 0.425040602684021, Train Acc: 0.8555,train F1-score:0.8487 Val Loss: 0.4861599802970886, Val Acc: 0.8474\n","Epoch [49/500], Loss: 0.4148474335670471, Train Acc: 0.8584,train F1-score:0.8512 Val Loss: 0.4820334315299988, Val Acc: 0.8561\n","Epoch [50/500], Loss: 0.4106808304786682, Train Acc: 0.8596,train F1-score:0.8534 Val Loss: 0.4804757237434387, Val Acc: 0.8568\n","Epoch [51/500], Loss: 0.39930832386016846, Train Acc: 0.8673,train F1-score:0.8613 Val Loss: 0.4819953143596649, Val Acc: 0.8527\n","Epoch [52/500], Loss: 0.3970910310745239, Train Acc: 0.8642,train F1-score:0.8580 Val Loss: 0.48408448696136475, Val Acc: 0.8554\n","Epoch [53/500], Loss: 0.3890226483345032, Train Acc: 0.8674,train F1-score:0.8609 Val Loss: 0.4814596176147461, Val Acc: 0.8594\n","Epoch [54/500], Loss: 0.3841305673122406, Train Acc: 0.8686,train F1-score:0.8633 Val Loss: 0.4766319990158081, Val Acc: 0.8594\n","Epoch [55/500], Loss: 0.37642067670822144, Train Acc: 0.8700,train F1-score:0.8639 Val Loss: 0.4741932451725006, Val Acc: 0.8601\n","Epoch [56/500], Loss: 0.3743792474269867, Train Acc: 0.8724,train F1-score:0.8665 Val Loss: 0.4715076982975006, Val Acc: 0.8621\n","Epoch [57/500], Loss: 0.365093857049942, Train Acc: 0.8750,train F1-score:0.8695 Val Loss: 0.47620445489883423, Val Acc: 0.8668\n","Epoch [58/500], Loss: 0.3653661012649536, Train Acc: 0.8742,train F1-score:0.8685 Val Loss: 0.481956422328949, Val Acc: 0.8681\n","Epoch [59/500], Loss: 0.3588283658027649, Train Acc: 0.8777,train F1-score:0.8727 Val Loss: 0.4807509779930115, Val Acc: 0.8655\n","Epoch [60/500], Loss: 0.3566974699497223, Train Acc: 0.8767,train F1-score:0.8709 Val Loss: 0.4810945987701416, Val Acc: 0.8655\n","Epoch [61/500], Loss: 0.35313180088996887, Train Acc: 0.8807,train F1-score:0.8767 Val Loss: 0.47828027606010437, Val Acc: 0.8688\n","Epoch [62/500], Loss: 0.3434813618659973, Train Acc: 0.8829,train F1-score:0.8781 Val Loss: 0.47988685965538025, Val Acc: 0.8695\n","Epoch [63/500], Loss: 0.34078818559646606, Train Acc: 0.8825,train F1-score:0.8776 Val Loss: 0.47606518864631653, Val Acc: 0.8708\n","Epoch [64/500], Loss: 0.33648955821990967, Train Acc: 0.8841,train F1-score:0.8802 Val Loss: 0.47875675559043884, Val Acc: 0.8688\n","Epoch [65/500], Loss: 0.33528050780296326, Train Acc: 0.8851,train F1-score:0.8796 Val Loss: 0.47834548354148865, Val Acc: 0.8701\n","Epoch [66/500], Loss: 0.3229905664920807, Train Acc: 0.8872,train F1-score:0.8826 Val Loss: 0.48002544045448303, Val Acc: 0.8688\n","Epoch [67/500], Loss: 0.3291034400463104, Train Acc: 0.8847,train F1-score:0.8796 Val Loss: 0.4786520302295685, Val Acc: 0.8728\n","Epoch [68/500], Loss: 0.32097163796424866, Train Acc: 0.8890,train F1-score:0.8842 Val Loss: 0.4867513179779053, Val Acc: 0.8681\n","Epoch [69/500], Loss: 0.3227609097957611, Train Acc: 0.8880,train F1-score:0.8833 Val Loss: 0.4867088794708252, Val Acc: 0.8775\n","Epoch [70/500], Loss: 0.31961995363235474, Train Acc: 0.8911,train F1-score:0.8867 Val Loss: 0.4942612648010254, Val Acc: 0.8722\n","Epoch [71/500], Loss: 0.3126436769962311, Train Acc: 0.8910,train F1-score:0.8865 Val Loss: 0.49842244386672974, Val Acc: 0.8742\n","Epoch [72/500], Loss: 0.3118113577365875, Train Acc: 0.8952,train F1-score:0.8917 Val Loss: 0.5019814372062683, Val Acc: 0.8695\n","Epoch [73/500], Loss: 0.30939993262290955, Train Acc: 0.8925,train F1-score:0.8867 Val Loss: 0.5051979422569275, Val Acc: 0.8722\n","Epoch [74/500], Loss: 0.32063114643096924, Train Acc: 0.8914,train F1-score:0.8898 Val Loss: 0.5023501515388489, Val Acc: 0.8728\n","Epoch [75/500], Loss: 0.3120100498199463, Train Acc: 0.8914,train F1-score:0.8855 Val Loss: 0.4958232045173645, Val Acc: 0.8755\n","Epoch [76/500], Loss: 0.29504331946372986, Train Acc: 0.8981,train F1-score:0.8944 Val Loss: 0.4970547556877136, Val Acc: 0.8722\n","Epoch [77/500], Loss: 0.30159157514572144, Train Acc: 0.9001,train F1-score:0.8974 Val Loss: 0.4981348216533661, Val Acc: 0.8701\n","Epoch [78/500], Loss: 0.30083805322647095, Train Acc: 0.8912,train F1-score:0.8858 Val Loss: 0.5069459080696106, Val Acc: 0.8788\n","Epoch [79/500], Loss: 0.29561322927474976, Train Acc: 0.9033,train F1-score:0.9003 Val Loss: 0.49506738781929016, Val Acc: 0.8748\n","Epoch [80/500], Loss: 0.2926490604877472, Train Acc: 0.9009,train F1-score:0.8980 Val Loss: 0.49516910314559937, Val Acc: 0.8755\n","Epoch [81/500], Loss: 0.29209834337234497, Train Acc: 0.8987,train F1-score:0.8940 Val Loss: 0.4910590350627899, Val Acc: 0.8748\n","Epoch [82/500], Loss: 0.2905309796333313, Train Acc: 0.9005,train F1-score:0.8984 Val Loss: 0.5047830939292908, Val Acc: 0.8708\n","Epoch [83/500], Loss: 0.29322314262390137, Train Acc: 0.8993,train F1-score:0.8955 Val Loss: 0.4919092357158661, Val Acc: 0.8788\n","Epoch [84/500], Loss: 0.2816832959651947, Train Acc: 0.9024,train F1-score:0.8989 Val Loss: 0.49418118596076965, Val Acc: 0.8809\n","Epoch [85/500], Loss: 0.2696904242038727, Train Acc: 0.9076,train F1-score:0.9048 Val Loss: 0.5173877477645874, Val Acc: 0.8728\n","Epoch [86/500], Loss: 0.2767949104309082, Train Acc: 0.9028,train F1-score:0.8999 Val Loss: 0.5010640025138855, Val Acc: 0.8815\n","Epoch [87/500], Loss: 0.2696036994457245, Train Acc: 0.9085,train F1-score:0.9051 Val Loss: 0.49163705110549927, Val Acc: 0.8809\n","Epoch [88/500], Loss: 0.2661394476890564, Train Acc: 0.9071,train F1-score:0.9036 Val Loss: 0.5030636191368103, Val Acc: 0.8775\n","Epoch [89/500], Loss: 0.2729221284389496, Train Acc: 0.9078,train F1-score:0.9058 Val Loss: 0.5000321269035339, Val Acc: 0.8829\n","Epoch [90/500], Loss: 0.26758942008018494, Train Acc: 0.9070,train F1-score:0.9041 Val Loss: 0.497817724943161, Val Acc: 0.8855\n","Epoch [91/500], Loss: 0.25460126996040344, Train Acc: 0.9138,train F1-score:0.9107 Val Loss: 0.5100713968276978, Val Acc: 0.8802\n","Epoch [92/500], Loss: 0.26554736495018005, Train Acc: 0.9088,train F1-score:0.9073 Val Loss: 0.4965680241584778, Val Acc: 0.8835\n","Epoch [93/500], Loss: 0.25664541125297546, Train Acc: 0.9109,train F1-score:0.9075 Val Loss: 0.49511080980300903, Val Acc: 0.8862\n","Epoch [94/500], Loss: 0.25129836797714233, Train Acc: 0.9132,train F1-score:0.9097 Val Loss: 0.49483251571655273, Val Acc: 0.8775\n","Epoch [95/500], Loss: 0.26071295142173767, Train Acc: 0.9102,train F1-score:0.9084 Val Loss: 0.4935283064842224, Val Acc: 0.8855\n","Epoch [96/500], Loss: 0.24774585664272308, Train Acc: 0.9141,train F1-score:0.9111 Val Loss: 0.4953586757183075, Val Acc: 0.8876\n","Epoch [97/500], Loss: 0.2460285723209381, Train Acc: 0.9152,train F1-score:0.9121 Val Loss: 0.49662619829177856, Val Acc: 0.8842\n","Epoch [98/500], Loss: 0.2496592402458191, Train Acc: 0.9132,train F1-score:0.9109 Val Loss: 0.49573472142219543, Val Acc: 0.8876\n","Epoch [99/500], Loss: 0.2439865618944168, Train Acc: 0.9202,train F1-score:0.9183 Val Loss: 0.504244863986969, Val Acc: 0.8842\n","Epoch [100/500], Loss: 0.24083168804645538, Train Acc: 0.9175,train F1-score:0.9148 Val Loss: 0.5042839646339417, Val Acc: 0.8869\n","Epoch [101/500], Loss: 0.23610036075115204, Train Acc: 0.9214,train F1-score:0.9192 Val Loss: 0.49795079231262207, Val Acc: 0.8802\n","Epoch [102/500], Loss: 0.23296117782592773, Train Acc: 0.9196,train F1-score:0.9173 Val Loss: 0.5032463669776917, Val Acc: 0.8809\n","Epoch [103/500], Loss: 0.24026361107826233, Train Acc: 0.9189,train F1-score:0.9159 Val Loss: 0.4929349422454834, Val Acc: 0.8862\n","Epoch [104/500], Loss: 0.2385912984609604, Train Acc: 0.9174,train F1-score:0.9157 Val Loss: 0.4950029253959656, Val Acc: 0.8869\n","Epoch [105/500], Loss: 0.2377200871706009, Train Acc: 0.9182,train F1-score:0.9153 Val Loss: 0.4865504801273346, Val Acc: 0.8849\n","Epoch [106/500], Loss: 0.23258653283119202, Train Acc: 0.9213,train F1-score:0.9194 Val Loss: 0.49969482421875, Val Acc: 0.8822\n","Epoch [107/500], Loss: 0.2293033003807068, Train Acc: 0.9230,train F1-score:0.9211 Val Loss: 0.5117324590682983, Val Acc: 0.8842\n","Epoch [108/500], Loss: 0.22635330259799957, Train Acc: 0.9202,train F1-score:0.9177 Val Loss: 0.5131272673606873, Val Acc: 0.8862\n","Epoch [109/500], Loss: 0.2300270050764084, Train Acc: 0.9228,train F1-score:0.9206 Val Loss: 0.5096742510795593, Val Acc: 0.8849\n","Epoch [110/500], Loss: 0.2276436984539032, Train Acc: 0.9227,train F1-score:0.9211 Val Loss: 0.5179756879806519, Val Acc: 0.8876\n","Epoch [111/500], Loss: 0.22190260887145996, Train Acc: 0.9246,train F1-score:0.9222 Val Loss: 0.5305829048156738, Val Acc: 0.8835\n","Epoch [112/500], Loss: 0.21768084168434143, Train Acc: 0.9246,train F1-score:0.9222 Val Loss: 0.5266762375831604, Val Acc: 0.8855\n","Epoch [113/500], Loss: 0.22141988575458527, Train Acc: 0.9241,train F1-score:0.9229 Val Loss: 0.5283876657485962, Val Acc: 0.8849\n","Epoch [114/500], Loss: 0.22303198277950287, Train Acc: 0.9246,train F1-score:0.9225 Val Loss: 0.5299281477928162, Val Acc: 0.8862\n","Epoch [115/500], Loss: 0.21455290913581848, Train Acc: 0.9272,train F1-score:0.9252 Val Loss: 0.5483871698379517, Val Acc: 0.8795\n","Epoch [116/500], Loss: 0.2194429636001587, Train Acc: 0.9261,train F1-score:0.9247 Val Loss: 0.5364212989807129, Val Acc: 0.8788\n","Epoch [117/500], Loss: 0.21996906399726868, Train Acc: 0.9215,train F1-score:0.9191 Val Loss: 0.534669816493988, Val Acc: 0.8842\n","Epoch [118/500], Loss: 0.2152046114206314, Train Acc: 0.9262,train F1-score:0.9241 Val Loss: 0.5403371453285217, Val Acc: 0.8842\n","Epoch [119/500], Loss: 0.22617386281490326, Train Acc: 0.9236,train F1-score:0.9232 Val Loss: 0.5522540807723999, Val Acc: 0.8815\n","Epoch [120/500], Loss: 0.2229602038860321, Train Acc: 0.9216,train F1-score:0.9182 Val Loss: 0.5287883877754211, Val Acc: 0.8855\n","Epoch [121/500], Loss: 0.21017898619174957, Train Acc: 0.9297,train F1-score:0.9285 Val Loss: 0.5373136401176453, Val Acc: 0.8822\n","Epoch [122/500], Loss: 0.2192082554101944, Train Acc: 0.9240,train F1-score:0.9224 Val Loss: 0.5349404811859131, Val Acc: 0.8822\n","Epoch [123/500], Loss: 0.21423333883285522, Train Acc: 0.9255,train F1-score:0.9230 Val Loss: 0.53239506483078, Val Acc: 0.8842\n","Epoch [124/500], Loss: 0.2105703502893448, Train Acc: 0.9284,train F1-score:0.9269 Val Loss: 0.5394529104232788, Val Acc: 0.8929\n","Epoch [125/500], Loss: 0.21303582191467285, Train Acc: 0.9275,train F1-score:0.9259 Val Loss: 0.5339277386665344, Val Acc: 0.8855\n","Epoch [126/500], Loss: 0.21173962950706482, Train Acc: 0.9238,train F1-score:0.9214 Val Loss: 0.519619882106781, Val Acc: 0.8902\n","Epoch [127/500], Loss: 0.20571886003017426, Train Acc: 0.9284,train F1-score:0.9268 Val Loss: 0.544165313243866, Val Acc: 0.8829\n","Epoch [128/500], Loss: 0.21698328852653503, Train Acc: 0.9241,train F1-score:0.9226 Val Loss: 0.5208637714385986, Val Acc: 0.8882\n","Epoch [129/500], Loss: 0.20657625794410706, Train Acc: 0.9286,train F1-score:0.9266 Val Loss: 0.513854444026947, Val Acc: 0.8869\n","Epoch [130/500], Loss: 0.20528121292591095, Train Acc: 0.9278,train F1-score:0.9253 Val Loss: 0.5145865678787231, Val Acc: 0.8869\n","Epoch [131/500], Loss: 0.20419955253601074, Train Acc: 0.9300,train F1-score:0.9281 Val Loss: 0.5161926746368408, Val Acc: 0.8936\n","Epoch [132/500], Loss: 0.19878549873828888, Train Acc: 0.9326,train F1-score:0.9311 Val Loss: 0.526846706867218, Val Acc: 0.8909\n","Epoch [133/500], Loss: 0.20432136952877045, Train Acc: 0.9248,train F1-score:0.9217 Val Loss: 0.4992574155330658, Val Acc: 0.8949\n","Epoch [134/500], Loss: 0.1942654252052307, Train Acc: 0.9351,train F1-score:0.9340 Val Loss: 0.5057466626167297, Val Acc: 0.8909\n","Epoch [135/500], Loss: 0.20182839035987854, Train Acc: 0.9311,train F1-score:0.9303 Val Loss: 0.5205472707748413, Val Acc: 0.8922\n","Epoch [136/500], Loss: 0.1996072381734848, Train Acc: 0.9312,train F1-score:0.9292 Val Loss: 0.5088287591934204, Val Acc: 0.8909\n","Epoch [137/500], Loss: 0.20447078347206116, Train Acc: 0.9301,train F1-score:0.9285 Val Loss: 0.5022266507148743, Val Acc: 0.8882\n","Epoch [138/500], Loss: 0.1985500454902649, Train Acc: 0.9340,train F1-score:0.9327 Val Loss: 0.4990555942058563, Val Acc: 0.8896\n","Epoch [139/500], Loss: 0.19090324640274048, Train Acc: 0.9356,train F1-score:0.9342 Val Loss: 0.5193681120872498, Val Acc: 0.8882\n","Epoch [140/500], Loss: 0.1927020251750946, Train Acc: 0.9334,train F1-score:0.9320 Val Loss: 0.5390912890434265, Val Acc: 0.8882\n","Epoch [141/500], Loss: 0.1866869479417801, Train Acc: 0.9346,train F1-score:0.9327 Val Loss: 0.5357550978660583, Val Acc: 0.8896\n","Epoch [142/500], Loss: 0.19202788174152374, Train Acc: 0.9338,train F1-score:0.9330 Val Loss: 0.5253040790557861, Val Acc: 0.8963\n","Epoch [143/500], Loss: 0.1949898600578308, Train Acc: 0.9311,train F1-score:0.9294 Val Loss: 0.5282426476478577, Val Acc: 0.8942\n","Epoch [144/500], Loss: 0.1954556405544281, Train Acc: 0.9341,train F1-score:0.9330 Val Loss: 0.5234892964363098, Val Acc: 0.8889\n","Epoch [145/500], Loss: 0.18590308725833893, Train Acc: 0.9344,train F1-score:0.9324 Val Loss: 0.5117388963699341, Val Acc: 0.8916\n","Epoch [146/500], Loss: 0.191055029630661, Train Acc: 0.9323,train F1-score:0.9314 Val Loss: 0.5038133263587952, Val Acc: 0.8909\n","Epoch [147/500], Loss: 0.17953327298164368, Train Acc: 0.9395,train F1-score:0.9383 Val Loss: 0.5228480100631714, Val Acc: 0.8909\n","Epoch [148/500], Loss: 0.18271446228027344, Train Acc: 0.9359,train F1-score:0.9344 Val Loss: 0.5250067710876465, Val Acc: 0.8942\n","Epoch [149/500], Loss: 0.1865105777978897, Train Acc: 0.9354,train F1-score:0.9343 Val Loss: 0.528476357460022, Val Acc: 0.8942\n","Epoch [150/500], Loss: 0.18673652410507202, Train Acc: 0.9350,train F1-score:0.9335 Val Loss: 0.5394696593284607, Val Acc: 0.8902\n","Epoch [151/500], Loss: 0.18207360804080963, Train Acc: 0.9368,train F1-score:0.9357 Val Loss: 0.5439262390136719, Val Acc: 0.8949\n","Epoch [152/500], Loss: 0.18006788194179535, Train Acc: 0.9395,train F1-score:0.9382 Val Loss: 0.5438381433486938, Val Acc: 0.8876\n","Epoch [153/500], Loss: 0.1806454360485077, Train Acc: 0.9370,train F1-score:0.9354 Val Loss: 0.5378857254981995, Val Acc: 0.8889\n","Epoch [154/500], Loss: 0.17176459729671478, Train Acc: 0.9404,train F1-score:0.9393 Val Loss: 0.5550423264503479, Val Acc: 0.8909\n","Epoch [155/500], Loss: 0.17660517990589142, Train Acc: 0.9400,train F1-score:0.9389 Val Loss: 0.5601627230644226, Val Acc: 0.8896\n","Early stopping at epoch 155\n","Test Loss: 0.4627014100551605, Test Accuracy: 0.8693029490616622\n","Precision: 0.8613, Recall: 0.8693, F1-score: 0.8606\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import SAGEConv\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score as calculate_f1_score\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load data\n","edge_index_train = torch.load('/content/drive/MyDrive/PROJECT/edge_index.pt')\n","edge_index_test = torch.load('/content/drive/MyDrive/PROJECT/edge_index_test.pt')\n","edge_index_val = torch.load('/content/drive/MyDrive/PROJECT/edge_index_val.pt')\n","\n","features_file_train = '/content/drive/MyDrive/PROJECT/feature_matrix_train.txt'\n","X_train = np.loadtxt(features_file_train)\n","features_file_test = '/content/drive/MyDrive/PROJECT/feature_matrix_test.txt'\n","X_test = np.loadtxt(features_file_test)\n","features_file_val = '/content/drive/MyDrive/PROJECT/feature_matrix_val.txt'\n","X_val = np.loadtxt(features_file_val)\n","\n","labels_test = pd.read_csv(\"/content/drive/MyDrive/PROJECT/test_filtered.csv\")\n","labels_train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/train_filtered.csv\")\n","labels_val = pd.read_csv(\"/content/drive/MyDrive/PROJECT/val_filtered.csv\")\n","\n","y_train = torch.tensor(labels_train['label'].values, dtype=torch.long).to(device)\n","y_val = torch.tensor(labels_val['label'].values, dtype=torch.long).to(device)\n","y_true = torch.tensor(labels_test['label'].values, dtype=torch.long).to(device)\n","\n","# Preprocess features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n","\n","X_test_scaled = scaler.transform(X_test)\n","X_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n","\n","X_val_scaled = scaler.transform(X_val)\n","X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n","\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n","        super(GraphSAGE, self).__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.convs = torch.nn.ModuleList([SAGEConv(hidden_channels, hidden_channels) for _ in range(num_layers - 2)])\n","        self.conv2 = SAGEConv(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        for conv in self.convs:\n","            x = F.relu(conv(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=-1)\n","# Initialize the GraphSAGE model\n","model = GraphSAGE(in_channels=X_train.shape[1], hidden_channels=128, out_channels=13,num_layers=4)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Convert data to appropriate format\n","edge_index_train = edge_index_train.to(device)\n","edge_index_val = edge_index_val.to(device)\n","edge_index_test = edge_index_test.to(device)\n","\n","# Training\n","def train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100):\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    current_patience = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    train_f1_scores = []  # Initialize list for training F1 scores\n","    epochss = []\n","\n","    for epoch in range(epochs):\n","        epochss.append(epoch)\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train, edge_train)\n","        loss = criterion(outputs, y_train)\n","        train_losses.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute training accuracy\n","        _, predicted_train = torch.max(outputs, 1)\n","        train_acc = torch.sum(predicted_train == y_train).item() / len(y_train)\n","        # Calculate training F1 score\n","        train_f1 = calculate_f1_score(y_train.cpu().numpy(), predicted_train.cpu().numpy(), average='weighted')\n","\n","\n","        # Save training F1 score\n","        train_f1_scores.append(train_f1)\n","\n","\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val, edge_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss)\n","            # Compute validation accuracy\n","            _, predicted_val = torch.max(val_outputs, 1)\n","            val_acc = torch.sum(predicted_val == y_val).item() / len(y_val)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), 'best_model4.pt')\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","            if current_patience >= patience:\n","                print(f'Early stopping at epoch {epoch}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}, Train Acc: {train_acc:.4f},train F1-score:{train_f1:.4f} Val Loss: {val_loss.item()}, Val Acc: {val_acc:.4f}')\n","\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer4/train_f1_scores_GS.txt',train_f1_scores)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer4/train_loss_GS.txt', train_losses)\n","    np.savetxt('/content/drive/MyDrive/PROJECT/layer4/epochs_GS.txt', epochss)\n","\n","\n","\n","\n","\n","\n","# Convert data to appropriate format\n","edge_train = edge_index_train.to(device)\n","edge_val = edge_index_val.to(device)\n","edge_test = edge_index_test.to(device)\n","\n","# Train the model\n","train(model, optimizer, criterion, X_train, edge_train, y_train, X_val, edge_val, y_val, epochs=500, patience=100)\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model4.pt'))\n","\n","# Testing\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test, edge_test)\n","    test_loss = criterion(test_outputs, y_true)\n","    _, predicted = torch.max(test_outputs, 1)\n","    accuracy = torch.sum(predicted == y_true).item() / len(y_true)\n","\n","print(f'Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n","\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","\n","# Calculate precision, recall, F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n","\n","print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n","\n","with open('/content/drive/MyDrive/PROJECT/layer4/f1_score_GS.txt', 'w') as file:\n","    file.write(f'{f1_score:.4f}')\n","\n","print(\"F1-score saved to file.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iW8JM_4shRDm","executionInfo":{"status":"ok","timestamp":1714455866280,"user_tz":-330,"elapsed":43843,"user":{"displayName":"Dhilja R","userId":"07628460364557409347"}},"outputId":"e87bc499-26ff-4127-d928-62db90c0a377"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/500], Loss: 2.560387134552002, Train Acc: 0.0286,train F1-score:0.0298 Val Loss: 1.93503999710083, Val Acc: 0.6787\n","Epoch [2/500], Loss: 1.9610605239868164, Train Acc: 0.6425,train F1-score:0.5047 Val Loss: 1.3121299743652344, Val Acc: 0.6780\n","Epoch [3/500], Loss: 1.5157147645950317, Train Acc: 0.6406,train F1-score:0.5051 Val Loss: 1.1936206817626953, Val Acc: 0.6446\n","Epoch [4/500], Loss: 1.4234930276870728, Train Acc: 0.5325,train F1-score:0.4844 Val Loss: 1.052481770515442, Val Acc: 0.6760\n","Epoch [5/500], Loss: 1.1843818426132202, Train Acc: 0.6219,train F1-score:0.5088 Val Loss: 1.0488173961639404, Val Acc: 0.6787\n","Epoch [6/500], Loss: 1.1609793901443481, Train Acc: 0.6419,train F1-score:0.5064 Val Loss: 1.0263724327087402, Val Acc: 0.6787\n","Epoch [7/500], Loss: 1.1369439363479614, Train Acc: 0.6430,train F1-score:0.5050 Val Loss: 1.0014164447784424, Val Acc: 0.6787\n","Epoch [8/500], Loss: 1.0989017486572266, Train Acc: 0.6436,train F1-score:0.5055 Val Loss: 0.9679543375968933, Val Acc: 0.6787\n","Epoch [9/500], Loss: 1.058333158493042, Train Acc: 0.6455,train F1-score:0.5148 Val Loss: 0.9357749819755554, Val Acc: 0.6801\n","Epoch [10/500], Loss: 1.0269855260849, Train Acc: 0.6493,train F1-score:0.5247 Val Loss: 0.9114761352539062, Val Acc: 0.6814\n","Epoch [11/500], Loss: 0.9964868426322937, Train Acc: 0.6486,train F1-score:0.5259 Val Loss: 0.8849889039993286, Val Acc: 0.6921\n","Epoch [12/500], Loss: 0.9689491987228394, Train Acc: 0.6533,train F1-score:0.5377 Val Loss: 0.8630979657173157, Val Acc: 0.6975\n","Epoch [13/500], Loss: 0.9497039914131165, Train Acc: 0.6566,train F1-score:0.5463 Val Loss: 0.8392823934555054, Val Acc: 0.6975\n","Epoch [14/500], Loss: 0.9197092652320862, Train Acc: 0.6652,train F1-score:0.5627 Val Loss: 0.8201354146003723, Val Acc: 0.7329\n","Epoch [15/500], Loss: 0.9013606309890747, Train Acc: 0.6764,train F1-score:0.5858 Val Loss: 0.7991707921028137, Val Acc: 0.7390\n","Epoch [16/500], Loss: 0.8738172054290771, Train Acc: 0.6873,train F1-score:0.6049 Val Loss: 0.7805761098861694, Val Acc: 0.7336\n","Epoch [17/500], Loss: 0.8478288650512695, Train Acc: 0.6939,train F1-score:0.6179 Val Loss: 0.7628827691078186, Val Acc: 0.7383\n","Epoch [18/500], Loss: 0.8268481492996216, Train Acc: 0.6975,train F1-score:0.6234 Val Loss: 0.7470701336860657, Val Acc: 0.7490\n","Epoch [19/500], Loss: 0.8007602095603943, Train Acc: 0.7095,train F1-score:0.6329 Val Loss: 0.7350515127182007, Val Acc: 0.7503\n","Epoch [20/500], Loss: 0.7833871245384216, Train Acc: 0.7130,train F1-score:0.6381 Val Loss: 0.722744882106781, Val Acc: 0.7537\n","Epoch [21/500], Loss: 0.7616403102874756, Train Acc: 0.7190,train F1-score:0.6416 Val Loss: 0.7109430432319641, Val Acc: 0.7550\n","Epoch [22/500], Loss: 0.7466038465499878, Train Acc: 0.7240,train F1-score:0.6494 Val Loss: 0.696125864982605, Val Acc: 0.7544\n","Epoch [23/500], Loss: 0.7265081405639648, Train Acc: 0.7274,train F1-score:0.6541 Val Loss: 0.6810217499732971, Val Acc: 0.7617\n","Epoch [24/500], Loss: 0.7072845697402954, Train Acc: 0.7351,train F1-score:0.6716 Val Loss: 0.6687078475952148, Val Acc: 0.7731\n","Epoch [25/500], Loss: 0.6927813291549683, Train Acc: 0.7436,train F1-score:0.6912 Val Loss: 0.6556251049041748, Val Acc: 0.7845\n","Epoch [26/500], Loss: 0.6750146746635437, Train Acc: 0.7567,train F1-score:0.7142 Val Loss: 0.6452236175537109, Val Acc: 0.7838\n","Epoch [27/500], Loss: 0.6683222651481628, Train Acc: 0.7551,train F1-score:0.7176 Val Loss: 0.643513023853302, Val Acc: 0.7845\n","Epoch [28/500], Loss: 0.6614168882369995, Train Acc: 0.7578,train F1-score:0.7157 Val Loss: 0.6244805455207825, Val Acc: 0.7818\n","Epoch [29/500], Loss: 0.6424313187599182, Train Acc: 0.7624,train F1-score:0.7274 Val Loss: 0.6038856506347656, Val Acc: 0.7932\n","Epoch [30/500], Loss: 0.6209794878959656, Train Acc: 0.7697,train F1-score:0.7363 Val Loss: 0.6039161682128906, Val Acc: 0.7898\n","Epoch [31/500], Loss: 0.6139976978302002, Train Acc: 0.7761,train F1-score:0.7465 Val Loss: 0.6008767485618591, Val Acc: 0.7831\n","Epoch [32/500], Loss: 0.6083360314369202, Train Acc: 0.7801,train F1-score:0.7550 Val Loss: 0.5809639692306519, Val Acc: 0.7979\n","Epoch [33/500], Loss: 0.5871334075927734, Train Acc: 0.7857,train F1-score:0.7549 Val Loss: 0.5742529630661011, Val Acc: 0.8059\n","Epoch [34/500], Loss: 0.574752151966095, Train Acc: 0.7921,train F1-score:0.7624 Val Loss: 0.5717188119888306, Val Acc: 0.8072\n","Epoch [35/500], Loss: 0.5680691003799438, Train Acc: 0.7960,train F1-score:0.7720 Val Loss: 0.5612165331840515, Val Acc: 0.8126\n","Epoch [36/500], Loss: 0.5554019212722778, Train Acc: 0.7989,train F1-score:0.7712 Val Loss: 0.5524539351463318, Val Acc: 0.8153\n","Epoch [37/500], Loss: 0.5397495627403259, Train Acc: 0.8054,train F1-score:0.7778 Val Loss: 0.5476992130279541, Val Acc: 0.8059\n","Epoch [38/500], Loss: 0.5288994908332825, Train Acc: 0.8091,train F1-score:0.7852 Val Loss: 0.5366019606590271, Val Acc: 0.8153\n","Epoch [39/500], Loss: 0.5190954804420471, Train Acc: 0.8100,train F1-score:0.7848 Val Loss: 0.5376626253128052, Val Acc: 0.8199\n","Epoch [40/500], Loss: 0.5103405714035034, Train Acc: 0.8128,train F1-score:0.7902 Val Loss: 0.5288627743721008, Val Acc: 0.8139\n","Epoch [41/500], Loss: 0.4964646100997925, Train Acc: 0.8200,train F1-score:0.8047 Val Loss: 0.5154595971107483, Val Acc: 0.8313\n","Epoch [42/500], Loss: 0.48458805680274963, Train Acc: 0.8263,train F1-score:0.8096 Val Loss: 0.5103453993797302, Val Acc: 0.8367\n","Epoch [43/500], Loss: 0.4765606224536896, Train Acc: 0.8393,train F1-score:0.8282 Val Loss: 0.5122650861740112, Val Acc: 0.8327\n","Epoch [44/500], Loss: 0.4661584496498108, Train Acc: 0.8427,train F1-score:0.8351 Val Loss: 0.50837641954422, Val Acc: 0.8394\n","Epoch [45/500], Loss: 0.4543534219264984, Train Acc: 0.8499,train F1-score:0.8410 Val Loss: 0.49827468395233154, Val Acc: 0.8487\n","Epoch [46/500], Loss: 0.43908268213272095, Train Acc: 0.8553,train F1-score:0.8471 Val Loss: 0.4900589883327484, Val Acc: 0.8494\n","Epoch [47/500], Loss: 0.4352852404117584, Train Acc: 0.8545,train F1-score:0.8473 Val Loss: 0.4919235408306122, Val Acc: 0.8474\n","Epoch [48/500], Loss: 0.4283422827720642, Train Acc: 0.8571,train F1-score:0.8503 Val Loss: 0.48284777998924255, Val Acc: 0.8467\n","Epoch [49/500], Loss: 0.414691686630249, Train Acc: 0.8650,train F1-score:0.8595 Val Loss: 0.47692838311195374, Val Acc: 0.8534\n","Epoch [50/500], Loss: 0.4023709297180176, Train Acc: 0.8649,train F1-score:0.8578 Val Loss: 0.4783210754394531, Val Acc: 0.8527\n","Epoch [51/500], Loss: 0.4031822085380554, Train Acc: 0.8654,train F1-score:0.8579 Val Loss: 0.4849349856376648, Val Acc: 0.8487\n","Epoch [52/500], Loss: 0.40027034282684326, Train Acc: 0.8696,train F1-score:0.8664 Val Loss: 0.49189215898513794, Val Acc: 0.8514\n","Epoch [53/500], Loss: 0.3998556137084961, Train Acc: 0.8661,train F1-score:0.8569 Val Loss: 0.47748956084251404, Val Acc: 0.8554\n","Epoch [54/500], Loss: 0.37679681181907654, Train Acc: 0.8757,train F1-score:0.8710 Val Loss: 0.473541796207428, Val Acc: 0.8561\n","Epoch [55/500], Loss: 0.3619734048843384, Train Acc: 0.8834,train F1-score:0.8785 Val Loss: 0.4778009355068207, Val Acc: 0.8601\n","Epoch [56/500], Loss: 0.36833447217941284, Train Acc: 0.8778,train F1-score:0.8713 Val Loss: 0.4685240685939789, Val Acc: 0.8574\n","Epoch [57/500], Loss: 0.36487528681755066, Train Acc: 0.8799,train F1-score:0.8766 Val Loss: 0.46706801652908325, Val Acc: 0.8601\n","Epoch [58/500], Loss: 0.35683876276016235, Train Acc: 0.8787,train F1-score:0.8714 Val Loss: 0.45220109820365906, Val Acc: 0.8568\n","Epoch [59/500], Loss: 0.3344370722770691, Train Acc: 0.8863,train F1-score:0.8821 Val Loss: 0.46440842747688293, Val Acc: 0.8561\n","Epoch [60/500], Loss: 0.3421791195869446, Train Acc: 0.8841,train F1-score:0.8810 Val Loss: 0.4661593437194824, Val Acc: 0.8581\n","Epoch [61/500], Loss: 0.3387318551540375, Train Acc: 0.8839,train F1-score:0.8772 Val Loss: 0.45592039823532104, Val Acc: 0.8541\n","Epoch [62/500], Loss: 0.3311411142349243, Train Acc: 0.8885,train F1-score:0.8856 Val Loss: 0.45231491327285767, Val Acc: 0.8594\n","Epoch [63/500], Loss: 0.3154652714729309, Train Acc: 0.8949,train F1-score:0.8911 Val Loss: 0.4617024064064026, Val Acc: 0.8614\n","Epoch [64/500], Loss: 0.32193225622177124, Train Acc: 0.8884,train F1-score:0.8827 Val Loss: 0.4493451416492462, Val Acc: 0.8661\n","Epoch [65/500], Loss: 0.30393776297569275, Train Acc: 0.8989,train F1-score:0.8962 Val Loss: 0.45961034297943115, Val Acc: 0.8641\n","Epoch [66/500], Loss: 0.3041546642780304, Train Acc: 0.8979,train F1-score:0.8942 Val Loss: 0.4660663604736328, Val Acc: 0.8628\n","Epoch [67/500], Loss: 0.30509352684020996, Train Acc: 0.8968,train F1-score:0.8927 Val Loss: 0.4672064781188965, Val Acc: 0.8648\n","Epoch [68/500], Loss: 0.29434657096862793, Train Acc: 0.9006,train F1-score:0.8971 Val Loss: 0.4492340385913849, Val Acc: 0.8688\n","Epoch [69/500], Loss: 0.28603261709213257, Train Acc: 0.9019,train F1-score:0.8992 Val Loss: 0.45247942209243774, Val Acc: 0.8715\n","Epoch [70/500], Loss: 0.28322577476501465, Train Acc: 0.9035,train F1-score:0.9001 Val Loss: 0.45728108286857605, Val Acc: 0.8728\n","Epoch [71/500], Loss: 0.2776567339897156, Train Acc: 0.9071,train F1-score:0.9041 Val Loss: 0.45665478706359863, Val Acc: 0.8688\n","Epoch [72/500], Loss: 0.26929208636283875, Train Acc: 0.9097,train F1-score:0.9076 Val Loss: 0.4625491201877594, Val Acc: 0.8668\n","Epoch [73/500], Loss: 0.2715076804161072, Train Acc: 0.9086,train F1-score:0.9060 Val Loss: 0.46287089586257935, Val Acc: 0.8628\n","Epoch [74/500], Loss: 0.2778531312942505, Train Acc: 0.9066,train F1-score:0.9050 Val Loss: 0.4815530478954315, Val Acc: 0.8655\n","Epoch [75/500], Loss: 0.2818964421749115, Train Acc: 0.9058,train F1-score:0.9019 Val Loss: 0.45524489879608154, Val Acc: 0.8728\n","Epoch [76/500], Loss: 0.27383652329444885, Train Acc: 0.9053,train F1-score:0.9039 Val Loss: 0.45291686058044434, Val Acc: 0.8815\n","Epoch [77/500], Loss: 0.2550499439239502, Train Acc: 0.9132,train F1-score:0.9097 Val Loss: 0.4568788707256317, Val Acc: 0.8768\n","Epoch [78/500], Loss: 0.2584778070449829, Train Acc: 0.9108,train F1-score:0.9081 Val Loss: 0.4442410469055176, Val Acc: 0.8715\n","Epoch [79/500], Loss: 0.2671700716018677, Train Acc: 0.9118,train F1-score:0.9110 Val Loss: 0.45851749181747437, Val Acc: 0.8762\n","Epoch [80/500], Loss: 0.25394415855407715, Train Acc: 0.9118,train F1-score:0.9092 Val Loss: 0.4523029923439026, Val Acc: 0.8688\n","Epoch [81/500], Loss: 0.2558963894844055, Train Acc: 0.9132,train F1-score:0.9100 Val Loss: 0.4513903260231018, Val Acc: 0.8695\n","Epoch [82/500], Loss: 0.26011112332344055, Train Acc: 0.9096,train F1-score:0.9094 Val Loss: 0.4562697410583496, Val Acc: 0.8735\n","Epoch [83/500], Loss: 0.25426074862480164, Train Acc: 0.9150,train F1-score:0.9118 Val Loss: 0.45277318358421326, Val Acc: 0.8782\n","Epoch [84/500], Loss: 0.24572746455669403, Train Acc: 0.9163,train F1-score:0.9131 Val Loss: 0.44006335735321045, Val Acc: 0.8809\n","Epoch [85/500], Loss: 0.23808106780052185, Train Acc: 0.9218,train F1-score:0.9208 Val Loss: 0.4542725682258606, Val Acc: 0.8782\n","Epoch [86/500], Loss: 0.23922990262508392, Train Acc: 0.9204,train F1-score:0.9186 Val Loss: 0.4783909320831299, Val Acc: 0.8768\n","Epoch [87/500], Loss: 0.23228858411312103, Train Acc: 0.9213,train F1-score:0.9187 Val Loss: 0.46525201201438904, Val Acc: 0.8748\n","Epoch [88/500], Loss: 0.22629477083683014, Train Acc: 0.9235,train F1-score:0.9218 Val Loss: 0.4683629870414734, Val Acc: 0.8829\n","Epoch [89/500], Loss: 0.22280409932136536, Train Acc: 0.9249,train F1-score:0.9232 Val Loss: 0.4916697144508362, Val Acc: 0.8835\n","Epoch [90/500], Loss: 0.22143296897411346, Train Acc: 0.9230,train F1-score:0.9211 Val Loss: 0.4764440953731537, Val Acc: 0.8862\n","Epoch [91/500], Loss: 0.2118959128856659, Train Acc: 0.9299,train F1-score:0.9286 Val Loss: 0.4744400680065155, Val Acc: 0.8842\n","Epoch [92/500], Loss: 0.2129811942577362, Train Acc: 0.9275,train F1-score:0.9257 Val Loss: 0.48748114705085754, Val Acc: 0.8829\n","Epoch [93/500], Loss: 0.2133432924747467, Train Acc: 0.9301,train F1-score:0.9285 Val Loss: 0.478368878364563, Val Acc: 0.8809\n","Epoch [94/500], Loss: 0.21330945193767548, Train Acc: 0.9307,train F1-score:0.9298 Val Loss: 0.5098208785057068, Val Acc: 0.8768\n","Epoch [95/500], Loss: 0.2242099642753601, Train Acc: 0.9226,train F1-score:0.9200 Val Loss: 0.49776801466941833, Val Acc: 0.8829\n","Epoch [96/500], Loss: 0.2164846956729889, Train Acc: 0.9244,train F1-score:0.9232 Val Loss: 0.49277767539024353, Val Acc: 0.8755\n","Epoch [97/500], Loss: 0.216363787651062, Train Acc: 0.9247,train F1-score:0.9225 Val Loss: 0.47756266593933105, Val Acc: 0.8855\n","Epoch [98/500], Loss: 0.20131127536296844, Train Acc: 0.9317,train F1-score:0.9300 Val Loss: 0.5167074799537659, Val Acc: 0.8795\n","Epoch [99/500], Loss: 0.21743950247764587, Train Acc: 0.9292,train F1-score:0.9285 Val Loss: 0.5146490335464478, Val Acc: 0.8755\n","Epoch [100/500], Loss: 0.22049647569656372, Train Acc: 0.9250,train F1-score:0.9228 Val Loss: 0.48872020840644836, Val Acc: 0.8862\n","Epoch [101/500], Loss: 0.207159161567688, Train Acc: 0.9281,train F1-score:0.9262 Val Loss: 0.4887387752532959, Val Acc: 0.8882\n","Epoch [102/500], Loss: 0.1959652304649353, Train Acc: 0.9351,train F1-score:0.9340 Val Loss: 0.5178776383399963, Val Acc: 0.8815\n","Epoch [103/500], Loss: 0.2063121795654297, Train Acc: 0.9276,train F1-score:0.9253 Val Loss: 0.5058311223983765, Val Acc: 0.8876\n","Epoch [104/500], Loss: 0.19332413375377655, Train Acc: 0.9352,train F1-score:0.9340 Val Loss: 0.5215986371040344, Val Acc: 0.8942\n","Epoch [105/500], Loss: 0.20367401838302612, Train Acc: 0.9328,train F1-score:0.9314 Val Loss: 0.5310131907463074, Val Acc: 0.8809\n","Epoch [106/500], Loss: 0.2110709846019745, Train Acc: 0.9283,train F1-score:0.9263 Val Loss: 0.5149052143096924, Val Acc: 0.8842\n","Epoch [107/500], Loss: 0.18912620842456818, Train Acc: 0.9350,train F1-score:0.9340 Val Loss: 0.5283377170562744, Val Acc: 0.8842\n","Epoch [108/500], Loss: 0.1828734278678894, Train Acc: 0.9383,train F1-score:0.9367 Val Loss: 0.5124418139457703, Val Acc: 0.8849\n","Epoch [109/500], Loss: 0.18403619527816772, Train Acc: 0.9380,train F1-score:0.9363 Val Loss: 0.5038607120513916, Val Acc: 0.8909\n","Epoch [110/500], Loss: 0.17783348262310028, Train Acc: 0.9397,train F1-score:0.9381 Val Loss: 0.5081424117088318, Val Acc: 0.8916\n","Epoch [111/500], Loss: 0.17515988647937775, Train Acc: 0.9410,train F1-score:0.9396 Val Loss: 0.5223298668861389, Val Acc: 0.8876\n","Epoch [112/500], Loss: 0.1759788542985916, Train Acc: 0.9408,train F1-score:0.9392 Val Loss: 0.5161053538322449, Val Acc: 0.8889\n","Epoch [113/500], Loss: 0.16926628351211548, Train Acc: 0.9436,train F1-score:0.9425 Val Loss: 0.5163111090660095, Val Acc: 0.8916\n","Epoch [114/500], Loss: 0.16947530210018158, Train Acc: 0.9430,train F1-score:0.9417 Val Loss: 0.5142929553985596, Val Acc: 0.8942\n","Epoch [115/500], Loss: 0.1660427749156952, Train Acc: 0.9452,train F1-score:0.9443 Val Loss: 0.5000457763671875, Val Acc: 0.8949\n","Epoch [116/500], Loss: 0.1594248265028, Train Acc: 0.9455,train F1-score:0.9443 Val Loss: 0.5071770548820496, Val Acc: 0.8936\n","Epoch [117/500], Loss: 0.16131678223609924, Train Acc: 0.9452,train F1-score:0.9438 Val Loss: 0.5061088800430298, Val Acc: 0.8942\n","Epoch [118/500], Loss: 0.1587005853652954, Train Acc: 0.9461,train F1-score:0.9453 Val Loss: 0.5285227298736572, Val Acc: 0.8936\n","Epoch [119/500], Loss: 0.1588052213191986, Train Acc: 0.9477,train F1-score:0.9463 Val Loss: 0.5233278870582581, Val Acc: 0.8835\n","Epoch [120/500], Loss: 0.16788031160831451, Train Acc: 0.9395,train F1-score:0.9398 Val Loss: 0.5867138504981995, Val Acc: 0.8855\n","Epoch [121/500], Loss: 0.2016007900238037, Train Acc: 0.9350,train F1-score:0.9319 Val Loss: 0.5239507555961609, Val Acc: 0.8782\n","Epoch [122/500], Loss: 0.18951603770256042, Train Acc: 0.9310,train F1-score:0.9323 Val Loss: 0.5457832217216492, Val Acc: 0.8922\n","Epoch [123/500], Loss: 0.1629807949066162, Train Acc: 0.9441,train F1-score:0.9422 Val Loss: 0.5559936165809631, Val Acc: 0.8835\n","Epoch [124/500], Loss: 0.1687241941690445, Train Acc: 0.9380,train F1-score:0.9355 Val Loss: 0.5117809772491455, Val Acc: 0.8882\n","Epoch [125/500], Loss: 0.1717703938484192, Train Acc: 0.9454,train F1-score:0.9450 Val Loss: 0.526279091835022, Val Acc: 0.8869\n","Epoch [126/500], Loss: 0.16374196112155914, Train Acc: 0.9458,train F1-score:0.9449 Val Loss: 0.5609174370765686, Val Acc: 0.8869\n","Epoch [127/500], Loss: 0.16695815324783325, Train Acc: 0.9397,train F1-score:0.9373 Val Loss: 0.5518713593482971, Val Acc: 0.8855\n","Epoch [128/500], Loss: 0.17870962619781494, Train Acc: 0.9408,train F1-score:0.9391 Val Loss: 0.5360230803489685, Val Acc: 0.8889\n","Epoch [129/500], Loss: 0.162418931722641, Train Acc: 0.9463,train F1-score:0.9454 Val Loss: 0.5690011978149414, Val Acc: 0.8869\n","Epoch [130/500], Loss: 0.16414043307304382, Train Acc: 0.9436,train F1-score:0.9427 Val Loss: 0.567640483379364, Val Acc: 0.8902\n","Epoch [131/500], Loss: 0.15424707531929016, Train Acc: 0.9464,train F1-score:0.9449 Val Loss: 0.565610408782959, Val Acc: 0.8849\n","Epoch [132/500], Loss: 0.15978260338306427, Train Acc: 0.9457,train F1-score:0.9444 Val Loss: 0.5690856575965881, Val Acc: 0.8929\n","Epoch [133/500], Loss: 0.15145640075206757, Train Acc: 0.9483,train F1-score:0.9476 Val Loss: 0.595530092716217, Val Acc: 0.8849\n","Epoch [134/500], Loss: 0.16335751116275787, Train Acc: 0.9434,train F1-score:0.9425 Val Loss: 0.5671616196632385, Val Acc: 0.8855\n","Epoch [135/500], Loss: 0.15812519192695618, Train Acc: 0.9474,train F1-score:0.9458 Val Loss: 0.5729578733444214, Val Acc: 0.8842\n","Epoch [136/500], Loss: 0.15353135764598846, Train Acc: 0.9481,train F1-score:0.9470 Val Loss: 0.5787166953086853, Val Acc: 0.8876\n","Epoch [137/500], Loss: 0.15188494324684143, Train Acc: 0.9488,train F1-score:0.9482 Val Loss: 0.5644364356994629, Val Acc: 0.8909\n","Epoch [138/500], Loss: 0.14958757162094116, Train Acc: 0.9504,train F1-score:0.9493 Val Loss: 0.5631831288337708, Val Acc: 0.8949\n","Epoch [139/500], Loss: 0.14767980575561523, Train Acc: 0.9499,train F1-score:0.9488 Val Loss: 0.567478597164154, Val Acc: 0.8936\n","Epoch [140/500], Loss: 0.14344647526741028, Train Acc: 0.9511,train F1-score:0.9502 Val Loss: 0.5810500979423523, Val Acc: 0.8916\n","Epoch [141/500], Loss: 0.14268386363983154, Train Acc: 0.9493,train F1-score:0.9486 Val Loss: 0.5798624753952026, Val Acc: 0.8882\n","Epoch [142/500], Loss: 0.14826607704162598, Train Acc: 0.9514,train F1-score:0.9503 Val Loss: 0.5712354183197021, Val Acc: 0.8862\n","Epoch [143/500], Loss: 0.14723530411720276, Train Acc: 0.9512,train F1-score:0.9504 Val Loss: 0.5754944682121277, Val Acc: 0.8876\n","Epoch [144/500], Loss: 0.1420644074678421, Train Acc: 0.9528,train F1-score:0.9517 Val Loss: 0.6104122400283813, Val Acc: 0.8869\n","Epoch [145/500], Loss: 0.1377040147781372, Train Acc: 0.9518,train F1-score:0.9507 Val Loss: 0.5763327479362488, Val Acc: 0.8902\n","Epoch [146/500], Loss: 0.13463181257247925, Train Acc: 0.9560,train F1-score:0.9552 Val Loss: 0.5535020232200623, Val Acc: 0.8916\n","Epoch [147/500], Loss: 0.1331910640001297, Train Acc: 0.9558,train F1-score:0.9549 Val Loss: 0.5539971590042114, Val Acc: 0.8936\n","Epoch [148/500], Loss: 0.13412973284721375, Train Acc: 0.9547,train F1-score:0.9535 Val Loss: 0.5750010013580322, Val Acc: 0.8916\n","Epoch [149/500], Loss: 0.13020063936710358, Train Acc: 0.9588,train F1-score:0.9578 Val Loss: 0.5745110511779785, Val Acc: 0.8929\n","Epoch [150/500], Loss: 0.13085298240184784, Train Acc: 0.9591,train F1-score:0.9584 Val Loss: 0.568698525428772, Val Acc: 0.8902\n","Epoch [151/500], Loss: 0.13117650151252747, Train Acc: 0.9551,train F1-score:0.9541 Val Loss: 0.5718756318092346, Val Acc: 0.8896\n","Epoch [152/500], Loss: 0.12771549820899963, Train Acc: 0.9567,train F1-score:0.9562 Val Loss: 0.5815402269363403, Val Acc: 0.8909\n","Epoch [153/500], Loss: 0.12462174892425537, Train Acc: 0.9563,train F1-score:0.9554 Val Loss: 0.5968014001846313, Val Acc: 0.8916\n","Epoch [154/500], Loss: 0.12725405395030975, Train Acc: 0.9569,train F1-score:0.9562 Val Loss: 0.577476441860199, Val Acc: 0.8922\n","Epoch [155/500], Loss: 0.12510442733764648, Train Acc: 0.9580,train F1-score:0.9572 Val Loss: 0.5823708772659302, Val Acc: 0.8929\n","Epoch [156/500], Loss: 0.1296541690826416, Train Acc: 0.9593,train F1-score:0.9583 Val Loss: 0.603083610534668, Val Acc: 0.8889\n","Epoch [157/500], Loss: 0.12784822285175323, Train Acc: 0.9575,train F1-score:0.9565 Val Loss: 0.5965261459350586, Val Acc: 0.8896\n","Epoch [158/500], Loss: 0.12509910762310028, Train Acc: 0.9588,train F1-score:0.9581 Val Loss: 0.6062957048416138, Val Acc: 0.8869\n","Epoch [159/500], Loss: 0.12842510640621185, Train Acc: 0.9568,train F1-score:0.9557 Val Loss: 0.6275213360786438, Val Acc: 0.8896\n","Epoch [160/500], Loss: 0.12346991896629333, Train Acc: 0.9574,train F1-score:0.9567 Val Loss: 0.6308109760284424, Val Acc: 0.8876\n","Epoch [161/500], Loss: 0.1250084787607193, Train Acc: 0.9570,train F1-score:0.9560 Val Loss: 0.6117814779281616, Val Acc: 0.8882\n","Epoch [162/500], Loss: 0.12144751101732254, Train Acc: 0.9598,train F1-score:0.9594 Val Loss: 0.6049913763999939, Val Acc: 0.8942\n","Epoch [163/500], Loss: 0.12170696258544922, Train Acc: 0.9585,train F1-score:0.9577 Val Loss: 0.6097362637519836, Val Acc: 0.8956\n","Epoch [164/500], Loss: 0.11749345064163208, Train Acc: 0.9610,train F1-score:0.9600 Val Loss: 0.6096833348274231, Val Acc: 0.8909\n","Epoch [165/500], Loss: 0.11322106420993805, Train Acc: 0.9623,train F1-score:0.9618 Val Loss: 0.6170905828475952, Val Acc: 0.8916\n","Epoch [166/500], Loss: 0.11896298825740814, Train Acc: 0.9592,train F1-score:0.9584 Val Loss: 0.6050131916999817, Val Acc: 0.8942\n","Epoch [167/500], Loss: 0.11995870620012283, Train Acc: 0.9583,train F1-score:0.9578 Val Loss: 0.63116854429245, Val Acc: 0.8922\n","Epoch [168/500], Loss: 0.11419292539358139, Train Acc: 0.9603,train F1-score:0.9593 Val Loss: 0.6450791954994202, Val Acc: 0.8942\n","Epoch [169/500], Loss: 0.11442764103412628, Train Acc: 0.9609,train F1-score:0.9604 Val Loss: 0.6091481447219849, Val Acc: 0.8936\n","Epoch [170/500], Loss: 0.10994379967451096, Train Acc: 0.9642,train F1-score:0.9638 Val Loss: 0.6261159181594849, Val Acc: 0.8942\n","Epoch [171/500], Loss: 0.11219184845685959, Train Acc: 0.9596,train F1-score:0.9585 Val Loss: 0.6262339353561401, Val Acc: 0.8902\n","Epoch [172/500], Loss: 0.11175227165222168, Train Acc: 0.9628,train F1-score:0.9624 Val Loss: 0.6485556364059448, Val Acc: 0.8922\n","Epoch [173/500], Loss: 0.1086433157324791, Train Acc: 0.9623,train F1-score:0.9613 Val Loss: 0.6355143785476685, Val Acc: 0.8963\n","Epoch [174/500], Loss: 0.11200089007616043, Train Acc: 0.9603,train F1-score:0.9599 Val Loss: 0.6535935401916504, Val Acc: 0.8902\n","Epoch [175/500], Loss: 0.10940121859312057, Train Acc: 0.9626,train F1-score:0.9615 Val Loss: 0.642236053943634, Val Acc: 0.8882\n","Epoch [176/500], Loss: 0.10798363387584686, Train Acc: 0.9614,train F1-score:0.9608 Val Loss: 0.638364315032959, Val Acc: 0.8929\n","Epoch [177/500], Loss: 0.1024894267320633, Train Acc: 0.9655,train F1-score:0.9646 Val Loss: 0.6552040576934814, Val Acc: 0.8862\n","Epoch [178/500], Loss: 0.103412926197052, Train Acc: 0.9645,train F1-score:0.9636 Val Loss: 0.6438015699386597, Val Acc: 0.8889\n","Epoch [179/500], Loss: 0.10940435528755188, Train Acc: 0.9620,train F1-score:0.9620 Val Loss: 0.6848157644271851, Val Acc: 0.8956\n","Epoch [180/500], Loss: 0.11687528342008591, Train Acc: 0.9590,train F1-score:0.9574 Val Loss: 0.6464406847953796, Val Acc: 0.8949\n","Epoch [181/500], Loss: 0.10759750753641129, Train Acc: 0.9633,train F1-score:0.9631 Val Loss: 0.6584047675132751, Val Acc: 0.8929\n","Epoch [182/500], Loss: 0.10486065596342087, Train Acc: 0.9655,train F1-score:0.9650 Val Loss: 0.6665477156639099, Val Acc: 0.8929\n","Epoch [183/500], Loss: 0.1047234758734703, Train Acc: 0.9631,train F1-score:0.9622 Val Loss: 0.6579316258430481, Val Acc: 0.8916\n","Early stopping at epoch 183\n","Test Loss: 0.5361124277114868, Test Accuracy: 0.8847184986595175\n","Precision: 0.8811, Recall: 0.8847, F1-score: 0.8824\n","F1-score saved to file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]}]}